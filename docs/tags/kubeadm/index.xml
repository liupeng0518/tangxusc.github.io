<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kubeadm on 苏连云的博客</title>
    <link>https://tangxusc.github.io/blog/tags/kubeadm/</link>
    <description>Recent content in Kubeadm on 苏连云的博客</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 20 Mar 2019 14:15:59 +0800</lastBuildDate>
    
	<atom:link href="https://tangxusc.github.io/blog/tags/kubeadm/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>jaeger-operator安装</title>
      <link>https://tangxusc.github.io/blog/2019/03/jaeger-operator%E5%AE%89%E8%A3%85/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/jaeger-operator%E5%AE%89%E8%A3%85/</guid>
      <description>jaeger-operator安装 在一个成规模的微服务系统中,一个功能不单由这一个服务完成,而是多个服务协作来共同完成,但是如果其中一个服务出现了错误,对于错误的追踪,对于整个调用链的追踪便成为了难题.
好在外国佬遇到了这些问题,指定了opentracing规范,并且提供了例如zipkin,pinpoint,jaeger等工具供我们使用
jaeger组件如下:
安装 1. elasticsearch jaeger的存储是依赖cassandra或elasticsearch的,jaeger本身并不存储数据,在此处我们使用es来存储数据
$ kubectl apply -f https://gitee.com/tanx/kubernetes-test/raw/master/kubernetes/init-cn/efk-elasticsearch.yaml   elasticsearch 他妈的又需要存储支持,惊不惊喜,意不意外.
 此处建议此es应和k8s中的日志收集使用一个es集群,方便管理,并且追踪数据并不需要支持事务等特性,符合日志存储模式.
2. jaeger operator 在github中提供了operator的安装,直接使用就行
$ kubectl create namespace observability $ kubectl create -f https://raw.githubusercontent.com/jaegertracing/jaeger-operator/master/deploy/crds/jaegertracing_v1_jaeger_crd.yaml $ kubectl create -f https://raw.githubusercontent.com/jaegertracing/jaeger-operator/master/deploy/service_account.yaml $ kubectl create -f https://raw.githubusercontent.com/jaegertracing/jaeger-operator/master/deploy/role.yaml $ kubectl create -f https://raw.githubusercontent.com/jaegertracing/jaeger-operator/master/deploy/role_binding.yaml $ kubectl create -f https://raw.githubusercontent.com/jaegertracing/jaeger-operator/master/deploy/operator.yaml   注意:如果安装在其他命名空间中貌似不行&amp;hellip; 起码我使用helm安装在jaeger命名空间中不行.
 安装成功后等待几分钟就可以看到如下资源的成功运行
$ kubectl get all -n observability NAME READY STATUS RESTARTS AGE pod/jaeger-operator-69c987b98-grv9n 1/1 Running 0 23h NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/jaeger-operator ClusterIP 10.</description>
    </item>
    
    <item>
      <title>kubeadm安装HA集群</title>
      <link>https://tangxusc.github.io/blog/2019/03/kubeadm%E5%AE%89%E8%A3%85ha%E9%9B%86%E7%BE%A4/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/kubeadm%E5%AE%89%E8%A3%85ha%E9%9B%86%E7%BE%A4/</guid>
      <description>kubeadm安装HA集群 鉴于使用二进制的方式安装较为复杂,且不太好处理证书的生成,分发等问题,并且对性能没有较高的要求,所以强烈推荐使用此模式,具体下来这个模式的好处为:
 官方提供的工具,有官方的文档支持 安装贼简单,没有其他的依赖 扩展性强,有官方的一些扩展支持 集群全部以容器启动,所以没那么多你需要管理的service  准备 硬件  一台或多台运行 Ubuntu 16.04 + 的主机(其他linux系统也行) 集群中完整的网络连接，公网或者私网都可以  各节点环境  #### docker  使用加入器安装docker
curl https://releases.rancher.com/install-docker/17.03.sh | sh   #### 禁用swap  然后需要禁用 swap 文件，这是 Kubernetes 的强制步骤。编辑/etc/fstab文件，注释掉引用swap的行，保存并重启后输入sudo swapoff -a即可
vim /etc.fastab #注释调swap的行 sudo swapoff -a   设置源 sudo apt-get update &amp;amp;&amp;amp; sudo apt-get install -y apt-transport-https curl -s https://gitee.com/tanx/kubernetes-test/raw/master/kubeadm/apt-key.gpg | sudo apt-key add - sudo cat &amp;lt;&amp;lt;EOF &amp;gt;/etc/apt/sources.</description>
    </item>
    
    <item>
      <title>kubeadm生成的token过期后，集群增加节点</title>
      <link>https://tangxusc.github.io/blog/2019/03/kubeadm%E7%94%9F%E6%88%90%E7%9A%84token%E8%BF%87%E6%9C%9F%E5%90%8E%E9%9B%86%E7%BE%A4%E5%A2%9E%E5%8A%A0%E8%8A%82%E7%82%B9/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/kubeadm%E7%94%9F%E6%88%90%E7%9A%84token%E8%BF%87%E6%9C%9F%E5%90%8E%E9%9B%86%E7%BE%A4%E5%A2%9E%E5%8A%A0%E8%8A%82%E7%82%B9/</guid>
      <description> kubeadm生成的token过期后，集群增加节点  重新生成token(默认token24小时后过期)
kubeadm token create TOKEN TTL EXPIRES USAGES DESCRIPTION EXTRA GROUPS 36iajs.t016zpxbyqdmivcq 19h 2018-07-04T12:48:32+08:00 authentication,signing The default bootstrap token generated by &#39;kubeadm init&#39;. system:bootstrappers:kubeadm:default-node-token  获取ca证书sha256编码hash值
openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2&amp;gt;/dev/null | openssl dgst -sha256 -hex | sed &#39;s/^.* //&#39;  节点加入集群
kubeadm join &amp;lt;节点IP&amp;gt;:6443 --token 36iajs.t016zpxbyqdmivcq --discovery-token-ca-cert-hash sha256:19246ce11ba3fc633fe0b21f2f8aaaebd7df9103ae47138dc0dd615f61a32d99   </description>
    </item>
    
    <item>
      <title>prometheus和alertmanager监控并发送邮件</title>
      <link>https://tangxusc.github.io/blog/2019/03/prometheus%E5%92%8Calertmanager%E7%9B%91%E6%8E%A7%E5%B9%B6%E5%8F%91%E9%80%81%E9%82%AE%E4%BB%B6/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/prometheus%E5%92%8Calertmanager%E7%9B%91%E6%8E%A7%E5%B9%B6%E5%8F%91%E9%80%81%E9%82%AE%E4%BB%B6/</guid>
      <description>prometheus和alertmanager监控并发送邮件 prometheus 简介 prometheus是时序的监控系统,通常使用prometheus是将prometheus当做采集存储中间件来使用,配合grafana做图表展示,配合alertmanager做自定义告警.
prometheus的每个样本的大小为1-2个字节,要评估服务器的容量,可以使用以下公式:
needed_disk_space = retention_time_seconds * ingested_samples_per_second * bytes_per_sample 需要的硬盘空间 = 数据保留时间(s) * 每秒采集的样本 * 样本大小  数据类型 在系统中所有的数据都以指标的方式来标示,指标包含label,指标格式如下:
&amp;lt;metric name&amp;gt;{&amp;lt;label name&amp;gt;=&amp;lt;label value&amp;gt;, ...} ##例如 api_http_requests_total{method=&amp;quot;POST&amp;quot;, handler=&amp;quot;/messages&amp;quot;}  在系统中,所有指标都在采集时得到其数据类型和描述,通常如下:
$ curl localhost:9090/metrics # HELP go_gc_duration_seconds A summary of the GC invocation durations. # TYPE go_gc_duration_seconds summary go_gc_duration_seconds{quantile=&amp;quot;0&amp;quot;} 2.9632e-05 go_gc_duration_seconds{quantile=&amp;quot;0.25&amp;quot;} 4.7174e-05 go_gc_duration_seconds{quantile=&amp;quot;0.5&amp;quot;} 5.8693e-05 go_gc_duration_seconds{quantile=&amp;quot;0.75&amp;quot;} 9.4042e-05 go_gc_duration_seconds{quantile=&amp;quot;1&amp;quot;} 0.021392614 go_gc_duration_seconds_sum 0.056610034 go_gc_duration_seconds_count 70 # HELP go_goroutines Number of goroutines that currently exist.</description>
    </item>
    
    <item>
      <title>使用 kubeadm 搭建 Kubernetes(1.10.2) 集群（国内环境）</title>
      <link>https://tangxusc.github.io/blog/2019/03/%E4%BD%BF%E7%94%A8-kubeadm-%E6%90%AD%E5%BB%BA-kubernetes1.10.2-%E9%9B%86%E7%BE%A4%E5%9B%BD%E5%86%85%E7%8E%AF%E5%A2%83/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/%E4%BD%BF%E7%94%A8-kubeadm-%E6%90%AD%E5%BB%BA-kubernetes1.10.2-%E9%9B%86%E7%BE%A4%E5%9B%BD%E5%86%85%E7%8E%AF%E5%A2%83/</guid>
      <description>本文由 简悦 SimpRead 转码， 原文地址 https://www.cnblogs.com/RainingNight/p/using-kubeadm-to-create-a-cluster.html
 使用 kubeadm 搭建 Kubernetes(1.10.2) 集群（国内环境） [TOC]
目标  在您的机器上建立一个安全的 Kubernetes 集群。 在集群里安装网络插件，以便应用之间可以相互通讯。 在集群上运行一个简单的微服务。  准备 主机  一台或多台运行 Ubuntu 16.04 + 的主机。 最好选至少有 2 GB 内存的双核主机。 集群中完整的网络连接，公网或者私网都可以。  软件 安装 Docker sudo apt-get update sudo apt-get install -y docker.io  Kubunetes 建议使用老版本的docker.io，如果需要使用最新版的docker-ce，可参考上一篇博客：Docker 初体验。
禁用 swap 文件 然后需要禁用 swap 文件，这是 Kubernetes 的强制步骤。实现它很简单，编辑/etc/fstab文件，注释掉引用swap的行，保存并重启后输入sudo swapoff -a即可。
 对于禁用swap内存，你可能会有点不解，具体原因可以查看 Github 上的 Issue：Kubelet/Kubernetes should work with Swap Enabled。</description>
    </item>
    
    <item>
      <title>使用nfs作为k8s的PersistentVolume</title>
      <link>https://tangxusc.github.io/blog/2019/03/%E4%BD%BF%E7%94%A8nfs%E4%BD%9C%E4%B8%BAk8s%E7%9A%84persistentvolume/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/%E4%BD%BF%E7%94%A8nfs%E4%BD%9C%E4%B8%BAk8s%E7%9A%84persistentvolume/</guid>
      <description>使用nfs作为k8s的PersistentVolume 在较小规模的生产和开发的过程中,对于k8s的某些应用可能我们需要提供存储的支持,在初期我们可能并不需要性能那么高,扩展性那么强的存储,那么这个时候nfs就成了我们的首选
本文将引导各位在服务器中部署nfs服务,并在k8s中使用nfs服务
准备 nfs server 一台
k8s集群 一台
NFS server配置 安装nfs,并启动 $ yum install -y nfs-utils ##关闭防火墙 $ systemctl disable firewalld $ systemctl stop firewalld ##开启nfs自动启动 $ systemctl enable rpcbind.service $ systemctl enable nfs-server.service ## 启动nfs $ systemctl start rpcbind.service $ systemctl start nfs-server.service  配置nfs 编辑文件/etc/exports设置nfs需要暴露的文件夹
$ vim /etc/exports #添加 此处暴露的是/home/nfsdata目录 /home/nfsdata *(insecure,rw,sync,no_root_squash) ###修改暴露的目录的权限 $ chmod 777 -R /home/nfsdata ##重启nfs $ systemctl restart nfs.service  验证 $ showmount -e 10.</description>
    </item>
    
    <item>
      <title>使用rook搭建存储集群</title>
      <link>https://tangxusc.github.io/blog/2019/03/%E4%BD%BF%E7%94%A8rook%E6%90%AD%E5%BB%BA%E5%AD%98%E5%82%A8%E9%9B%86%E7%BE%A4/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/%E4%BD%BF%E7%94%A8rook%E6%90%AD%E5%BB%BA%E5%AD%98%E5%82%A8%E9%9B%86%E7%BE%A4/</guid>
      <description>使用rook搭建存储集群 rook是云原生的存储协调器,为各种存储提供解决方案,提供自我管理,自我扩展,自我修复的存储服务,在kubernetes中实际实现是operator方式.
在rook 0.9版本中,Ceph已经是beta支持状态了.
Ceph是一种高度可扩展的分布式存储解决方案，适用于具有多年生产部署的块存储，对象存储和共享文件系统.
安装 rook提供了operator的方式来处理ceph存储的安装,所以此处安装使用helm来安装rook
helm repo add rook-stable https://charts.rook.io/stable helm install --namespace rook-ceph-system rook-stable/rook-ceph  ceph集群 安装了rook后,还需要在rook中声明CRD来建立ceph集群
cluster.yaml
apiVersion: v1 kind: Namespace metadata: name: rook-ceph --- apiVersion: v1 kind: ServiceAccount metadata: name: rook-ceph-osd namespace: rook-ceph --- apiVersion: v1 kind: ServiceAccount metadata: name: rook-ceph-mgr namespace: rook-ceph --- kind: Role apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: rook-ceph-osd namespace: rook-ceph rules: - apiGroups: [&amp;quot;&amp;quot;] resources: [&amp;quot;configmaps&amp;quot;] verbs: [ &amp;quot;get&amp;quot;, &amp;quot;list&amp;quot;, &amp;quot;watch&amp;quot;, &amp;quot;create&amp;quot;, &amp;quot;update&amp;quot;, &amp;quot;delete&amp;quot; ] --- kind: Role apiVersion: rbac.</description>
    </item>
    
    <item>
      <title>自动部署k8s基础应用</title>
      <link>https://tangxusc.github.io/blog/2019/03/%E8%87%AA%E5%8A%A8%E9%83%A8%E7%BD%B2k8s%E5%9F%BA%E7%A1%80%E5%BA%94%E7%94%A8/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/%E8%87%AA%E5%8A%A8%E9%83%A8%E7%BD%B2k8s%E5%9F%BA%E7%A1%80%E5%BA%94%E7%94%A8/</guid>
      <description>自动部署k8s基础应用 在k8s集群安装完成后,我们需要为集群安装很多初始应用,此处通过一个简易脚本方式安装以下应用:
 helm-tiller kubeapp rook rook-cluster elasticsearch-fluentd-kinaba prometheus metrics-server jaeger dashboard  该脚本使用如下:
curl https://gitee.com/tanx/kubernetes-test/raw/master/kubernetes/init/init.sh | sh  对于国内用户,推荐使用cn脚本:
curl https://gitee.com/tanx/kubernetes-test/raw/master/kubernetes/init-cn/init.sh | sh  脚本说明 helm-tiller wget https://storage.googleapis.com/kubernetes-helm/helm-v2.13.0-linux-amd64.tar.gz &amp;amp;&amp;amp; tar -zxvf helm-v2.13.0-linux-amd64.tar.gz &amp;amp;&amp;amp; sudo cp linux-amd64/helm /usr/local/bin helm init --upgrade -i registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.13.0 --stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts --service-account=clusterrole-aggregation-controller helm repo add rook-stable https://charts.rook.io/stable helm repo add bitnami https://charts.bitnami.com/bitnami helm repo update  kubeapp helm install --name kubeapps --namespace kubeapps bitnami/kubeapps kubectl create serviceaccount kubeapps-operator kubectl create clusterrolebinding kubeapps-operator --clusterrole=cluster-admin --serviceaccount=default:kubeapps-operator export kubeappsPWD=$( kubectl get secret $(kubectl get serviceaccount kubeapps-operator -o jsonpath=&#39;{.</description>
    </item>
    
  </channel>
</rss>