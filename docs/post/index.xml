<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on 苏连云的博客</title>
    <link>https://tangxusc.github.io/blog/post/</link>
    <description>Recent content in Posts on 苏连云的博客</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 12 Apr 2019 15:59:49 +0800</lastBuildDate>
    
	<atom:link href="https://tangxusc.github.io/blog/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>迈向istio-11 升级到1.1.2</title>
      <link>https://tangxusc.github.io/blog/2019/04/%E8%BF%88%E5%90%91istio-11-%E5%8D%87%E7%BA%A7%E5%88%B01.1.2/</link>
      <pubDate>Fri, 12 Apr 2019 15:59:49 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/04/%E8%BF%88%E5%90%91istio-11-%E5%8D%87%E7%BA%A7%E5%88%B01.1.2/</guid>
      <description>废话 istio经过8个月的发展和社区中的各位大佬的孜孜不倦的贡献,终于发布了1.1版本,新版本的口号叫企业级就绪(以后会不会出云供应商就绪?),这8个月中可以看到istio还是发生了很大的变化的,纵观历史,istio进行了三次腾飞:
 0.8 不在是模型了 1.0 生产就绪了(其实没有),抛弃了ingress,模型改动比较大 1.1 企业就绪了,理清楚了流量管理的模型啊啊啊啊,sidecar不再操碎了心,pilot解放了,mixer adapter又被默认的关了,ServiceEntry存在感突然就没啦.  废话太多了,接下来我们进入正题,去看看istio1.1到底更新了那些东西
 注意,不会全部的解析,只捡我认为重要的说.
 升级内容 安装 1.安装模式变化 现在提供了两个helm chart来安装,分别为istio-init和istio 这两个职责如下:
istio-init: 负责通过job.batch/istio-init-crd-10,job.batch/istio-init-crd-11 这两个job来安装istio的crd资源(一共53个,如果启用cert-manager则为58个),可通过命令查看:
$ kubectl get crds | grep &#39;istio.io\|certmanager.k8s.io&#39; | wc -l 53  istio: 现在istio安装的时候是没有启用grafana,kiali的,并且已经说明使用kiali替换servicegraph,所以在安装时,需要手动开启:
# # addon grafana configuration # grafana: enabled: true # # addon kiali tracing configuration # kiali: enabled: true createDemoSecret: true  并且现在支持自定义kiali的用户名密码了,如果还是使用admin/admin 那么就需要createDemoSecret: true
istio现在提供了一个cni组件来避免init-container的privilege问题,不过这个cni需要kubelet的cni支持,kubelet的网络就两种kubenet和cni,也就是说 /etc/systemd/system/kubelet.service.d/10-kubeadm.conf文件中的
Environment=&amp;quot;KUBELET_NETWORK_ARGS=--network-plugin=cni --cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/opt/cni/bin&amp;quot;  这里的--network-plugin=cni 值只能是 cni了</description>
    </item>
    
    <item>
      <title>Controller manager高可用实现方式</title>
      <link>https://tangxusc.github.io/blog/2019/03/controller-manager%E9%AB%98%E5%8F%AF%E7%94%A8%E5%AE%9E%E7%8E%B0%E6%96%B9%E5%BC%8F/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/controller-manager%E9%AB%98%E5%8F%AF%E7%94%A8%E5%AE%9E%E7%8E%B0%E6%96%B9%E5%BC%8F/</guid>
      <description>本文由 简悦 SimpRead 转码， 原文地址 https://www.colabug.com/2801661.html
 这不是一系列入门级别的文章，也不是按部就班而来的，而是我看到哪里，发现有些代码写的精妙的地方，都值得我们学习下，顺手记录下来，一方面是让自己将来可以有迹可循，另外对大家应该也会有所帮助。而且记录本身成本并不是很高。
高可用部署情况下，需要部署多个 controller manager （以下简称 cm ），每个 cm 需要 --leader-elect=true 启动参数，即告知 cm 以高可用方式启动，谁要想进行真正的工作，必须先抢到锁，被选举为 leader 才行，而抢不到所得只能待机，在 leader 因为异常终止的时候，由剩余的其余节点再次获得锁。
关于分布式锁的实现很多，可以自己从零开始制造。当然更简单的是基于现有中间件，比如有基于 Redis 或数据库的实现方式，最近 Zookeeper/ETCD 也提供了相关功能。但 K8s 的实现并没有使用这些方式，而是另辟蹊径使用了资源锁的概念，简单来说就是通过创建 K8s 的资源（当前的实现中实现了 ConfigMap 和 Endpoint 两种类型的资源）来维护锁的状态。
分布式锁一般实现原理就是大家先去抢锁，抢到的人成为 leader ，然后 leader 会定期更新锁的状态，声明自己的活动状态，不让其他人把锁抢走。K8s 的资源锁也类似，抢到锁的节点会将自己的标记（目前是 hostname）设为锁的持有者，其他人则需要通过对比锁的更新时间和持有者来判断自己是否能成为新的 leader ，而 leader 则可以通过更新 RenewTime 来确保持续保有该锁。
大概看了下 K8s 的实现，老实说其实现方式并不算高雅，但是却给我们开拓了一种思路：K8s 里的 resource 是万能的，不要以为 Endpoint 只是 Endpoint 。不过反过来有时候也挺让人费解的，刚了解的时候容易摸不着头脑，也不是好事。而且 scheduler 和 cm 都采用了资源锁，但是实现起来却不尽相同，也值得吐槽下。不管怎么说，这个实现算是挺有意思的实现，值得我们深入了解下。
我们首先来看一下 cm 启动的时候，是如何去 初始化 抢锁的。启动的时候，如果指定了 --leader-elect=true 参数的话，则会进入下面的代码，首先获取自己的资源标志（这里是 hostname 加一串随机数字）。</description>
    </item>
    
    <item>
      <title>Gitflow 工作流</title>
      <link>https://tangxusc.github.io/blog/2019/03/gitflow-%E5%B7%A5%E4%BD%9C%E6%B5%81/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/gitflow-%E5%B7%A5%E4%BD%9C%E6%B5%81/</guid>
      <description>本文由 简悦 SimpRead 转码， 原文地址 http://blog.jobbole.com/76867/
 这节介绍的 Gitflow工作流借鉴自在 nvie 的 _Vincent Driessen_。
Gitflow工作流定义了一个围绕项目发布的严格分支模型。虽然比功能分支工作流复杂几分，但提供了用于一个健壮的用于管理大型项目的框架。
Gitflow工作流没有用超出功能分支工作流的概念和命令，而是为不同的分支分配一个很明确的角色，并定义分支之间如何和什么时候进行交互。除了使用功能分支，在做准备、维护和记录发布也使用各自的分支。当然你可以用上功能分支工作流所有的好处：Pull Requests、隔离实验性开发和更高效的协作。
工作方式 Gitflow工作流仍然用中央仓库作为所有开发者的交互中心。和其它的工作流一样，开发者在本地工作并push分支到要中央仓库中。
历史分支 相对使用仅有的一个master分支，Gitflow工作流使用 2 个分支来记录项目的历史。master分支存储了正式发布的历史，而develop分支作为功能的集成分支。这样也方便master分支上的所有提交分配一个版本号。
剩下要说明的问题围绕着这 2 个分支的区别展开。
功能分支 每个新功能位于一个自己的分支，这样可以 push到中央仓库以备份和协作。但功能分支不是从master分支上拉出新分支，而是使用develop分支作为父分支。当新功能完成时，合并回develop分支。新功能提交应该从不直接与master分支交互。
注意，从各种含义和目的上来看，功能分支加上develop分支就是功能分支工作流的用法。但Gitflow工作流没有在这里止步。
发布分支 一旦develop分支上有了做一次发布（或者说快到了既定的发布日）的足够功能，就从develop分支上fork一个发布分支。新建的分支用于开始发布循环，所以从这个时间点开始之后新的功能不能再加到这个分支上 —— 这个分支只应该做Bug修复、文档生成和其它面向发布任务。一旦对外发布的工作都完成了，发布分支合并到master分支并分配一个版本号打好Tag。另外，这些从新建发布分支以来的做的修改要合并回develop分支。
使用一个用于发布准备的专门分支，使得一个团队可以在完善当前的发布版本的同时，另一个团队可以继续开发下个版本的功能。 这也打造定义良好的开发阶段（比如，可以很轻松地说，『这周我们要做准备发布版本 4.0』，并且在仓库的目录结构中可以实际看到）。
常用的分支约定：
用于新建发布分支的分支: develop 用于合并的分支: master 分支命名: release-* 或 release/*
维护分支 维护分支或说是热修复（hotfix）分支用于生成快速给产品发布版本（production releases）打补丁，这是唯一可以直接从master分支fork出来的分支。修复完成，修改应该马上合并回master分支和develop分支（当前的发布分支），master分支应该用新的版本号打好Tag。
为Bug修复使用专门分支，让团队可以处理掉问题而不用打断其它工作或是等待下一个发布循环。你可以把维护分支想成是一个直接在master分支上处理的临时发布。
示例 下面的示例演示本工作流如何用于管理单个发布循环。假设你已经创建了一个中央仓库。
创建开发分支 第一步为master分支配套一个develop分支。简单来做可以本地创建一个空的develop分支，push到服务器上：
git branch develop git push -u origin develop
以后这个分支将会包含了项目的全部历史，而master分支将只包含了部分历史。其它开发者这时应该克隆中央仓库，建好develop分支的跟踪分支：
git clone ssh://user@host/path/to/repo.git git checkout -b develop origin/develop</description>
    </item>
    
    <item>
      <title>Go 程序性能优化及 pprof 使用方法详解</title>
      <link>https://tangxusc.github.io/blog/2019/03/go-%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E5%8F%8A-pprof-%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95%E8%AF%A6%E8%A7%A3/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/go-%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E5%8F%8A-pprof-%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95%E8%AF%A6%E8%A7%A3/</guid>
      <description>本文由 简悦 SimpRead 转码， 原文地址 https://www.jb51.net/article/127551.htm
 Go 程序性能优化及 pprof 使用方法详解  更新时间：2017 年 11 月 05 日 10:50:22 作者：snowInPluto
这篇文章主要为大家详细介绍了 Go 程序性能优化及 pprof 的使用方法，具有一定的参考价值，感兴趣的小伙伴们可以参考一下
Go 程序的性能优化及 pprof 的使用
程序的性能优化无非就是对程序占用资源的优化。对于服务器而言，最重要的两项资源莫过于 CPU 和内存。性能优化，就是在对于不影响程序数据处理能力的情况下，我们通常要求程序的 CPU 的内存占用尽量低。反过来说，也就是当程序 CPU 和内存占用不变的情况下，尽量地提高程序的数据处理能力或者说是吞吐量。
Go 的原生工具链中提供了非常多丰富的工具供开发者使用，其中包括 pprof。
对于 pprof 的使用要分成下面两部分来说。
Web 程序使用 pprof
先写一个简单的 Web 服务程序。程序在 9876 端口上接收请求。
package main import ( &amp;quot;bytes&amp;quot; &amp;quot;io/ioutil&amp;quot; &amp;quot;log&amp;quot; &amp;quot;math/rand&amp;quot; &amp;quot;net/http&amp;quot; _ &amp;quot;net/http/pprof&amp;quot; ) func main() { http.HandleFunc(&amp;quot;/test&amp;quot;, handler) log.Fatal(http.ListenAndServe(&amp;quot;:9876&amp;quot;, nil)) } func handler(w http.</description>
    </item>
    
    <item>
      <title>Golang大杀器之性能剖析PProf</title>
      <link>https://tangxusc.github.io/blog/2019/03/golang%E5%A4%A7%E6%9D%80%E5%99%A8%E4%B9%8B%E6%80%A7%E8%83%BD%E5%89%96%E6%9E%90pprof/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/golang%E5%A4%A7%E6%9D%80%E5%99%A8%E4%B9%8B%E6%80%A7%E8%83%BD%E5%89%96%E6%9E%90pprof/</guid>
      <description>本文由 简悦 SimpRead 转码， 原文地址 https://github.com/EDDYCJY/blog/blob/master/golang/2018-09-15-Golang%20%E5%A4%A7%E6%9D%80%E5%99%A8%E4%B9%8B%E6%80%A7%E8%83%BD%E5%89%96%E6%9E%90%20PProf.md
 Golang 大杀器之性能剖析 PProf 前言 写了几吨代码，实现了几百个接口。功能测试也通过了，终于成功的部署上线了
结果，性能不佳，什么鬼？
想做性能分析 PProf 想要进行性能优化，首先瞩目在 Go 自身提供的工具链来作为分析依据，本文将带你学习、使用 Go 后花园，涉及如下：
 runtime/pprof：采集程序（非 Server）的运行数据进行分析 net/http/pprof：采集 HTTP Server 的运行时数据进行分析  是什么 pprof 是用于可视化和分析性能分析数据的工具
pprof 以 profile.proto 读取分析样本的集合，并生成报告以可视化并帮助分析数据（支持文本和图形报告）
profile.proto 是一个 Protocol Buffer v3 的描述文件，它描述了一组 callstack 和 symbolization 信息， 作用是表示统计分析的一组采样的调用栈，是很常见的 stacktrace 配置文件格式
支持什么使用模式  Report generation：报告生成 Interactive terminal use：交互式终端使用 Web interface：Web 界面  可以做什么  CPU Profiling：CPU 分析，按照一定的频率采集所监听的应用程序 CPU（含寄存器）的使用情况，可确定应用程序在主动消耗 CPU 周期时花费时间的位置 Memory Profiling：内存分析，在应用程序进行堆分配时记录堆栈跟踪，用于监视当前和历史内存使用情况，以及检查内存泄漏 Block Profiling：阻塞分析，记录 goroutine 阻塞等待同步（包括定时器通道）的位置 Mutex Profiling：互斥锁分析，报告互斥锁的竞争情况  一个简单的例子 我们将编写一个简单且有点问题的例子，用于基本的程序初步分析</description>
    </item>
    
    <item>
      <title>Go模块简介</title>
      <link>https://tangxusc.github.io/blog/2019/03/go%E6%A8%A1%E5%9D%97%E7%AE%80%E4%BB%8B/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/go%E6%A8%A1%E5%9D%97%E7%AE%80%E4%BB%8B/</guid>
      <description>本文由 简悦 SimpRead 转码， 原文地址 https://roberto.selbach.ca/intro-to-go-modules/
 Go模块简介 发表于 2018年8月18日(https://roberto.selbach.ca/intro-to-go-modules/) 作者：Roberto Selbach
即将发布的Go编程语言版本1.11将为_模块_带来实验性支持 ，几天前Go.A的新依赖管理系统，我写了一篇关于它的快速帖子。自那篇文章上线以来，事情发生了一些变化，因为我们现在非常接近新版本，我认为现在是另一篇文章更适合实践的好时机。所以这就是我们要做的：我们将创建一个新的包，然后我们将发布一些版本，看看它是如何工作的。
创建模块 首先要做的事情。让我们创建我们的包。我们称之为“testmod”。这里有一个重要的细节：这个目录应该 在你的外面，因为默认情况下，模块支持在其中被禁用。Go模块是可能在某些时候完全消除的第一步。$GOPATH$GOPATH
$ mkdir testmod $ cd testmod  我们的包很简单：
package testmod import &amp;quot;fmt&amp;quot; // Hi returns a friendly greeting func Hi(name string) string { return fmt.Sprintf(&amp;quot;Hi, %s&amp;quot;, name) }  包完成但它仍然不是_模块_。让我们改变这一点。
$ go mod init github.com/robteix/testmod go: creating new go.mod: module github.com/robteix/testmod  这将go.mod在包目录中创建一个新文件，其中包含以下内容：
module github.com/robteix/testmod  这里不是很多，但这有效地将我们的包变成了一个 _模块_。我们现在可以将这个代码推送到一个存储库：
$ git init $ git add * $ git commit -am &amp;quot;First commit&amp;quot; $ git push -u origin master  到目前为止，任何愿意使用此软件包的人都会go get ：</description>
    </item>
    
    <item>
      <title>Intellij IDEA 基于编辑器的 REST 客户端介绍</title>
      <link>https://tangxusc.github.io/blog/2019/03/intellij-idea-%E5%9F%BA%E4%BA%8E%E7%BC%96%E8%BE%91%E5%99%A8%E7%9A%84-rest-%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%BB%8B%E7%BB%8D/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/intellij-idea-%E5%9F%BA%E4%BA%8E%E7%BC%96%E8%BE%91%E5%99%A8%E7%9A%84-rest-%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%BB%8B%E7%BB%8D/</guid>
      <description>本文由 简悦 SimpRead 转码， 原文地址 https://blog.csdn.net/u011054333/article/details/78705256
 最近 Intellij IDEA 更新到了 2017.3 这一版本，这个版本又增加了很多新功能。我觉得其中这个基于编辑器的 REST 客户端这个功能很不错，可以为我们带来很多方便。这个功能并不仅仅在 Intellij IDEA 才有，最近更新的所有 Jetbrains 系 IIDE 都有这个功能。
以往我们开发和调试网络程序，用到的无非是这几种办法：浏览器 F12 工具、Fiddler、Wireshark、curl 等命令行工具、手动使用 HTTP 客户端类库编程。不过这些方法总是有些不好用。Jetbrains 这个基于编辑器的 REST 客户端用起来倒是让我眼前一亮。
使用方法 要使用这个功能很简单，在 IDE 中新建一个后缀名为.http的文件，然后就可以使用这个功能了。截图如下。
这个功能使用起来非常简单，使用大写的 HTTP 动词（GET、POST、DELETE、PUT 等等）后面加上要访问的网址即可，如果端口号不是 80 或者 443，可以使用冒号 + 端口号的形式写在网址后面。如果需要修改 Cookie、ContentType、UA 等设置，直接写在后面几行即可，Jetbrains 提供了非常完善的补全支持，我们只要敲第一个大写字母即可获得相应的代码提示。想要发起一个请求的时候，直接点击前面的绿色运行按钮即可。一个文件中可以保存多个请求，如果以后还想再次运行只要打开这个文件即可。
配置环境变量 Jetbrains 还提供了一个环境变量的功能，让我们使用这个编辑器 REST 客户端更加简单。只要在项目中添加一个名为rest-client.env.json的文件，然后配置不同环境下要使用的环境变量。然后就能在 REST 客户端中使用了。例如配置文件是这样的。
{ &amp;quot;dev&amp;quot;: { &amp;quot;host&amp;quot;: &amp;quot;http://httpbin.org&amp;quot; }, &amp;quot;prod&amp;quot;: { &amp;quot;host&amp;quot;: &amp;quot;http://httpbin.org&amp;quot; } }  那么在点击运行按钮的时候就会弹出选择要使用哪个环境变量。我们只要选择就可以针对不同环境使用不同配置了。在代码中只要使用双括号引用环境变量即可。</description>
    </item>
    
    <item>
      <title>Java 奇技淫巧 - 插件化注解处理 API(Pluggable Annotation Processing API)</title>
      <link>https://tangxusc.github.io/blog/2019/03/java-%E5%A5%87%E6%8A%80%E6%B7%AB%E5%B7%A7-%E6%8F%92%E4%BB%B6%E5%8C%96%E6%B3%A8%E8%A7%A3%E5%A4%84%E7%90%86-apipluggable-annotation-processing-api/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/java-%E5%A5%87%E6%8A%80%E6%B7%AB%E5%B7%A7-%E6%8F%92%E4%BB%B6%E5%8C%96%E6%B3%A8%E8%A7%A3%E5%A4%84%E7%90%86-apipluggable-annotation-processing-api/</guid>
      <description>本文由 简悦 SimpRead 转码， 原文地址 https://www.cnblogs.com/throwable/p/9139908.html
 插件化注解处理 API(Pluggable Annotation Processing API) Java 奇技淫巧 - 插件化注解处理 API(Pluggable Annotation Processing API) 参考资料  JDK6 的新特性之六: 插入式注解处理 API(Pluggable Annotation Processing API) Java Annotation Processing and Creating a Builder  简介 插件化注解处理 (Pluggable Annotation Processing)APIJSR 269 提供一套标准 API 来处理 AnnotationsJSR 175, 实际上 JSR 269 不仅仅用来处理 Annotation，我觉得更强大的功能是它建立了 Java 语言本身的一个模型, 它把 method、package、constructor、type、variable、enum、annotation 等 Java 语言元素映射为 Types 和 Elements，从而将 Java 语言的语义映射成为对象，我们可以在 javax.lang.model 包下面可以看到这些类。所以我们可以利用 JSR 269 提供的 API 来构建一个功能丰富的元编程 (metaprogramming) 环境。JSR 269 用 Annotation Processor 在编译期间而不是运行期间处理 Annotation, Annotation Processor 相当于编译器的一个插件, 所以称为插入式注解处理.</description>
    </item>
    
    <item>
      <title>Java 读写锁实现原理</title>
      <link>https://tangxusc.github.io/blog/2019/03/java-%E8%AF%BB%E5%86%99%E9%94%81%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/java-%E8%AF%BB%E5%86%99%E9%94%81%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/</guid>
      <description>本文由 简悦 SimpRead 转码， 原文地址 https://my.oschina.net/editorial-story/blog/1928306
 最近做的一个小项目中有这样的需求：整个项目有一份*config.json*保存着项目的一些配置，是存储在本地文件的一个资源，并且应用中存在读写（读 &amp;gt;&amp;gt; 写）更新问题。既然读写并发操作，那么就涉及到操作互斥，这里自然想到了读写锁，本文对读写锁方面的知识做个梳理。
为什么需要读写锁？ 与传统锁不同的是读写锁的规则是可以共享读，但只能一个写，总结起来为：读读不互斥，读写互斥，写写互斥，而一般的独占锁是：读读互斥，读写互斥，写写互斥，而场景中往往读远远大于写，读写锁就是为了这种优化而创建出来的一种机制。
注意是读远远大于写，一般情况下独占锁的效率低来源于高并发下对临界区的激烈竞争导致线程上下文切换。因此当并发不是很高的情况下，读写锁由于需要额外维护读锁的状态，可能还不如独占锁的效率高。因此需要根据实际情况选择使用。
一个简单的读写锁实现 根据上面理论可以利用两个 int 变量来简单实现一个读写锁，实现虽然烂，但是原理都是差不多的，值得阅读下。
public class ReadWriteLock { /** * 读锁持有个数 */ private int readCount = 0; /** * 写锁持有个数 */ private int writeCount = 0; /** * 获取读锁,读锁在写锁不存在的时候才能获取 */ public synchronized void lockRead() throws InterruptedException { // 写锁存在,需要wait while (writeCount &amp;gt; 0) { wait(); } readCount++; } /** * 释放读锁 */ public synchronized void unlockRead() { readCount--; notifyAll(); } /** * 获取写锁,当读锁存在时需要wait.</description>
    </item>
    
    <item>
      <title>Java8 日期和时间</title>
      <link>https://tangxusc.github.io/blog/2019/03/java8-%E6%97%A5%E6%9C%9F%E5%92%8C%E6%97%B6%E9%97%B4/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/java8-%E6%97%A5%E6%9C%9F%E5%92%8C%E6%97%B6%E9%97%B4/</guid>
      <description>本文由 简悦 SimpRead 转码， 原文地址 https://blog.csdn.net/a80596890555/article/details/58687444
 如何正确处理时间 现实生活的世界里，时间是不断向前的，如果向前追溯时间的起点，可能是宇宙出生时，又或是是宇宙出现之前， 但肯定是我们目前无法找到的，我们不知道现在距离时间原点的精确距离。所以我们要表示时间， 就需要人为定义一个原点。
原点被规定为，格林威治时间 (GMT)1970 年 1 月 1 日的午夜 为起点, 之于为啥是 GMT 时间，大概是因为本初子午线在那的原因吧。
java 中的时间 如果你跟你朋友说：“我们 1484301456 一起去吃饭，别迟到！”，而你朋友能马上理解你说的时间，表示时间就会很简单， 只需要一个 long 值来表示原点的偏移量，这是个绝对时间，在世界范围内都适用。但实际上我们不能马上理解这串数字， 而且我们需要不同的时间单位来表示时间的跨度，比如一个季度是 3 个月，一个月有 30 天等。 你可以跟朋友约好 “明天这个时候再见面”, 你朋友很容易理解明天的意思，但要是没有’天’这个单位， 他就需要在那串数字上加上 86400(一天是 86400 秒)。
Java 三次引入处理时间的 API，JDK1.0 中包含了一个Date类，但大多数方法在 java1.1 引入Calendear类之后被弃用了。 它的实例都是可变的，而且它的 API 很难使用，比如月份是从 0 开始这种反人类的设置。
java8 引入的java.time API 已经纠正了之前的问题。它已经完全实现了JSR310规范。
java8 时间 API 介绍及使用 在新的时间 API 中，Instant表示一个精确的时间点，Duration和Period表示两个时间点之间的时间量。 LocalDate表示日期，即 xx 年 xx 月 xx 日，即不包括时间也不带时区。LocalTime与LocalDate类似， 但只包含时间。LocalDateTime则包含日期和时间。ZoneDateTime表示一个带时区的时间。 DateTimeFormatter提供格式化和解析功能。下面详细的介绍使用方法。</description>
    </item>
    
    <item>
      <title>Java注释处理和创建构建器</title>
      <link>https://tangxusc.github.io/blog/2019/03/java%E6%B3%A8%E9%87%8A%E5%A4%84%E7%90%86%E5%92%8C%E5%88%9B%E5%BB%BA%E6%9E%84%E5%BB%BA%E5%99%A8/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/java%E6%B3%A8%E9%87%8A%E5%A4%84%E7%90%86%E5%92%8C%E5%88%9B%E5%BB%BA%E6%9E%84%E5%BB%BA%E5%99%A8/</guid>
      <description>本文由 简悦 SimpRead 转码， 原文地址 http://www.baeldung.com/java-annotation-processing-builder
 1.简介 本文是Java源代码级别注释处理的简介，并提供了使用此技术在编译期间生成其他源文件的示例。
2.注释处理的应用 源级注释处理首先出现在Java 5中。它是一种在编译阶段生成其他源文件的便捷技术。
源文件不必是Java文件 - 您可以根据源代码中的注释生成任何类型的描述，元数据，文档，资源或任何其他类型的文件。
注释处理在许多无处不在的Java库中被广泛使用，例如，在QueryDSL和JPA中生成元类，以使用Lombok库中的样板代码来扩充类。
需要注意的一件重要事情是注释处理API的局限性 - 它只能用于生成新文件，而不能用于更改现有文件。
值得注意的例外是Lombok库，它使用注释处理作为引导机制，将自身包含在编译过程中，并通过一些内部编译器API修改AST。这种hacky技术与注释处理的预期目的无关，因此本文不讨论。
3.注释处理API 注释处理在多轮中完成。每一轮都从编译器搜索源文件中的注释并选择适合这些注释的注释处理器开始。反过来，每个注释处理器在相应的源上被调用。
如果在此过程中生成了任何文件，则会以生成的文件作为输入启动另一轮。此过程将继续，直到在处理阶段没有生成新文件。
反过来，每个注释处理器在相应的源上被调用。如果在此过程中生成了任何文件，则会以生成的文件作为输入启动另一轮。此过程将继续，直到在处理阶段没有生成新文件。
注释处理API位于_javax.annotation.processing_包中。您必须实现的主要接口是_Processor_接口，它具有_AbstractProcessor_类形式的部分实现。这个类是我们要扩展的类，以创建我们自己的注释处理器。
4.设置项目 为了演示注释处理的可能性，我们将开发一个简单的处理器，用于为带注释的类生成流畅的对象构建器。
我们将把项目分成两个Maven模块。其中之一，_注释处理器_模块，将包含处理器本身和注释，另一个_注释用户_模块将包含注释类。这是注释处理的典型用例。
_注释处理器_模块的设置如下。我们将使用Google的自动服务库来生成稍后将讨论的处理器元数据文件，以及针对Java 8源代码调整的_maven-compiler-plugin_。这些依赖项的版本将提取到属性部分。
可以在Maven Central存储库中找到最新版本的自动服务库和maven-compiler-plugin：
&amp;lt;properties&amp;gt; &amp;lt;auto-service.version&amp;gt;1.0-rc2&amp;lt;/auto-service.version&amp;gt; &amp;lt;maven-compiler-plugin.version&amp;gt; 3.5.1 &amp;lt;/maven-compiler-plugin.version&amp;gt; &amp;lt;/properties&amp;gt; &amp;lt;dependencies&amp;gt; &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;com.google.auto.service&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;auto-service&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;${auto-service.version}&amp;lt;/version&amp;gt; &amp;lt;scope&amp;gt;provided&amp;lt;/scope&amp;gt; &amp;lt;/dependency&amp;gt; &amp;lt;/dependencies&amp;gt; &amp;lt;build&amp;gt; &amp;lt;plugins&amp;gt; &amp;lt;plugin&amp;gt; &amp;lt;groupId&amp;gt;org.apache.maven.plugins&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;maven-compiler-plugin&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;${maven-compiler-plugin.version}&amp;lt;/version&amp;gt; &amp;lt;configuration&amp;gt; &amp;lt;source&amp;gt;1.8&amp;lt;/source&amp;gt; &amp;lt;target&amp;gt;1.8&amp;lt;/target&amp;gt; &amp;lt;/configuration&amp;gt; &amp;lt;/plugin&amp;gt; &amp;lt;/plugins&amp;gt; &amp;lt;/build&amp;gt;  带有注释源的注释用户 Maven模块不需要任何特殊调整，除了在依赖项部分中添加对注释处理器模块的依赖：
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;com.baeldung&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;annotation-processing&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;1.0.0-SNAPSHOT&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt;  5.</description>
    </item>
    
    <item>
      <title>Jenkins Sonar Github 代码质量管理</title>
      <link>https://tangxusc.github.io/blog/2019/03/jenkins-sonar-github-%E4%BB%A3%E7%A0%81%E8%B4%A8%E9%87%8F%E7%AE%A1%E7%90%86/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/jenkins-sonar-github-%E4%BB%A3%E7%A0%81%E8%B4%A8%E9%87%8F%E7%AE%A1%E7%90%86/</guid>
      <description>本文由 简悦 SimpRead 转码， 原文地址 https://segmentfault.com/a/1190000015592863
 背景 前阵子老美的 Audit 要求各个开发组截图各自 repository 的 Sonar Analysis Report，我跑去 Sonarqube 一看。。。好家伙！全是红灯，简直惨不忍睹
当然这其中有历史问题，因为我们是半路接管的欧美 team 的 code，很多 issue 都是 old code 所遗留的。
不逃避责任，其中也有一部分是我们后续提交的新代码造成的，通过项目 2 年来的日积月累，issue 多的有点积重难返，sonarqube 虽然在每次 jenkins build 都会生成 report，但是我们却没有把它作为 build 成功失败的一个硬指标。只要 build 成功通过 QA 测试就好了嘛！管他娘的 sonar quality gate
结果 为了出一份体面漂亮的 report 给 audit，我不得不快马加鞭的 checkout -b quick_fix_sonar_issues, 花了一整天的功夫把 block 和 critical 的 issue 降到了阈值以下。
临阵磨枪的我体会到了以下 3 个痛点
 有些 Sonar 能检测出来的 issue，确实能规避一些产品上的潜在 bug 有些同事在 code 中犯的错误真的很低级，但是人工 code review 中很难被发现，不是我的锅，我现在却在为同事擦屁股。 虽然快速 fix 了 issue，但是 code 的 owner 并不是我，我有可能为了迎合 sonar 的 rule 而产生了潜在的新的 issue，而和 owner 去一一 check 又增加了很多沟通成本，另外 owner 很有可能已经离职了  思考 囧则思变！如何改进我们的开发流程？在代码开发阶段就能让 Sonar 分析出问题？强制 owner 必须解决完 issue 才能提交代码？ 嗯！是时候对目前存在弊端的开发流程进行改进了！</description>
    </item>
    
    <item>
      <title>Kubernetes Operator SDK</title>
      <link>https://tangxusc.github.io/blog/2019/03/kubernetes-operator-sdk/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/kubernetes-operator-sdk/</guid>
      <description>本文由 简悦 SimpRead 转码， 原文地址 https://banzaicloud.com/blog/operator-sdk/
 在Banzai Cloud，我们一直在寻找新的创新技术，以支持我们的用户使用Pipeline过渡到部署到Kubernetes的微服务。最近几个月，我们与CoreOS和RedHat合作，开展了运营商及其刚刚开源的项目，并在GitHub上提供。如果您通读这篇博客，您将了解到什么是operator，如何使用它operator sdk来开发operator我们在Banzai Cloud开发和使用的具体示例。我们的GitHub上还有一些运营商都可以在新的运营商SDK 上构建。
TL;博士：  今天发布了一个新的Kubernetes运营商框架 我们积极参与了新的SDK，因此我们发布了一些 本博客中讨论的操作员可以为任何基于JVM的应用程序提供无缝的框控制，而无需实际具有刮擦界面  在Kubernetes上部署和运行由多个相互依赖的组件/服务组成的复杂应用程序并不总是微不足道的Kubernetes提供的构造。就像一个简单的例子，如果一个应用程序需要最少数量的实例，可以通过Kubernetes部署来解决。但是，如果实例的数量发生变化（高级/低级），则必须在运行时重新配置或重新初始化这些实例，而不是我们需要对这些事件作出反应并执行必要的重新配置步骤。尝试通过实现使用Kubernetes命令行工具的脚本来解决这些问题很容易变得麻烦，特别是当我们接近现实生活中的用例时，我们必须处理弹性，日志收集，监视等。
CoreOS引入了运营商来自动处理这些复杂的运营场景。简而言之operators，通过第三方资源机制（自定义资源）扩展Kubernetes API，并提供对细胞内部正在进行的细粒度访问和控制。
在我们进一步讨论之前，先谈谈Kubernetes的_自定义资源，_以便更好地了解它operator是什么。甲_资源_在Kubernetes是在端点Kubernetes API，其存储一定的Kubernetes对象（例如对象波德）_种类_（例如，POD）。一个_自定义资源_本质上是一种_资源_，可以添加到Kubernetes扩展基本Kubernetes API。一旦_自定义资源_安装用户可以管理这种对象kubectl相同的方式，为他们做内置Kubernetes资源，如_豆荚_的例子。必须有一个控制器来执行由此引起的操作kubectl。定制控制器是_自定义资源的_控制器。总而言之，a operator是一个自定义控制器，可以处理某种_自定义资源_。
CoreOS还开发了用于开发此类的SDK operators。SDK简化了a的实现，operator因为它提供了高级API来编写操作逻辑，为它生成框架，使开发人员无需编写样板代码。
我们来看看我们如何使用Operator SDK。
首先，我们需要将Operator SDK安装到我们的开发机器上。如果您准备冒险使用最新最好的安装来自master分支机构的CLI 。安装CLI后，开发流程将如下所示：
 创建一个新的操作员项目 定义要监视的Kubernetes资源 在指定的处理程序中定义操作符逻辑 更新并生成自定义资源的代码 构建并生成运营商部署清单 部署运营商 创建自定义资源  创建一个新的操作员项目 运行CLI以创建新operator项目。
$ cd $GOPATH/src/github.com/&amp;lt;your-github-repo&amp;gt;/ $ operator-sdk new &amp;lt;operator-project-name&amp;gt; --api-version=&amp;lt;your-api-group&amp;gt;/&amp;lt;version&amp;gt; --kind=&amp;lt;custom-resource-kind&amp;gt; $ cd &amp;lt;operator-project-name&amp;gt;   operator-project-name - CLI在此目录下生成项目框架 your-api-group - 这是我们处理的自定义资源的Kubernetes API组operator（例如mycompany.com） version - 这是我们处理的自定义资源的Kubernetes API版本operator（例如v1alpha，beta等，请参阅Kubernetes API版本） custom-resource-kind - 自定义资源类型的名称  定义要监视的Kubernetes资源 该main.</description>
    </item>
    
    <item>
      <title>Lombok 原理分析与功能实现</title>
      <link>https://tangxusc.github.io/blog/2019/03/lombok-%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90%E4%B8%8E%E5%8A%9F%E8%83%BD%E5%AE%9E%E7%8E%B0/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/lombok-%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90%E4%B8%8E%E5%8A%9F%E8%83%BD%E5%AE%9E%E7%8E%B0/</guid>
      <description>本文由 简悦 SimpRead 转码， 原文地址 https://blog.mythsman.com/2017/12/19/1/
 前言 这两天没什么重要的事情做，但是想着还要春招总觉得得学点什么才行，正巧想起来前几次面试的时候面试官总喜欢问一些框架的底层实现，但是我学东西比较倾向于用到啥学啥，因此在这些方面吃了很大的亏。而且其实很多框架也多而杂，代码起来费劲，无非就是几套设计模式套一套，用到的东西其实也就那么些，感觉没啥新意。刚这两天读” 深入理解 JVM” 的时候突然想起来有个叫 Lombok 的东西以前一直不能理解他的实现原理，现在正好趁着闲暇的时间研究研究。
Lombok 代码 Lombok 是一个开源项目，源代码托管在 GITHUB/rzwitserloot，如果需要在 maven 里引用，只需要添加下依赖:
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.projectlombok&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;lombok&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;1.16.8&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt;  功能 那么 Lombok 是做什么的呢？其实很简单，一个最简单的例子就是它能够实现通过添加注解，能够自动生成一些方法。比如这样的类:
@Getter class Test{ private String value; }  我们用 Lombok 提供的 @Getter 来注解这个类，这个类在编译的时候就会变成:
class Test{ private String value; public String getValue(){ return this.value; } }  当然 Lombok 也提供了很多其他的注解，这只是其中一个最典型的例子。其他的用法网上的资料已经很多了，这里就不啰嗦。 看上去是很方便的一个功能，尤其是在很多项目里有很多 bean，每次都要手写或自动生成 setter getter 方法，搞得代码很长而且没有啥意义，因此这个对简化代码的强迫症们还是很有吸引力的。 但是，我们发现这个包跟一般的包有很大区别，绝大多数 java 包都工作在运行时，比如 spring 提供的那种注解，通过在运行时用反射来实现业务逻辑。Lombok 这个东西工作却在编译期，在运行时是无法通过反射获取到这个注解的。 而且由于他相当于是在编译期对代码进行了修改，因此从直观上看，源代码甚至是语法有问题的。 一个更直接的体现就是，普通的包在引用之后一般的 IDE 都能够自动识别语法，但是 Lombok 的这些注解，一般的 IDE 都无法自动识别，比如我们上面的 Test 类，如果我们在其他地方这么调用了一下:</description>
    </item>
    
    <item>
      <title>Minikube 安装</title>
      <link>https://tangxusc.github.io/blog/2019/03/minikube-%E5%AE%89%E8%A3%85/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/minikube-%E5%AE%89%E8%A3%85/</guid>
      <description>本文由 简悦 SimpRead 转码， 原文地址 https://yq.aliyun.com/articles/221687
 为了方便大家开发和体验 Kubernetes，社区提供了可以在本地部署的 Minikube。由于网络访问原因，很多朋友无法使用 minikube 进行实验。为此我们提供了一个修改版的 Minikube，可以从阿里云的镜像地址来获取所需 Docker 镜像和配置。
注：
 本文已更新到 Minikube v0.28.0/Kubernetes v1.10.0 如需更新 minikube，需要更新 minikube 安装包
 minikube delete 删除现有虚机，删除 ~/.minikube 目录缓存的文件
 重新创建 minikube 环境
 Docker 社区版也为 Mac/Windows 用户提供了 Kubernetes 开发环境的支持 https://yq.aliyun.com/articles/508460，大家也可以试用
  配置 先决条件  安装 kubectl  Minikube 在不同操作系统上支持不同的驱动
 macOS
 xhyve driver, VirtualBox 或 VMware Fusion  Linux
 VirtualBox 或 KVM NOTE: Minikube 也支持 --vm-driver=none 选项来在本机运行 Kubernetes 组件，这时候需要本机安装了 Docker。在使用 0.</description>
    </item>
    
    <item>
      <title>Operator 原理</title>
      <link>https://tangxusc.github.io/blog/2019/03/operator-%E5%8E%9F%E7%90%86/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/operator-%E5%8E%9F%E7%90%86/</guid>
      <description>本文由 简悦 SimpRead 转码， 原文地址 https://blog.csdn.net/yan234280533/article/details/75333246
 Operator 是 CoreOS 推出的旨在简化复杂有状态应用管理的框架，它是一个感知应用状态的控制器，通过扩展 Kubernetes API 来自动创建、管理和配置应用实例。
Operator 原理 Operator 基于 Third Party Resources 扩展了新的应用资源，并通过控制器来保证应用处于预期状态。比如 etcd operator 通过下面的三个步骤模拟了管理 etcd 集群的行为：
 通过 Kubernetes API 观察集群的当前状态； 分析当前状态与期望状态的差别； 调用 etcd 集群管理 API 或 Kubernetes API 消除这些差别。  Operator 本质是通过在 Kubenertes 中部署对应的 Third-Party Resource (TPR) 插件，然后通过部署 Third-Party Resource 的方式来部署对应的应用。Third-Party Resource 会调用 Kubenertes 部署 API 部署相应的 Kubenertes 资源，并对资源状态进行管理。
如何创建 Operator Operator 是一个感知应用状态的控制器，所以实现一个 Operator 最关键的就是把管理应用状态的所有操作封装到配置资源和控制器中。通常来说 Operator 需要包括以下功能：</description>
    </item>
    
    <item>
      <title>RKE安装kubernetes集群</title>
      <link>https://tangxusc.github.io/blog/2019/03/rke%E5%AE%89%E8%A3%85kubernetes%E9%9B%86%E7%BE%A4/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/rke%E5%AE%89%E8%A3%85kubernetes%E9%9B%86%E7%BE%A4/</guid>
      <description>RKE安装kubernetes集群 准备工作  1,将普通用户加入到Docker组
sudo usermod -aG docker 用户名  2,关闭防火墙
sudo ufw disable  3.0 安装openssh-server(如果已经安装,可省略)
#centos yum install openssh-server #ubuntu apt-get install openssh-server  3,建立ssh单向通道
ssh-keygen #三次回车，生成ssh公钥和私钥文件 ssh-copy-id &amp;lt;节点用户名&amp;gt;@&amp;lt;节点IP&amp;gt; 例如: ssh-copy-id demo@1.2.3.4  4,验证ssh
ssh 192.168.3.162 exit  5,禁用selinux(CentOS7)
vi /etc/sysconfig/selinux 设置SELINUX=disabled  6,禁用swap
vi /etc/fstab swap那句话注释掉,重启  安装步骤 1,从github rke的仓库中下载rke文件
https://github.com/rancher/rke/releases/  2,在rke同级文件夹下创建cluster.yml
nodes: - address: 节点IP(例如:1.2.3.4) user: 节点用户名(例如:demo) role: [controlplane,worker,etcd] #network: # plugin: flannel # options: # flannel_iface: enp3s0  3,授予执行权限</description>
    </item>
    
    <item>
      <title>Ubuntu 中登录相关的日志</title>
      <link>https://tangxusc.github.io/blog/2019/03/ubuntu-%E4%B8%AD%E7%99%BB%E5%BD%95%E7%9B%B8%E5%85%B3%E7%9A%84%E6%97%A5%E5%BF%97/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/ubuntu-%E4%B8%AD%E7%99%BB%E5%BD%95%E7%9B%B8%E5%85%B3%E7%9A%84%E6%97%A5%E5%BF%97/</guid>
      <description>本文由 简悦 SimpRead 转码， 原文地址 https://www.cnblogs.com/sparkdev/p/7694202.html
 Ubuntu 中登录相关的日志 登录相关的日志涉及到系统的安全，所以是系统管理中非常重要的一部分内容。本文试图对登录相关的日志做一个整理。
/var/log/auth.log 这是一个文本文件，记录了所有和用户认证相关的日志。无论是我们通过 ssh 登录，还是通过 sudo 执行命令都会在 auth.log 中产生记录。执行
$ ssh nick@myserver 上图显示日志记录了通过秘钥认证的方式登录主机并退出的过程。
再执行下面的命令试试：
$ sudo vim /etc/passwd 日志同样会详细的记录本次 sudo 操作的过程。从中我们可以看到是哪个用户通过 sudo 执行了什么命令！
/var/run/utmp 这是一个二进制文件，所以不能直接通过文本编辑器查看其内容。 它记录当前登录的每个用户的信息。因此这个文件会随着用户登录和注销系统而不断变化，它只保留当时联机的用户记录，不会为用户保留永久的记录。系统中需要查询当前用户状态的程序，如 who、w、users 等就需要访问这个文件。
/var/log/wtmp 这是一个二进制文件，所以不能直接通过文本编辑器查看其内容。 该日志文件永久记录每个用户登录、注销及系统的启动、停机的事件。因此随着系统正常运行时间的增加，该文件的大小也会越来越大，增加的速度取决于系统用户登录的次数。该日志文件可以用来查看用户的登录记录，last 命令就通过访问这个文件获得这些信息。
/var/log/btmp 这是一个二进制文件，所以不能直接通过文本编辑器查看其内容。 这个文件记录的是所有失败的登录尝试，使用 last 命令及其 -f 选项可以查看这个文件的内容：
$ sudo last -f /var/log/btmp /var/log/lastlog 这是一个二进制文件，所以不能直接通过文本编辑器查看其内容。 它会记录系统中所有用户最近一次登陆的信息。比如我们通过 ssh 登录时提示的此用户最后一次的登录时间，就是从这个文件中取出的：
其实这个文件的主要使用者是 lastlog 命令。
特别是 /var/run/utmp、/var/log/wtmp 和 /var/log/lastlog 这三个文件，它们都是日志系统中的关键文件，并且具有如下的逻辑联系： 当一个用户登录系统时，login 程序在 lastlog 文件中查看用户的 UID。如果该用户存在，就把该用户上次登录、注销的时间以及从哪个主机登录的信息写到标准输出中。然后 login 程序在 lastlog 中记录新的登录时间，并打开 utmp 文件添加用户本次的登录记录。接下来，login 程序打开 wtmp 文件并添加用户在 utmp 文件中的记录。当用户退出时会把更新的 utmp 文件中的记录添加到 wtmp 文件中，并从 utmp 文件中删除用户的记录。</description>
    </item>
    
    <item>
      <title>docker maven plugin使用</title>
      <link>https://tangxusc.github.io/blog/2019/03/docker-maven-plugin%E4%BD%BF%E7%94%A8/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/docker-maven-plugin%E4%BD%BF%E7%94%A8/</guid>
      <description>docker maven plugin使用 随着容器化的进行，测试环境和线上环境开始尝试容器化发布。因此需要将现有的maven工程进行容器化，容器化的好处不言而喻，但是针对原先没有解耦的应用（容器配置和代码耦合在一起），制作镜像还是有些成本的。本文主要记录对于webx和springboot应用的镜像制作。
springboot镜像制作 springboot制作官方有介绍，最主要的就是在pom.xml中增加docker maven plugin，然后配置读取最终生成的jar即可。
&amp;lt;plugin&amp;gt; &amp;lt;groupId&amp;gt;com.spotify&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;docker-maven-plugin&amp;lt;/artifactId&amp;gt; &amp;lt;configuration&amp;gt; &amp;lt;imageName&amp;gt;aegis-package-switch:1.0&amp;lt;/imageName&amp;gt; &amp;lt;dockerDirectory&amp;gt;${project.basedir}/docker&amp;lt;/dockerDirectory&amp;gt; &amp;lt;resources&amp;gt; &amp;lt;resource&amp;gt; &amp;lt;targetPath&amp;gt;/&amp;lt;/targetPath&amp;gt; &amp;lt;directory&amp;gt;bin&amp;lt;/directory&amp;gt; &amp;lt;include&amp;gt;run.sh&amp;lt;/include&amp;gt; &amp;lt;/resource&amp;gt; &amp;lt;resource&amp;gt; &amp;lt;targetPath&amp;gt;/&amp;lt;/targetPath&amp;gt; &amp;lt;directory&amp;gt;${project.build.directory}&amp;lt;/directory&amp;gt; &amp;lt;include&amp;gt;${project.build.finalName}.jar&amp;lt;/include&amp;gt; &amp;lt;/resource&amp;gt; &amp;lt;/resources&amp;gt; &amp;lt;/configuration&amp;gt; &amp;lt;/plugin&amp;gt;  docker-maven-plugin主要配置有：
 镜像名称 dockerfile文件 需要添加到镜像中的资源  docker-maven-plugin插件本身可以通过xml配置设置类似dockerfile中的简单操作（如添加文件等），但是为了统一和可读性，还是建议统一使用dockerfile。上面的示例中dockerfile位于项目目录的docker子目录中，目录结构类似：
. ├── bin │ └── run.sh ├── docker │ └── Dockerfile ├── pom.xml ├── src │ ├── main │ └── test  resources标签中包含需要添加到镜像中的文件，实际执行时插件会将它们复制到target/docker目录中，供dockerfile使用，否则dockerfile中将无法引用到文件。
然后就是最重要的dockerfile，springboot应用启动比较方便，依赖也很少，只要使用包含java的基础镜像即可。
FROM j8:1.0 RUN mkdir /work WORKDIR /work ADD run.sh /work/run.</description>
    </item>
    
    <item>
      <title>docker入门</title>
      <link>https://tangxusc.github.io/blog/2019/03/docker%E5%85%A5%E9%97%A8/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/docker%E5%85%A5%E9%97%A8/</guid>
      <description>Docker简介 Docker 最初是 dotCloud 公司创始人 Solomon Hykes 在法国期间发起的一个公司内部项目，它是基于 dotCloud 公司多年云服务技术的一次革新，并于 2013 年 3 月以 Apache 2.0 授权协议开源)，主要项目代码在 GitHub 上进行维护。Docker 项目后来还加入了 Linux 基金会，并成立推动开放容器联盟。
Docker 自开源后受到广泛的关注和讨论，至今其 GitHub 项目已经超过 3 万 6 千个星标和一万多个 fork。甚至由于 Docker 项目的火爆，在 2013 年底，dotCloud 公司决定改名为 Docker。Docker 最初是在 Ubuntu 12.04 上开发实现的；Red Hat 则从 RHEL 6.5 开始对 Docker 进行支持；Google 也在其 PaaS 产品中广泛应用 Docker。
Docker 使用 Google 公司推出的 Go 语言 进行开发实现，基于 Linux 内核的 cgroup，namespace，以及 AUFS 类的 Union FS 等技术，对进程进行封装隔离，属于操作系统层面的虚拟化技术。由于隔离的进程独立于宿主和其它的隔离的进程，因此也称其为容器。最初实现是基于 LXC，从 0.7 以后开始去除 LXC，转而使用自行开发的 libcontainer，从 1.</description>
    </item>
    
    <item>
      <title>docker安装(国内环境)</title>
      <link>https://tangxusc.github.io/blog/2019/03/docker%E5%AE%89%E8%A3%85%E5%9B%BD%E5%86%85%E7%8E%AF%E5%A2%83/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/docker%E5%AE%89%E8%A3%85%E5%9B%BD%E5%86%85%E7%8E%AF%E5%A2%83/</guid>
      <description> docker安装(国内环境) curl https://releases.rancher.com/install-docker/17.03.sh | sh  普通用户运行docker命令 usermod -aG docker &amp;lt;用户名&amp;gt;  参照  https://rancher.com/docs/rke/v0.1.x/en/os/ https://rancher.com/docs/rancher/v1.6/en/hosts/#supported-docker-versions  </description>
    </item>
    
    <item>
      <title>drone CI的安装(docker版本)</title>
      <link>https://tangxusc.github.io/blog/2019/03/drone-ci%E7%9A%84%E5%AE%89%E8%A3%85docker%E7%89%88%E6%9C%AC/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/drone-ci%E7%9A%84%E5%AE%89%E8%A3%85docker%E7%89%88%E6%9C%AC/</guid>
      <description>drone CI的安装(docker版本) 因原生的安装使用的是docker-compose进行安装,在此我使用docker进行安装 * drone 版本:0.8.4 * 强烈不推荐现在使用drone,现在感觉非常不可靠(按照文档运行各种错误)
 服务端运行脚本:
docker run -d -p 8000:8000 -v /var/lib/drone:/var/lib/drone/ -e DRONE_OPEN=true -e DRONE_HOST=&amp;lt;访问地址和端口&amp;gt; -e DRONE_GITHUB=true -e DRONE_GITHUB_CLIENT=&amp;lt;DRONE_GITHUB_CLIENT&amp;gt; -e DRONE_GITHUB_SECRET=&amp;lt;DRONE_GITHUB_SECRET&amp;gt; -e DRONE_SECRET=&amp;lt;自定义一个秘钥&amp;gt; --name drone drone/drone:0.8.4   例如:
docker run -d -p 8000:8000 -v /var/lib/drone:/var/lib/drone/ -e DRONE_OPEN=true -e DRONE_HOST=http://10.130.0.159:8000 -e DRONE_GITLAB=true -e DRONE_GITLAB_URL=https://gitlab.com -e DRONE_GITLAB_CLIENT=abba054b74a664f8226a7a99ccbc67f1140739465a4c1b0e85d -e DRONE_GITLAB_SECRET=42149d15d0e1636de99f6bc22c7d18e9f1825dcf49f0b0544dd -e DRONE_SECRET=abcd123456 --name drone drone/drone:0.8.4   代理运行脚本:
docker run -d -v /var/run/docker.sock:/var/run/docker.sock -e DRONE_SERVER=&amp;lt;服务器地址和端口&amp;gt; -e DRONE_SECRET=&amp;lt;自定义的秘钥&amp;gt; --link drone:drone --name drone-agent drone/agent:0.</description>
    </item>
    
    <item>
      <title>fluentd 安装、配置、使用介绍</title>
      <link>https://tangxusc.github.io/blog/2019/03/fluentd-%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%E4%BD%BF%E7%94%A8%E4%BB%8B%E7%BB%8D/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/fluentd-%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%E4%BD%BF%E7%94%A8%E4%BB%8B%E7%BB%8D/</guid>
      <description>本文由 简悦 SimpRead 转码， 原文地址 https://blog.laisky.com/p/fluentd/
 fluentd 安装、配置、使用介绍  fluentd v1.0
 updated at 2016/10/8 updated at 2018/1/9: 完善内容 updated at 2018/1/23: 增加示例，更新为 v1.0 updated at 2018/4/9: 增加负载均衡 updated at 2018/4/25: 增加了负载均衡的一些实践细节   一、简介 fluentd2 是一个针对日志的收集、处理、转发系统。通过丰富的插件系统， 可以收集来自于各种系统或应用的日志，转化为用户指定的格式后，转发到用户所指定的日志存储系统之中。
通过 fluentd，你可以非常轻易的实现像追踪日志文件并将其过滤后转存到 MongoDB 这样的操作。fluentd 可以彻底的将你从繁琐的日志处理中解放出来。
用图来做说明的话，使用 fluentd 以前，你的系统是这样的：
使用了 fluentd 后，你的系统会成为这样：
（图片来源3）
此文将会对 fluentd 的安装、配置、使用等各方面做一个简要的介绍。
fluentd 既可以作为日志收集器安装到每一个结点上， 也可以作为一个服务端收集各个结点上报的日志流。 你甚至也可以在各个结点上都部署 fluentd 收集日志，然后上报到一个 fluentd 集群做统一处理， 然后再转发到最终的日志存储服务器。
所以在一个完整的日志收集、处理系统里，你可以构建一个这样的日志处理流：
Apps (with fluentd/fluent-bit) -&amp;gt; broker (kafka) -&amp;gt; fluentd cluster -&amp;gt; elasticsearch -&amp;gt; kibana  其中提到的 fluent-bit 是一个极简版的 fluentd，专门用作日志的收集和转发， 可以在应用结点上取代 fluentd 收集日志，满足极端的资源要求。</description>
    </item>
    
    <item>
      <title>gRPC-interceptor</title>
      <link>https://tangxusc.github.io/blog/2019/03/grpc-interceptor/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/grpc-interceptor/</guid>
      <description>本文由 简悦 SimpRead 转码， 原文地址 https://colobu.com/2017/04/17/dive-into-gRPC-interceptor/
 gRPC-Go 增加了拦截器 (interceptor) 的功能， 就像 Java Servlet 中的 filter 一样，可以对 RPC 的请求和响应进行拦截处理，而且既可以在客户端进行拦截，也可以对服务器端进行拦截。
利用拦截器，可以对 gRPC 进行扩展，利用社区的力量将 gRPC 发展壮大，也可以让开发者更灵活地处理 gRPC 流程中的业务逻辑。下面列出了利用拦截器实现的一些功能框架：
 Go gRPC Middleware: 提供了拦截器的 interceptor 链式的功能，可以将多个拦截器组合成一个拦截器链，当然它还提供了其它的功能，所以以 gRPC 中间件命名。 grpc-multi-interceptor: 是另一个 interceptor 链式功能的库，也可以将单向的或者流式的拦截器组合。 grpc_auth: 身份验证拦截器 grpc_ctxtags: 为上下文增加Tag map 对象 grpc_zap: 支持zap日志框架 grpc_logrus: 支持logrus日志框架 grpc_prometheus: 支持 prometheus otgrpc: 支持 opentracing/zipkin grpc_opentracing: 支持 opentracing/zipkin grpc_retry: 为客户端增加重试的功能 grpc_validator: 为服务器端增加校验的功能 xrequestid: 将 request id 设置到 context 中 go-grpc-interceptor: 解析Accept-Language并设置到 context requestdump: 输出 request/response  也有其它一些文章介绍的利用拦截器的例子，如下面的两篇文章： Introduction to OAuth on gRPC、gRPC 实践 拦截器 Interceptor</description>
    </item>
    
    <item>
      <title>git subtree 教程</title>
      <link>https://tangxusc.github.io/blog/2019/03/git-subtree-%E6%95%99%E7%A8%8B/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/git-subtree-%E6%95%99%E7%A8%8B/</guid>
      <description>本文由 简悦 SimpRead 转码， 原文地址 https://segmentfault.com/a/1190000012002151
 关于子仓库或者说是仓库共用，git 官方推荐的工具是 git subtree。 我自己也用了一段时间的 git subtree，感觉比 git submodule 好用，但是也有一些缺点，在可接受的范围内。 所以对于仓库共用，在 git subtree 与 git submodule 之中选择的话，我推荐 git subtree。
git subtree 是什么？为什么使用 git subtree git subtree 可以实现一个仓库作为其他仓库的子仓库。 使用 git subtree 有以下几个原因：
 旧版本的 git 也支持 (最老版本可以到 v1.5.2).
 git subtree 与 git submodule 不同，它不增加任何像.gitmodule这样的新的元数据文件.
 git subtree 对于项目中的其他成员透明，意味着可以不知道 git subtree 的存在.
  当然，git subtree 也有它的缺点，但是这些缺点还在可以接受的范围内：
 必须学习新的指令 (如：git subtree).
 子仓库的更新与推送指令相对复杂。</description>
    </item>
    
    <item>
      <title>git 必须要熟练掌握的命令</title>
      <link>https://tangxusc.github.io/blog/2019/03/git-%E5%BF%85%E9%A1%BB%E8%A6%81%E7%86%9F%E7%BB%83%E6%8E%8C%E6%8F%A1%E7%9A%84%E5%91%BD%E4%BB%A4/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/git-%E5%BF%85%E9%A1%BB%E8%A6%81%E7%86%9F%E7%BB%83%E6%8E%8C%E6%8F%A1%E7%9A%84%E5%91%BD%E4%BB%A4/</guid>
      <description>本文由 简悦 SimpRead 转码， 原文地址 https://segmentfault.com/a/1190000013241889
 因为结合了开发中可能遇到的场景，篇幅较长，不过我觉得很有助于你理解 git 的运作机制，而不是死记硬背命令。
HEAD 指针 始终指向的是当前分支的最新版本号，HEAD^, HEAD^^, ^ 的个数 n 或 HEAD~n，n 代表前 n 个版本号。
在项目中直接使用 linux rm 只会删除工作区的文件，git rm 同在删除工作区文件的同时删除 stage 中的，或使用 git rm &amp;ndash;cached 只删除 stage 中的。
一些基本的操作
#全局配置 git config --global user.name &amp;quot;your username&amp;quot; git config --global user.email youremail@email.com git config --global color.ui true # mkdir git_proj &amp;amp; cd git_proj git init echo &amp;quot;# readme.md&amp;quot; &amp;gt;&amp;gt; README.md git add README.</description>
    </item>
    
    <item>
      <title>git常用命令速查表</title>
      <link>https://tangxusc.github.io/blog/2019/03/git%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E9%80%9F%E6%9F%A5%E8%A1%A8/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/git%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E9%80%9F%E6%9F%A5%E8%A1%A8/</guid>
      <description></description>
    </item>
    
    <item>
      <title>git指令列表</title>
      <link>https://tangxusc.github.io/blog/2019/03/git%E6%8C%87%E4%BB%A4%E5%88%97%E8%A1%A8/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/git%E6%8C%87%E4%BB%A4%E5%88%97%E8%A1%A8/</guid>
      <description></description>
    </item>
    
    <item>
      <title>git版本控制最佳实践</title>
      <link>https://tangxusc.github.io/blog/2019/03/git%E7%89%88%E6%9C%AC%E6%8E%A7%E5%88%B6%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/git%E7%89%88%E6%9C%AC%E6%8E%A7%E5%88%B6%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/</guid>
      <description></description>
    </item>
    
    <item>
      <title>golang日志库</title>
      <link>https://tangxusc.github.io/blog/2019/03/golang%E6%97%A5%E5%BF%97%E5%BA%93/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/golang%E6%97%A5%E5%BF%97%E5%BA%93/</guid>
      <description>golang 日志库 golang 标准库的日志框架非常简单，仅仅提供了 print，panic 和 fatal 三个函数对于更精细的日志级别、日志文件分割以及日志分发等方面并没有提供支持。所以催生了很多第三方的日志库，但是在 golang 的世界里，没有一个日志库像 slf4j 那样在 Java 中具有绝对统治地位。golang 中，流行的日志框架包括 logrus、zap、zerolog、seelog 等。 logrus 是目前 Github 上 star 数量最多的日志库，目前 (2018.08，下同)star 数量为 8119，fork 数为 1031。logrus 功能强大，性能高效，而且具有高度灵活性，提供了自定义插件的功能。很多开源项目，如 docker，prometheus 等，都是用了 logrus 来记录其日志。 zap 是 Uber 推出的一个快速、结构化的分级日志库。具有强大的 ad-hoc 分析功能，并且具有灵活的仪表盘。zap 目前在 GitHub 上的 star 数量约为 4.3k。 seelog 提供了灵活的异步调度、格式化和过滤功能。目前在 GitHub 上也有约 1.1k。
logrus 特性 logrus 具有以下特性：
 完全兼容 golang 标准库日志模块：logrus 拥有六种日志级别：debug、info、warn、error、fatal 和 panic，这是 golang 标准库日志模块的 API 的超集。如果您的项目使用标准库日志模块，完全可以以最低的代价迁移到 logrus 上。 可扩展的 Hook 机制：允许使用者通过 hook 的方式将日志分发到任意地方，如本地文件系统、标准输出、logstash、elasticsearch 或者 mq 等，或者通过 hook 定义日志内容和格式等。 可选的日志输出格式：logrus 内置了两种日志格式，JSONFormatter和TextFormatter，如果这两个格式不满足需求，可以自己动手实现接口Formatter，来定义自己的日志格式。 Field 机制：logrus 鼓励通过 Field 机制进行精细化的、结构化的日志记录，而不是通过冗长的消息来记录日志。 logrus 是一个可插拔的、结构化的日志框架。  logrus 的使用 第一个示例 最简单的使用 logrus 的示例如下：</description>
    </item>
    
    <item>
      <title>helm简介</title>
      <link>https://tangxusc.github.io/blog/2019/03/helm%E7%AE%80%E4%BB%8B/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/helm%E7%AE%80%E4%BB%8B/</guid>
      <description>helm简介 简介 helm是k8s中的包管理器,每一个包称为一个chart,helm使用更为便捷的方式来管理我们k8s集群上的应用的安装和分发.
helm下载地址如下: https://github.com/helm/helm/releases
下载解压后,将二进制可执行文件helm放在$path下即可
在helm中有三个重要的概念:
 chart 应用的模板 release 应用的实例,用模板+ values.yaml运行起来的实际应用实例 repo 仓库,存放应用模板的地方  整体的架构图如下:
一个chart实际为一个目录,目录大致如下:
#创建一个mychart的应用 $ helm create mychart #查看应用目录结构 $ tree mychart/ mychart/ ├── charts ├── Chart.yaml	#用于描述这个Chart的相关信息,包括名字,描述信息以及版本等。 ├── templates	#目录下是YAML文件的模板,该模板文件遵循Go template语法。 │ ├── deployment.yaml │ ├── _helpers.tpl │ ├── ingress.yaml │ ├── NOTES.txt │ └── service.yaml └── values.yaml	#用于存储templates目录中模板文件中用到变量的值。\  一个典型的helm chart文件目录如下:
examples/ Chart.yaml # Yaml文件，用于描述Chart的基本信息，包括名称版本等 LICENSE # [可选] 协议 README.md # [可选] 当前Chart的介绍 values.</description>
    </item>
    
    <item>
      <title>jaeger-operator安装</title>
      <link>https://tangxusc.github.io/blog/2019/03/jaeger-operator%E5%AE%89%E8%A3%85/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/jaeger-operator%E5%AE%89%E8%A3%85/</guid>
      <description>jaeger-operator安装 在一个成规模的微服务系统中,一个功能不单由这一个服务完成,而是多个服务协作来共同完成,但是如果其中一个服务出现了错误,对于错误的追踪,对于整个调用链的追踪便成为了难题.
好在外国佬遇到了这些问题,指定了opentracing规范,并且提供了例如zipkin,pinpoint,jaeger等工具供我们使用
jaeger组件如下:
安装 1. elasticsearch jaeger的存储是依赖cassandra或elasticsearch的,jaeger本身并不存储数据,在此处我们使用es来存储数据
$ kubectl apply -f https://gitee.com/tanx/kubernetes-test/raw/master/kubernetes/init-cn/efk-elasticsearch.yaml   elasticsearch 他妈的又需要存储支持,惊不惊喜,意不意外.
 此处建议此es应和k8s中的日志收集使用一个es集群,方便管理,并且追踪数据并不需要支持事务等特性,符合日志存储模式.
2. jaeger operator 在github中提供了operator的安装,直接使用就行
$ kubectl create namespace observability $ kubectl create -f https://raw.githubusercontent.com/jaegertracing/jaeger-operator/master/deploy/crds/jaegertracing_v1_jaeger_crd.yaml $ kubectl create -f https://raw.githubusercontent.com/jaegertracing/jaeger-operator/master/deploy/service_account.yaml $ kubectl create -f https://raw.githubusercontent.com/jaegertracing/jaeger-operator/master/deploy/role.yaml $ kubectl create -f https://raw.githubusercontent.com/jaegertracing/jaeger-operator/master/deploy/role_binding.yaml $ kubectl create -f https://raw.githubusercontent.com/jaegertracing/jaeger-operator/master/deploy/operator.yaml   注意:如果安装在其他命名空间中貌似不行&amp;hellip; 起码我使用helm安装在jaeger命名空间中不行.
 安装成功后等待几分钟就可以看到如下资源的成功运行
$ kubectl get all -n observability NAME READY STATUS RESTARTS AGE pod/jaeger-operator-69c987b98-grv9n 1/1 Running 0 23h NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/jaeger-operator ClusterIP 10.</description>
    </item>
    
    <item>
      <title>jgit获取git仓库中的部分文件</title>
      <link>https://tangxusc.github.io/blog/2019/03/jgit%E8%8E%B7%E5%8F%96git%E4%BB%93%E5%BA%93%E4%B8%AD%E7%9A%84%E9%83%A8%E5%88%86%E6%96%87%E4%BB%B6/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/jgit%E8%8E%B7%E5%8F%96git%E4%BB%93%E5%BA%93%E4%B8%AD%E7%9A%84%E9%83%A8%E5%88%86%E6%96%87%E4%BB%B6/</guid>
      <description>jgit获取git仓库中的部分文件 jgit在获取文件时,只能获取仓库中的全部文件,本文提供一种方法,使用jgit获取仓库中的部分文件.
适用场景:
 用户提交git仓库地址,文件地址,需要读取仓库中的远程文件  原理 在git的命令行中,存在一个ls-remote命令,显示远程存储库中可用的引用以及关联的提交ID
tangxu@tangxu-pc:$ git ls-remote From https://gitee.com/tanx/cavy-platform.git befcac9af04c7822aed0a7f7604d4b9248de3c61	HEAD befcac9af04c7822aed0a7f7604d4b9248de3c61	refs/heads/master  jgit中存在一个CheckoutCommand对象,对象中包含方法
public CheckoutCommand setStartPoint(String startPoint) {//startPoint则为引用id this.checkCallable(); this.startPoint = startPoint; this.startCommit = null; this.checkOptions(); return this; }  和addPath方法
public CheckoutCommand addPath(String path) {//path为文件路径 this.checkCallable(); this.paths.add(path); return this; }  具体实现 话不多述,我们直接上代码:
//url 远程仓库url地址,以.git结尾 //branch 分支名称 private String getRefHash(String url,String branch) throws GitAPIException { Collection&amp;lt;Ref&amp;gt; call = Git.lsRemoteRepository().setRemote(url).call(); for (Ref ref : call) { String[] split = ref.</description>
    </item>
    
    <item>
      <title>kubeadm安装HA集群</title>
      <link>https://tangxusc.github.io/blog/2019/03/kubeadm%E5%AE%89%E8%A3%85ha%E9%9B%86%E7%BE%A4/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/kubeadm%E5%AE%89%E8%A3%85ha%E9%9B%86%E7%BE%A4/</guid>
      <description>kubeadm安装HA集群 鉴于使用二进制的方式安装较为复杂,且不太好处理证书的生成,分发等问题,并且对性能没有较高的要求,所以强烈推荐使用此模式,具体下来这个模式的好处为:
 官方提供的工具,有官方的文档支持 安装贼简单,没有其他的依赖 扩展性强,有官方的一些扩展支持 集群全部以容器启动,所以没那么多你需要管理的service  准备 硬件  一台或多台运行 Ubuntu 16.04 + 的主机(其他linux系统也行) 集群中完整的网络连接，公网或者私网都可以  各节点环境  #### docker  使用加入器安装docker
curl https://releases.rancher.com/install-docker/17.03.sh | sh   #### 禁用swap  然后需要禁用 swap 文件，这是 Kubernetes 的强制步骤。编辑/etc/fstab文件，注释掉引用swap的行，保存并重启后输入sudo swapoff -a即可
vim /etc.fastab #注释调swap的行 sudo swapoff -a   设置源 sudo apt-get update &amp;amp;&amp;amp; sudo apt-get install -y apt-transport-https curl -s https://gitee.com/tanx/kubernetes-test/raw/master/kubeadm/apt-key.gpg | sudo apt-key add - sudo cat &amp;lt;&amp;lt;EOF &amp;gt;/etc/apt/sources.</description>
    </item>
    
    <item>
      <title>kubeadm生成的token过期后，集群增加节点</title>
      <link>https://tangxusc.github.io/blog/2019/03/kubeadm%E7%94%9F%E6%88%90%E7%9A%84token%E8%BF%87%E6%9C%9F%E5%90%8E%E9%9B%86%E7%BE%A4%E5%A2%9E%E5%8A%A0%E8%8A%82%E7%82%B9/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/kubeadm%E7%94%9F%E6%88%90%E7%9A%84token%E8%BF%87%E6%9C%9F%E5%90%8E%E9%9B%86%E7%BE%A4%E5%A2%9E%E5%8A%A0%E8%8A%82%E7%82%B9/</guid>
      <description> kubeadm生成的token过期后，集群增加节点  重新生成token(默认token24小时后过期)
kubeadm token create TOKEN TTL EXPIRES USAGES DESCRIPTION EXTRA GROUPS 36iajs.t016zpxbyqdmivcq 19h 2018-07-04T12:48:32+08:00 authentication,signing The default bootstrap token generated by &#39;kubeadm init&#39;. system:bootstrappers:kubeadm:default-node-token  获取ca证书sha256编码hash值
openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2&amp;gt;/dev/null | openssl dgst -sha256 -hex | sed &#39;s/^.* //&#39;  节点加入集群
kubeadm join &amp;lt;节点IP&amp;gt;:6443 --token 36iajs.t016zpxbyqdmivcq --discovery-token-ca-cert-hash sha256:19246ce11ba3fc633fe0b21f2f8aaaebd7df9103ae47138dc0dd615f61a32d99   </description>
    </item>
    
    <item>
      <title>prometheus和alertmanager监控并发送邮件</title>
      <link>https://tangxusc.github.io/blog/2019/03/prometheus%E5%92%8Calertmanager%E7%9B%91%E6%8E%A7%E5%B9%B6%E5%8F%91%E9%80%81%E9%82%AE%E4%BB%B6/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/prometheus%E5%92%8Calertmanager%E7%9B%91%E6%8E%A7%E5%B9%B6%E5%8F%91%E9%80%81%E9%82%AE%E4%BB%B6/</guid>
      <description>prometheus和alertmanager监控并发送邮件 prometheus 简介 prometheus是时序的监控系统,通常使用prometheus是将prometheus当做采集存储中间件来使用,配合grafana做图表展示,配合alertmanager做自定义告警.
prometheus的每个样本的大小为1-2个字节,要评估服务器的容量,可以使用以下公式:
needed_disk_space = retention_time_seconds * ingested_samples_per_second * bytes_per_sample 需要的硬盘空间 = 数据保留时间(s) * 每秒采集的样本 * 样本大小  数据类型 在系统中所有的数据都以指标的方式来标示,指标包含label,指标格式如下:
&amp;lt;metric name&amp;gt;{&amp;lt;label name&amp;gt;=&amp;lt;label value&amp;gt;, ...} ##例如 api_http_requests_total{method=&amp;quot;POST&amp;quot;, handler=&amp;quot;/messages&amp;quot;}  在系统中,所有指标都在采集时得到其数据类型和描述,通常如下:
$ curl localhost:9090/metrics # HELP go_gc_duration_seconds A summary of the GC invocation durations. # TYPE go_gc_duration_seconds summary go_gc_duration_seconds{quantile=&amp;quot;0&amp;quot;} 2.9632e-05 go_gc_duration_seconds{quantile=&amp;quot;0.25&amp;quot;} 4.7174e-05 go_gc_duration_seconds{quantile=&amp;quot;0.5&amp;quot;} 5.8693e-05 go_gc_duration_seconds{quantile=&amp;quot;0.75&amp;quot;} 9.4042e-05 go_gc_duration_seconds{quantile=&amp;quot;1&amp;quot;} 0.021392614 go_gc_duration_seconds_sum 0.056610034 go_gc_duration_seconds_count 70 # HELP go_goroutines Number of goroutines that currently exist.</description>
    </item>
    
    <item>
      <title>redis配置详解</title>
      <link>https://tangxusc.github.io/blog/2019/03/redis%E9%85%8D%E7%BD%AE%E8%AF%A6%E8%A7%A3/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/redis%E9%85%8D%E7%BD%AE%E8%AF%A6%E8%A7%A3/</guid>
      <description>#redis.conf # Redis configuration file example. # ./redis-server /path/to/redis.conf ################################## INCLUDES ################################### #这在你有标准配置模板但是每个redis服务器又需要个性设置的时候很有用。 # include /path/to/local.conf # include /path/to/other.conf ################################ GENERAL ##################################### #是否在后台执行，yes：后台运行；no：不是后台运行（老版本默认） daemonize yes #3.2里的参数，是否开启保护模式，默认开启。要是配置里没有指定bind和密码。开启该参数后，redis只会本地进行访问，拒绝外部访问。要是开启了密码 和bind，可以开启。否 则最好关闭，设置为no。 protected-mode yes #redis的进程文件 pidfile /var/run/redis/redis-server.pid #redis监听的端口号。 port 6379 #此参数确定了TCP连接中已完成队列(完成三次握手之后)的长度， 当然此值必须不大于Linux系统定义的/proc/sys/net/core/somaxconn值，默认是511，而Linux的默认参数值是128。当系统并发量大并且客户端速度缓慢的时候，可以将这二个参数一起参考设定。该内核参数默认值一般是128，对于负载很大的服务程序来说大大的不够。一般会将它修改为2048或者更大。在/etc/sysctl.conf中添加:net.core.somaxconn = 2048，然后在终端中执行sysctl -p。 tcp-backlog 511 #指定 redis 只接收来自于该 IP 地址的请求，如果不进行设置，那么将处理所有请求 bind 127.0.0.1 #配置unix socket来让redis支持监听本地连接。 # unixsocket /var/run/redis/redis.sock #配置unix socket使用文件的权限 # unixsocketperm 700 # 此参数为设置客户端空闲超过timeout，服务端会断开连接，为0则服务端不会主动断开连接，不能小于0。 timeout 0 #tcp keepalive参数。如果设置不为0，就使用配置tcp的SO_KEEPALIVE值，使用keepalive有两个好处:检测挂掉的对端。降低中间设备出问题而导致网络看似连接却已经与对端端口的问题。在Linux内核中，设置了keepalive，redis会定时给对端发送ack。检测到对端关闭需要两倍的设置值。 tcp-keepalive 0 #指定了服务端日志的级别。级别包括：debug（很多信息，方便开发、测试），verbose（许多有用的信息，但是没有debug级别信息多），notice（适当的日志级别，适合生产环境），warn（只有非常重要的信息） loglevel notice #指定了记录日志的文件。空字符串的话，日志会打印到标准输出设备。后台运行的redis标准输出是/dev/null。 logfile /var/log/redis/redis-server.</description>
    </item>
    
    <item>
      <title>sonarqube使用指南</title>
      <link>https://tangxusc.github.io/blog/2019/03/sonarqube%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/sonarqube%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/</guid>
      <description>sonarqube使用指南 简介 ​ Sonar是一个用于代码质量管理的开源平台，用于管理源代码的质量，可以从多个维度检测代码质量通过插件形式，可以支持包括java,C#,C/C++,PL/SQL,Cobol,JavaScrip,Groovy等等二十几种编程语言的代码质量管理与检测
糟糕的复杂度分布 ​ 文件、类、方法等，如果复杂度过高将难以改变，这会使得开发人员难以理解它们，且如果没有自动化的单元测试，对于程序中的任何组件的改变都将可能导致需要全面的回归测试
重复 ​ 显然程序中包含大量复制粘贴的代码是质量低下的 sonar可以展示源码中重复严重的地方
缺乏单元测试 sonar可以很方便地统计并展示单元测试覆盖率  没有代码标准 ​ sonar可以通过PMD,CheckStyle,Findbugs等等代码规则检测工具规范代码编写
没有足够的或者过多的注释 ​ 没有注释将使代码可读性变差，特别是当不可避免地出现人员变动时，程序的可读性将大幅下降 而过多的注释又会使得开发人员将精力过多地花费在阅读注释上，亦违背初衷
潜在的bug ​ sonar可以通过PMD,CheckStyle,Findbugs等等代码规则检测工具检测出潜在的bug
糟糕的设计 通过sonar可以找出循环，展示包与包、类与类之间的相互依赖关系 可以检测自定义的架构规则 通过sonar可以管理第三方的jar包 可以利用LCOM4检测单个任务规则的应用情况 检测耦合  安装 pg.yaml
apiVersion: apps/v1 kind: Deployment metadata: name: postgres labels: app: postgres spec: replicas: 1 template: metadata: name: postgres labels: app: postgres spec: containers: - name: postgres image: postgres:10 imagePullPolicy: IfNotPresent ports: - containerPort: 5432 env: - name: POSTGRES_USER value: sonar - name: POSTGRES_PASSWORD value: sonar volumeMounts: - mountPath: /var/lib/postgresql/data name: postgres-data restartPolicy: Always volumes: - name: postgres-data emptyDir: {} selector: matchLabels: app: postgres --- apiVersion: v1 kind: Service metadata: name: postgres spec: selector: app: postgres ports: - port: 5432  sonar.</description>
    </item>
    
    <item>
      <title>了解 etcd</title>
      <link>https://tangxusc.github.io/blog/2019/03/%E4%BA%86%E8%A7%A3-etcd/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/%E4%BA%86%E8%A7%A3-etcd/</guid>
      <description>本文由 简悦 SimpRead 转码， 原文地址 https://segmentfault.com/a/1190000008361945
 说明 这是一篇非常入门的文章，让你大概了解一下 etcd。写这篇文章时使用 etcd 的版本是 3.1.0。 etcd 是以实现共享配置和服务发现为目的，提供一致性的键值存储的分布式数据库。kubernetes 等项目使用了 etcd。
下载安装 去这里下载 release 包，解压后是一些文档和两个二进制文件 etcd 和 etcdctl。etcd 是 server 端，etcdctl 是客户端。将 etcd 和 etcdctl 加入 PATH 路径方便我们执行命令。
运行 server 执行命令 etcd，即可启动 server
ming@ming:/tmp$ etcd 2017-02-14 14:04:40.164639 I | etcdmain: etcd Version: 3.1.0 2017-02-14 14:04:40.164725 I | etcdmain: Git SHA: 8ba2897 2017-02-14 14:04:40.164736 I | etcdmain: Go Version: go1.7.4 2017-02-14 14:04:40.164776 I | etcdmain: Go OS/Arch: linux/amd64 2017-02-14 14:04:40.</description>
    </item>
    
    <item>
      <title>五分钟读懂 UML 类图</title>
      <link>https://tangxusc.github.io/blog/2019/03/%E4%BA%94%E5%88%86%E9%92%9F%E8%AF%BB%E6%87%82-uml-%E7%B1%BB%E5%9B%BE/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/%E4%BA%94%E5%88%86%E9%92%9F%E8%AF%BB%E6%87%82-uml-%E7%B1%BB%E5%9B%BE/</guid>
      <description>本文由 简悦 SimpRead 转码， 原文地址 https://www.cnblogs.com/shindo/p/5579191.html
 五分钟读懂 UML 类图 平时阅读一些远吗分析类文章或是设计应用架构时没少与 UML 类图打交道。实际上，UML 类图中最常用到的元素五分钟就能掌握，下面赶紧来一起认识一下它吧：
一、类的属性的表示方式
在 UML 类图中，类使用包含类名、属性 (field) 和方法(method) 且带有分割线的矩形来表示，比如下图表示一个 Employee 类，它包含 name,age 和 email 这 3 个属性，以及 modifyInfo() 方法。
那么属性 / 方法名称前加的加号和减号是什么意思呢？它们表示了这个属性或方法的可见性，UML 类图中表示可见性的符号有三种：
· + ：表示 public
· - ：表示 private
· #：表示 protected（friendly 也归入这类）
因此，上图中的 Employee 类具有 3 个私有属性和一个公有方法。
实际上，属性的完整表示方式是这样的：
可见性 名称 ：类型 [= 缺省值]
中括号中的内容表示是可选的
二、类的方法的表示方式
上图中我们已经看到了方法的表示形式。实际上，方法的完整表示方式如下：
可见性 名称 (参数列表) [ ： 返回类型]
同样，中括号中的内容是可选的。</description>
    </item>
    
    <item>
      <title>从分布式一致性谈到 CAP 理论、BASE 理论</title>
      <link>https://tangxusc.github.io/blog/2019/03/%E4%BB%8E%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%80%E8%87%B4%E6%80%A7%E8%B0%88%E5%88%B0-cap-%E7%90%86%E8%AE%BAbase-%E7%90%86%E8%AE%BA/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/%E4%BB%8E%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%80%E8%87%B4%E6%80%A7%E8%B0%88%E5%88%B0-cap-%E7%90%86%E8%AE%BAbase-%E7%90%86%E8%AE%BA/</guid>
      <description>本文由 简悦 SimpRead 转码， 原文地址 https://www.cnblogs.com/szlbm/p/5588543.html
 从分布式一致性谈到 CAP 理论、BASE 理论 问题的提出
在计算机科学领域，分布式一致性是一个相当重要且被广泛探索与论证问题，首先来看三种业务场景。
1、火车站售票
假如说我们的终端用户是一位经常坐火车的旅行家，通常他是去车站的售票处购买车 票，然后拿着车票去检票口，再坐上火车，开始一段美好的旅行 &amp;mdash;- 一切似乎都是那么和谐。想象一下，如果他选择的目的地是杭州，而某一趟开往杭州的火车 只剩下最后一张车票，可能在同一时刻，不同售票窗口的另一位乘客也购买了同一张车票。假如说售票系统没有进行一致性的保障，两人都购票成功了。而在检票口 检票的时候，其中一位乘客会被告知他的车票无效 &amp;mdash;- 当然，现代的中国铁路售票系统已经很少出现这样的问题了。但在这个例子中我们可以看出，终端用户对 于系统的需求非常简单：
&amp;ldquo;请售票给我，如果没有余票了，请在售票的时候就告诉我票是无效的&amp;rdquo;
这就对购票系统提出了严格的一致性要求 &amp;mdash;- 系统的数据（本例中指的就是那趟开往杭州的火车的余票数）无论在哪个售票窗口，每时每刻都必须是准确无误的！
2、银行转账
假如我们的终端用户是一位刚毕业的大学生，通常在拿到第一个月工资的时候，都会选 择向家里汇款。当他来到银行柜台，完成转账操作后，银行的柜台服务员会友善地提醒他：&amp;rdquo;您的转账将在 N 个工作日后到账！&amp;rdquo;。此时这名毕业生有一定的沮丧， 会对那名柜台服务员叮嘱：&amp;rdquo;好吧，多久没关系，钱不要少就好了！&amp;rdquo;&amp;mdash;- 这也成为了几乎所有用户对于现代银行系统最基本的需求
3、网上购物
假如说我们的终端用户是一位网购达人，当他看见一件库存量为 5 的心仪商品，会迅速地确认购买，写下收货地址，然后下单 &amp;mdash;- 然而，在下单的那个瞬间，系统可能会告知该用户：&amp;rdquo;库存量不足！&amp;rdquo;。此时绝大部分消费者都会抱怨自己动作太慢，使得心爱的商品被其他人抢走了。
但其实有过网购系统开发经验的工程师一定明白，在商品详情页上显示的那个库存量，通常不是该商品的真实库存量，只有在真正下单购买的时候，系统才会检查该商品的真实库存量。但是，谁在意呢？
问题的解读
对于上面三个例子，相信大家一定看出来了，我们的终端用户在使用不同的计算机产品时对于数据一致性的需求是不一样的：
1、有些系统，既要快速地响应用户，同时还要保证系统的数据对于任意客户端都是真实可靠的，就像火车站售票系统
2、有些系统，需要为用户保证绝对可靠的数据安全，虽然在数据一致性上存在延时，但最终务必保证严格的一致性，就像银行的转账系统
3、有些系统，虽然向用户展示了一些可以说是 &amp;ldquo;错误&amp;rdquo; 的数据，但是在整个系统使用过程中，一定会在某一个流程上对系统数据进行准确无误的检查，从而避免用户发生不必要的损失，就像网购系统
分布一致性的提出
在分布式系统中要解决的一个重要问题就是数据的复制。在我们的日常开发经验中，相 信很多开发人员都遇到过这样的问题：假设客户端 C1 将系统中的一个值 K 由 V1 更新为 V2，但客户端 C2 无法立即读取到 K 的最新值，需要在一段时间之后才能 读取到。这很正常，因为数据库复制之间存在延时。
分布式系统对于数据的复制需求一般都来自于以下两个原因：
1、为了增加系统的可用性，以防止单点故障引起的系统不可用
2、提高系统的整体性能，通过负载均衡技术，能够让分布在不同地方的数据副本都能够为用户提供服务
数据复制在可用性和性能方面给分布式系统带来的巨大好处是不言而喻的，然而数据复制所带来的一致性挑战，也是每一个系统研发人员不得不面对的。
所谓分布一致性问题，是指在分布式环境中引入数据复制机制之后，不同数据节点之间 可能出现的，并无法依靠计算机应用程序自身解决的数据不一致的情况。简单讲，数据一致性就是指在对一个副本数据进行更新的时候，必须确保也能够更新其他的 副本，否则不同副本之间的数据将不一致。</description>
    </item>
    
    <item>
      <title>使用 Golang 利用 ectd 实现一个分布式锁</title>
      <link>https://tangxusc.github.io/blog/2019/03/%E4%BD%BF%E7%94%A8-golang-%E5%88%A9%E7%94%A8-ectd-%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/%E4%BD%BF%E7%94%A8-golang-%E5%88%A9%E7%94%A8-ectd-%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/</guid>
      <description>本文由 简悦 SimpRead 转码， 原文地址 https://www.cnblogs.com/diegodu/p/5603678.html
  本文 http://blog.codeg.cn/post/blog/2016-02-24-distrubute-lock-over-etcd/ 是作者 zieckey 在研究和学习相关内容时所做的笔记，欢迎广大朋友指正和交流！ 版权所有，欢迎转载和分享，但请保留此段声明。
etcd是随着CoreOS项目一起成长起来的，随着 Golang 和 CoreOS 等项目在开源社区日益火热， etcd作为一个高可用、强一致性的分布式 Key-Value 存储系统被越来越多的开发人员关注和使用。
这篇文章全方位介绍了 etcd 的应用场景，这里简单摘要如下：
 服务发现（Service Discovery） 消息发布与订阅 负载均衡 分布式通知与协调 分布式锁 分布式队列 集群监控与 Leader 竞选 为什么用 etcd 而不用 ZooKeeper  本文重点介绍如何利用ectd实现一个分布式锁。 锁的概念大家都熟悉，当我们希望某一事件在同一时间点只有一个线程 (goroutine) 在做，或者某一个资源在同一时间点只有一个服务能访问，这个时候我们就需要用到锁。 例如我们要实现一个分布式的 id 生成器，多台服务器之间的协调就非常麻烦。分布式锁就正好派上用场。
其基本实现原理为：
 在 ectd 系统里创建一个 key 如果创建失败，key 存在，则监听该 key 的变化事件，直到该 key 被删除，回到 1 如果创建成功，则认为我获得了锁  具体代码如下：
package etcdsync import ( &amp;quot;fmt&amp;quot; &amp;quot;io&amp;quot; &amp;quot;os&amp;quot; &amp;quot;sync&amp;quot; &amp;quot;time&amp;quot; &amp;quot;github.</description>
    </item>
    
    <item>
      <title>使用 kubeadm 搭建 Kubernetes(1.10.2) 集群（国内环境）</title>
      <link>https://tangxusc.github.io/blog/2019/03/%E4%BD%BF%E7%94%A8-kubeadm-%E6%90%AD%E5%BB%BA-kubernetes1.10.2-%E9%9B%86%E7%BE%A4%E5%9B%BD%E5%86%85%E7%8E%AF%E5%A2%83/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/%E4%BD%BF%E7%94%A8-kubeadm-%E6%90%AD%E5%BB%BA-kubernetes1.10.2-%E9%9B%86%E7%BE%A4%E5%9B%BD%E5%86%85%E7%8E%AF%E5%A2%83/</guid>
      <description>本文由 简悦 SimpRead 转码， 原文地址 https://www.cnblogs.com/RainingNight/p/using-kubeadm-to-create-a-cluster.html
 使用 kubeadm 搭建 Kubernetes(1.10.2) 集群（国内环境） [TOC]
目标  在您的机器上建立一个安全的 Kubernetes 集群。 在集群里安装网络插件，以便应用之间可以相互通讯。 在集群上运行一个简单的微服务。  准备 主机  一台或多台运行 Ubuntu 16.04 + 的主机。 最好选至少有 2 GB 内存的双核主机。 集群中完整的网络连接，公网或者私网都可以。  软件 安装 Docker sudo apt-get update sudo apt-get install -y docker.io  Kubunetes 建议使用老版本的docker.io，如果需要使用最新版的docker-ce，可参考上一篇博客：Docker 初体验。
禁用 swap 文件 然后需要禁用 swap 文件，这是 Kubernetes 的强制步骤。实现它很简单，编辑/etc/fstab文件，注释掉引用swap的行，保存并重启后输入sudo swapoff -a即可。
 对于禁用swap内存，你可能会有点不解，具体原因可以查看 Github 上的 Issue：Kubelet/Kubernetes should work with Swap Enabled。</description>
    </item>
    
    <item>
      <title>使用go mod(1.11)安装grpc</title>
      <link>https://tangxusc.github.io/blog/2019/03/%E4%BD%BF%E7%94%A8go-mod1.11%E5%AE%89%E8%A3%85grpc/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/%E4%BD%BF%E7%94%A8go-mod1.11%E5%AE%89%E8%A3%85grpc/</guid>
      <description>优势 不在使用git clone具体的golang库的源代码
安装较为简单
查看golang版本 $ go version go version go1.11 linux/amd64  因为go1.11才有了go mod 所以在此必须使用1.11版本
配置protoc 在 https://github.com/protocolbuffers/protobuf/releases中下载对应平台的版本到系统中.
版本规则为protoc-&amp;lt;version&amp;gt;-&amp;lt;platform&amp;gt;.zip
$ wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protoc-3.6.1-linux-x86_64.zip $ unzip -o -d /protoc-3.6.1-linux-x86_64 protoc-3.6.1-linux-x86_64.zip  protoc 即Protocol Buffers v3 用于生成gRPC服务代码的protoc编译器
配置环境变量 $ sudo vim /etc/profile export protoc=&amp;lt;刚才解压的路径&amp;gt;/protoc-3.6.1-linux-x86_64 export PATH=$PATH:$GOROOT/bin:$protoc/bin  在其中配置protoc的bin文件夹的位置
测试 $ protoc --version libprotoc 3.6.1  安装插件 因为在国内是访问不到golang官方的一些仓库的,所以此处我们需要建立一个项目 然后让此项目替我们拉取插件.
拉取插件 #创建文件夹 $ mkdir temp &amp;amp;&amp;amp; cd temp #初始化模块 $ go mod init #替换模块中一些依赖包 $ go mod edit -replace=golang.</description>
    </item>
    
    <item>
      <title>使用nfs作为k8s的PersistentVolume</title>
      <link>https://tangxusc.github.io/blog/2019/03/%E4%BD%BF%E7%94%A8nfs%E4%BD%9C%E4%B8%BAk8s%E7%9A%84persistentvolume/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/%E4%BD%BF%E7%94%A8nfs%E4%BD%9C%E4%B8%BAk8s%E7%9A%84persistentvolume/</guid>
      <description>使用nfs作为k8s的PersistentVolume 在较小规模的生产和开发的过程中,对于k8s的某些应用可能我们需要提供存储的支持,在初期我们可能并不需要性能那么高,扩展性那么强的存储,那么这个时候nfs就成了我们的首选
本文将引导各位在服务器中部署nfs服务,并在k8s中使用nfs服务
准备 nfs server 一台
k8s集群 一台
NFS server配置 安装nfs,并启动 $ yum install -y nfs-utils ##关闭防火墙 $ systemctl disable firewalld $ systemctl stop firewalld ##开启nfs自动启动 $ systemctl enable rpcbind.service $ systemctl enable nfs-server.service ## 启动nfs $ systemctl start rpcbind.service $ systemctl start nfs-server.service  配置nfs 编辑文件/etc/exports设置nfs需要暴露的文件夹
$ vim /etc/exports #添加 此处暴露的是/home/nfsdata目录 /home/nfsdata *(insecure,rw,sync,no_root_squash) ###修改暴露的目录的权限 $ chmod 777 -R /home/nfsdata ##重启nfs $ systemctl restart nfs.service  验证 $ showmount -e 10.</description>
    </item>
    
    <item>
      <title>使用rook搭建存储集群</title>
      <link>https://tangxusc.github.io/blog/2019/03/%E4%BD%BF%E7%94%A8rook%E6%90%AD%E5%BB%BA%E5%AD%98%E5%82%A8%E9%9B%86%E7%BE%A4/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/%E4%BD%BF%E7%94%A8rook%E6%90%AD%E5%BB%BA%E5%AD%98%E5%82%A8%E9%9B%86%E7%BE%A4/</guid>
      <description>使用rook搭建存储集群 rook是云原生的存储协调器,为各种存储提供解决方案,提供自我管理,自我扩展,自我修复的存储服务,在kubernetes中实际实现是operator方式.
在rook 0.9版本中,Ceph已经是beta支持状态了.
Ceph是一种高度可扩展的分布式存储解决方案，适用于具有多年生产部署的块存储，对象存储和共享文件系统.
安装 rook提供了operator的方式来处理ceph存储的安装,所以此处安装使用helm来安装rook
helm repo add rook-stable https://charts.rook.io/stable helm install --namespace rook-ceph-system rook-stable/rook-ceph  ceph集群 安装了rook后,还需要在rook中声明CRD来建立ceph集群
cluster.yaml
apiVersion: v1 kind: Namespace metadata: name: rook-ceph --- apiVersion: v1 kind: ServiceAccount metadata: name: rook-ceph-osd namespace: rook-ceph --- apiVersion: v1 kind: ServiceAccount metadata: name: rook-ceph-mgr namespace: rook-ceph --- kind: Role apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: rook-ceph-osd namespace: rook-ceph rules: - apiGroups: [&amp;quot;&amp;quot;] resources: [&amp;quot;configmaps&amp;quot;] verbs: [ &amp;quot;get&amp;quot;, &amp;quot;list&amp;quot;, &amp;quot;watch&amp;quot;, &amp;quot;create&amp;quot;, &amp;quot;update&amp;quot;, &amp;quot;delete&amp;quot; ] --- kind: Role apiVersion: rbac.</description>
    </item>
    
    <item>
      <title>使用shipyard proxy开启docker remote远程端口(2375)</title>
      <link>https://tangxusc.github.io/blog/2019/03/%E4%BD%BF%E7%94%A8shipyard-proxy%E5%BC%80%E5%90%AFdocker-remote%E8%BF%9C%E7%A8%8B%E7%AB%AF%E5%8F%A32375/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/%E4%BD%BF%E7%94%A8shipyard-proxy%E5%BC%80%E5%90%AFdocker-remote%E8%BF%9C%E7%A8%8B%E7%AB%AF%E5%8F%A32375/</guid>
      <description> 使用shipyard proxy开启docker remote远程端口(2375) 优势  不用重启docker 操作更为便捷 对于端口的定制更为方便 没有复杂的linux操作  * 注意shipyard已经没有维护了(但此镜像任然可以使用) 原理: shipyard/docker-proxy:latest 镜像可以在宿主机docker上运行一个容器,容器内挂载/var/run/docker.sock文件 将此文件暴露后,就开启了docker remote api
docker命令行运行 docker run -d -v /var/run/docker.sock:/var/run/docker.sock -p 2375:2375 --name shipyard-docker-proxy shipyard/docker-proxy:latest  rancher调度(v2) version: &#39;2&#39; services: proxy: image: shipyard/docker-proxy:latest stdin_open: true volumes: - /var/run/docker.sock:/var/run/docker.sock tty: true ports: - 2375:2375/tcp labels: io.rancher.container.pull_image: always  </description>
    </item>
    
    <item>
      <title>傻瓜式的 go modules 的讲解和代码</title>
      <link>https://tangxusc.github.io/blog/2019/03/%E5%82%BB%E7%93%9C%E5%BC%8F%E7%9A%84-go-modules-%E7%9A%84%E8%AE%B2%E8%A7%A3%E5%92%8C%E4%BB%A3%E7%A0%81/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/%E5%82%BB%E7%93%9C%E5%BC%8F%E7%9A%84-go-modules-%E7%9A%84%E8%AE%B2%E8%A7%A3%E5%92%8C%E4%BB%A3%E7%A0%81/</guid>
      <description>本文由 简悦 SimpRead 转码， 原文地址 https://it.520mwx.com/view/10804
 一 国内关于 gomod 的文章，哪怕是使用了百度 -csdn，依然全是理论，虽然 golang 的使用者大多是大神但是也有像我这样的的弱鸡是不是？
所以，我就写个傻瓜式教程了。
二 1. 新建文件夹 go_moudiules_demo
2.go mod 之，生成 gomod.go 文件
go mod init go_moudiules_demo 语法 go mod init [module] 3. 创建 main.go，默认包名是 gomod，需要改成 main
4. 创建正真的存放代码的文件夹 demo 和文件 gomod.go，注意不能与 main 放在同一文件夹下，因为会造成包名冲突
 5. 根据规则引入代码，这里有个坑，因为 goland 做的不太好，实际上 golang 的所有工具都做的不太好，导致代码报红，但是实际上 go build/run 还是能跑通的
 当然 goland 也可以配置，就是不知道怎么去红名。。。　三 总结 gomod 最容易让人进了误区就是，把自己之前的代码都 gomod 一次，那么后面使用的时候直接根据 gomod 的 package 找之前的代码，简直美滋滋。</description>
    </item>
    
    <item>
      <title>再探go modules使用与细节</title>
      <link>https://tangxusc.github.io/blog/2019/03/%E5%86%8D%E6%8E%A2go-modules%E4%BD%BF%E7%94%A8%E4%B8%8E%E7%BB%86%E8%8A%82/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/%E5%86%8D%E6%8E%A2go-modules%E4%BD%BF%E7%94%A8%E4%B8%8E%E7%BB%86%E8%8A%82/</guid>
      <description>还有半个月 go1.12 就要发布了。这是首个将 go modules 纳入正式支持的稳定版本。
 距离 go modules 随着 go1.11 正式面向广大开发者进行体验也已经过去了半年，这段时间 go modules 也发生了一些变化，借此机会我想再次深入探讨 go modules 的使用，同时对这个新生包管理方案做一些思考。
版本控制和语义化版本 包的版本控制总是一个包管理器绕不开的古老话题，自然对于我们的 go modules 也是这样。
我们将学习一种新的版本指定方式，然后深入地探讨一下 golang 官方推荐的semver即语义化版本。
控制包版本 在讨论 go get 进行包管理时我们曾经讨论过如何对包版本进行控制（文章在此），支持的格式如下：
vX.Y.Z-pre.0.yyyymmddhhmmss-abcdefabcdef vX.0.0-yyyymmddhhmmss-abcdefabcdef vX.Y.(Z+1)-0.yyyymmddhhmmss-abcdefabcdef vX.Y.Z  在 go.mod 文件中我们也需要这样指定，否则 go mod 无法正常工作，这带来了 2 个痛点：
 目标库需要打上符合要求的 tag，如果 tag 不符合要求不排除日后出现兼容问题（目前来说只要正确指定 tag 就行，唯一的特殊情况在下一节介绍） 如果目标库没有打上 tag，那么就必须毫无差错的编写大串的版本信息，大大加重了使用者的负担  基于以上原因，现在可以直接使用 commit 的 hash 来指定版本，如下：
# 使用go get时 go get github.com/mqu/go-notify@ef6f6f49 # 在go.mod中指定 module my-module require ( // other packages github.</description>
    </item>
    
    <item>
      <title>分布式事务Saga模式</title>
      <link>https://tangxusc.github.io/blog/2019/03/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1saga%E6%A8%A1%E5%BC%8F/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1saga%E6%A8%A1%E5%BC%8F/</guid>
      <description>分布式事务Saga模式 两阶段提交2PC是分布式事务中最强大的事务类型之一，两段提交就是分两个阶段提交，第一阶段询问各个事务数据源是否准备好，第二阶段才真正将数据提交给事务数据源，当需要同时更新多个数据源实体时，例如确认订单并立即更新库存时，它非常有用。
但是，当您使用微服务时，情况会变得复杂。每个服务都拥有自己的数据库系统，每个服务不能再越过别的服务直接访问那个服务的数据库了。
比如服务A有自己的数据库O1，服务B有自己的数据库O2，服务B如果想同时更新数据库O1和O2，就不能越过服务B直接操作数据库O1，而两段提交正是适合这样的场合，服务B可以在一个JTA事务中同时调用数据库O1的XA数据源JNDI，再调用数据库O2的XA数据源JNDI，那么O1和O2的数据就会实现两段提交，O1数据库执行修改或插入操作后其实没有保存到数据库，只有等O2数据库执行修改或插入操作后，才在第二阶段提交确认保存，这个过程如果有任何出错，数据库O1和数据库O2如同没有执行修改或插入一样，两者数据状态是一致的。
因此，在微服务架构中，因为一个服务不能越过其他服务直接访问它们的数据库，两段提交可能不适用，当然EJB提供了基于容器的跨服务分布式事务，虽然听起来很容易，但是因为是同步操作，对网络硬件要求比较高，一旦发生事务出错，需要手工介入数据库进行强制回滚，如果跨N个服务调用出错，出错定位是非常困难的，很难判断问题出在哪个服务器或哪段通讯上，不可能进行时间和空间的同时定位。
基于以上原因，本文介绍Saga模式是一种分布式异步事务，一种最终一致性事务，是一种柔性事务，当然从传统ACID同步事务过渡到异步事务需要很多思维方式切换和步骤证明，可见本站其他文章，这里。
下面以电子商务案例说明Saga模式实现：
API网关 后面是四个微服务：OrderService(订购) StockService(库存) PaymentService(支付) 和DeliveryService(货运)
在这个例子中，我们不能将“下订单，向客户收费，更新库存”这几个动作放入一个ACID事务中。但是又必须一致地执行这整个流程，这就需要创建一个分布式事务。
我们都知道，实施分布式任务是多么困难，而且不幸的是，事务也不例外。处理瞬态状态，服务之间的最终一致性，隔离和回滚是应该在设计阶段考虑的情况。
幸运的是，我们已经为它提出了一些很好的模式，因为我们已经实施了二十多年的分布式事务。今天我想谈论的那个叫做Saga模式。
Saga是最着名的分布式事务模式之一。关于它的第一篇文章早在1987年就已经发表了， 从那以后它一直是一个受欢迎的解决方案。
Saga是一系列本地交易，每笔事务都会更新单个服务中的数据。第一个事务由系统外部请求启动，然后每个后续步骤由前一个事件完成而触发。
对于我们这个电子商务示例，非常高层次级的Saga设计实现如下所示：
现在有两种不同的方式来实现saga事务，最流行的两种方式是：
 事件/编排Choreography：没有中央协调器（没有单点风险）时，每个服务产生并聆听其他服务的事件，并决定是否应采取行动。
 命令/协调orchestrator：中央协调器负责集中处理事件的决策和业务逻辑排序。
  让我们对这两个实现进行更深入的了解，以了解它们的工作方式。
事件/编排Events/Choreography 在Events/Choreography方法中，第一个服务执行一个事务，然后发布一个事件。该事件被一个或多个服务进行监听，这些服务再执行本地事务并发布（或不发布）新的事件。
当最后一个服务执行本地事务并且不发布任何事件时，意味着分布式事务结束，或者它发布的事件没有被任何Saga参与者听到都意味着事务结束。
让我们看看它在我们的电子商务示例中的外观：
步骤如下： 1. 订单服务保存新订单，将状态设置为pengding挂起状态，并发布名为ORDER_CREATED_EVENT的事件。
 支付服务监听ORDER_CREATED_EVENT，并公布事件BILLED_ORDER_EVENT。
 库存服务监听BILLED_ORDER_EVENT，更新库存，并发布ORDER_PREPARED_EVENT。
 货运服务监听ORDER_PREPARED_EVENT，然后交付产品。最后，它发布ORDER_DELIVERED_EVENT
 最后，订单服务侦听ORDER_DELIVERED_EVENT并设置订单的状态为concluded完成。
  在上面的情况下，如果需要跟踪订单的状态，订单服务可以简单地监听所有事件并更新其状态。 在这个案例中，除了订单服务以外的其他服务都是订单服务的子服务，也就是说，为完成一个订单服务，需要经过这些步骤，订单服务与这些服务是包含与被包含关系，因此，订单服务在业务上天然是一个协调器。
回滚分布式事务并不是免费的。通常情况下，您必须实施额外操作才能弥补以前所做的工作。
假设库存服务在事务过程中失败了。让我们看看回滚是什么样子的：
1.库存服务产生PRODUCT_OUT_OF_STOCK_EVENT ;
2.订购服务和支付服务会监听到上面库存服务的这一事件： 1. 支付服务会退款给客户。 2. 订单服务将订单状态设置为失败。
请注意，为每个事务定义一个公共共享ID非常重要，因此每当您抛出一个事件时，所有侦听器都可以立即知道它引用的是哪个事务。
saga事件/编排设计的优点和缺点 事件/编排是实现Saga模式的自然方式; 它很简单，容易理解，不需要太多的努力来构建，所有参与者都是松散耦合的，因为他们彼此之间没有直接的耦合。如果您的事务涉及2至4个步骤，则可能是非常合适的。
但是，如果您在事务中不断添加额外步骤，则此方法可能会很快变得混乱，因为很难跟踪哪些服务监听哪些事件。此外，它还可能在服务之间添加循环依赖，因为它们必须订阅彼此的事件。
最后，使用这种设计来实现测试将会非常棘手。为了模拟交易行为，您应该运行所有服务。
Saga的命令/协调模式 这里我们定义了一项新服务，全权负责告诉每个参与者该做什么以及什么时候该做什么。saga协调器orchestrator以命令/回复的方式与每项服务进行通信，告诉他们应该执行哪些操作。
 订单服务保存pending状态，并要求订单Saga协调器（简称OSO）开始启动订单事务。
 OSO向收款服务发送执行收款命令，收款服务回复Payment Executed消息
 OSO向库存服务发送准备订单命令，库存服务将回复OrderPrepared消息</description>
    </item>
    
    <item>
      <title>前端加载私有文件随想</title>
      <link>https://tangxusc.github.io/blog/2019/03/%E5%89%8D%E7%AB%AF%E5%8A%A0%E8%BD%BD%E7%A7%81%E6%9C%89%E6%96%87%E4%BB%B6%E9%9A%8F%E6%83%B3/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/%E5%89%8D%E7%AB%AF%E5%8A%A0%E8%BD%BD%E7%A7%81%E6%9C%89%E6%96%87%E4%BB%B6%E9%9A%8F%E6%83%B3/</guid>
      <description>前端加载私有文件随想 在前后端分离的工程中,前后端通信链路由前端调用后端,变成了前端&amp;ndash;&amp;gt;api gateway&amp;ndash;&amp;gt;后端,那么前端在加载一些私有资源(需要权限,需要认证)等文件时会存在无法传入token的问题 本文是针对前端加载资源的随想
 未前后端分离调用链路
前端 ------&amp;gt; 后端 ------&amp;gt; 返回结果到前端  前后端分离调用链路
前端 ------&amp;gt; API gateway ------&amp;gt; 后端 ------&amp;gt; 前端   解决方案列表 在前端加载一些文件,例如图片,浏览器会根据image标签的src属性自动获取图片的二进制流,浏览器在加载图片时,不会带上token在header中,一切都是浏览器默认行为,所以此处无法利用浏览器默认行为加载私有文件,所以提出了以下几种解决方案.
 前端ajax加载,后端返回base64,前端再做处理 前端在url中加入token,传入到后端 前端获取token,存入cookie,后端根据优先级来获取token  下面我们分别来说说这三种方案.
前端ajax加载,后端返回base64,前端再做处理 链路图如下:
前端ajax ------&amp;gt; API gateway ------&amp;gt; 后端base64 ------&amp;gt; 前端设置图片base64  前端:使用ajax加载图片,ajax中带上token等认证信息 后端:在接收到请求后,将图片转为base64字符串,通过response返回至前端 前端:在接收到后端返回后,将base64设置至相关的image中
 优点: 遵循原有token机制 可解决加载机制
 缺点:
 base64在后端转换增加了后端压力
 base64在前端处理较为麻烦(如果有大量图片,容易产生阻塞)
 整个链路复杂度太高了,涉及到很多流的操作
 其他类型的文件无法满足此需求
  前端在url中加入token,传入到后端 链路图如下:
前端修改图片url ------&amp;gt; API gateway ------&amp;gt; 后端返回图片二进制  前端:在加载图片时,先修改图片的url,带上token参数 后端:在接收到请求后返回图片二进制</description>
    </item>
    
    <item>
      <title>在golang中创建调用图</title>
      <link>https://tangxusc.github.io/blog/2019/03/%E5%9C%A8golang%E4%B8%AD%E5%88%9B%E5%BB%BA%E8%B0%83%E7%94%A8%E5%9B%BE/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/%E5%9C%A8golang%E4%B8%AD%E5%88%9B%E5%BB%BA%E8%B0%83%E7%94%A8%E5%9B%BE/</guid>
      <description>本文由 简悦 SimpRead 转码， 原文地址 https://stackoverflow.com/questions/31362332/creating-call-graph-in-golang
 我正在寻找为golang项目生成调用图的可能性。类似于Doxygen的 C ++类图功能（使用选项CALL_GRAPH = YES）。
到目前为止我找到了
http://saml.rilspace.org/profiling-and-creating-call-graphs-for-go-programs-with-go-tool-pprof 或 http://blog.golang.org/profiling-go-programs
这会在程序运行时每秒100次对程序的调用堆栈进行采样，并创建一个对分析有用的图形。如果你的程序大部分时间都花在与你无关的函数上，我发现这个解决方案不是很有用。
然后是这样的：
https://godoc.org/golang.org/x/tools/go/callgraph/static
从它的描述听起来像我需要的，但似乎没有文档，我不明白如何使用它。
我也找到了
https://github.com/davecheney/graphpkg/blob/master/README.md 状语从句： https://github.com/paetzke/go-dep-graph/blob/master/README.org
但他们只创建依赖图。
戴夫C.
你和&amp;hellip;&amp;hellip;很接近/x/tools/go/callgraph/static。我很确定go install golang.org/x/tools/cmd/callgraph你想要的是什么。一旦安装，运行它没有参数，以查看它的完整帮助/用法。
（一般来说，&amp;hellip;&amp;hellip; /x/tools/下面的东西是有点可重用的包，命令行前端生活在&amp;hellip;下面/x/tools/cmd，你可以安装它们go install golang.org/x/tools/cmd/...，文字/...匹配所有的子包）。
例如，只运行callgraph生成以下开头和结尾的使用输出：
callgraph：显示Go程序的调用图。
用法：
callgraph [-algo=static|cha|rta|pta] [-test] [-format=...] &amp;lt;args&amp;gt;...
标志：
-algo 指定调用图构造算法，其中之一是：
 static static calls only (unsound) cha Class Hierarchy Analysis rta Rapid Type Analysis pta inclusion-based Points-To Analysis The algorithms are ordered by increasing precision in their treatment of dynamic calls (and thus also computational cost).</description>
    </item>
    
    <item>
      <title>基于 go&#43;etcd 实现分布式锁</title>
      <link>https://tangxusc.github.io/blog/2019/03/%E5%9F%BA%E4%BA%8E-go-etcd-%E5%AE%9E%E7%8E%B0%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/%E5%9F%BA%E4%BA%8E-go-etcd-%E5%AE%9E%E7%8E%B0%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/</guid>
      <description>本文由 简悦 SimpRead 转码， 原文地址 https://www.jianshu.com/p/d3068d0ac7c1
 package main import ( &amp;quot;context&amp;quot; &amp;quot;fmt&amp;quot; &amp;quot;go.etcd.io/etcd/clientv3&amp;quot; &amp;quot;time&amp;quot; ) func main() { var ( config clientv3.Config client *clientv3.Client lease clientv3.Lease leaseResp *clientv3.LeaseGrantResponse leaseId clientv3.LeaseID leaseRespChan &amp;lt;-chan *clientv3.LeaseKeepAliveResponse err error ) //客户端配置 config = clientv3.Config{ Endpoints: []string{&amp;quot;127.0.0.1:2379&amp;quot;}, DialTimeout: 5 * time.Second, } //建立连接 if client, err = clientv3.New(config); err != nil { fmt.Println(err) return } //上锁（创建租约，自动续租） lease = clientv3.NewLease(client) //设置一个ctx取消自动续租 ctx,cancleFunc := context.WithCancel(context.TODO()) //设置10秒租约（过期时间） if leaseResp,err = lease.</description>
    </item>
    
    <item>
      <title>如何一步一步用 DDD 设计一个电商网站</title>
      <link>https://tangxusc.github.io/blog/2019/03/%E5%A6%82%E4%BD%95%E4%B8%80%E6%AD%A5%E4%B8%80%E6%AD%A5%E7%94%A8-ddd-%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E7%94%B5%E5%95%86%E7%BD%91%E7%AB%99/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/%E5%A6%82%E4%BD%95%E4%B8%80%E6%AD%A5%E4%B8%80%E6%AD%A5%E7%94%A8-ddd-%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E7%94%B5%E5%95%86%E7%BD%91%E7%AB%99/</guid>
      <description> 本文由 简悦 SimpRead 转码， 原文地址 http://www.cnblogs.com/Zachary-Fan/p/5991674.html
 本系列所有文章
如何一步一步用 DDD 设计一个电商网站（一）—— 先理解核心概念
如何一步一步用 DDD 设计一个电商网站（二）—— 项目架构
如何一步一步用 DDD 设计一个电商网站（三）—— 初涉核心域
如何一步一步用 DDD 设计一个电商网站（四）—— 把商品卖给用户
如何一步一步用 DDD 设计一个电商网站（五）—— 停下脚步，重新出发
如何一步一步用 DDD 设计一个电商网站（六）—— 给购物车加点料，集成售价上下文
如何一步一步用 DDD 设计一个电商网站（七）—— 实现售价上下文
如何一步一步用 DDD 设计一个电商网站（八）—— 会员价的集成
如何一步一步用 DDD 设计一个电商网站（九）—— 小心陷入值对象持久化的坑
如何一步一步用 DDD 设计一个电商网站（十）—— 一个完整的购物车
如何一步一步用 DDD 设计一个电商网站（十一）—— 最后的准备
如何一步一步用 DDD 设计一个电商网站（十二）—— 提交并生成订单
如何一步一步用 DDD 设计一个电商网站（十三）—— 领域事件扩展
 本文只是收集 如何一步一步用 DDD 设计一个电商网站,作为索引使用
~!~
 </description>
    </item>
    
    <item>
      <title>如何使用 go get 下载 gitlab 私有项目</title>
      <link>https://tangxusc.github.io/blog/2019/03/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-go-get-%E4%B8%8B%E8%BD%BD-gitlab-%E7%A7%81%E6%9C%89%E9%A1%B9%E7%9B%AE/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-go-get-%E4%B8%8B%E8%BD%BD-gitlab-%E7%A7%81%E6%9C%89%E9%A1%B9%E7%9B%AE/</guid>
      <description>本文由 简悦 SimpRead 转码， 原文地址 http://holys.im/2016/09/20/go-get-in-gitlab/
 据此 issue，gitlab 7.8 就开始支持 go get private repo。
假设 gitlab 服务是： mygitlab.com
使用方式：
$ go get -v mygitlab.com/user/repo  如果 mygitlab.com 不支持 https, 还得加上 -insecure 参数
$ go get -v -insecure mygitlab.com/user/repo  但是 -insecure 参数是 go 1.5 以后才有的，所以如果低于 1.5 版本，赶紧升级一下吧。
默认需要输入用户名和密码，比较繁琐。 由于 go get 底层实际还是用了 git 去操作。可以配置 .gitconfig 使之用 http =&amp;gt; ssh 的访问方式 (个人感觉就是重写了 url)
$ git config --global url.&amp;quot;git@mygitlab.com:&amp;quot;.insteadOf &amp;quot;http://mygitlab.com/&amp;quot; // 其实就是在 `.</description>
    </item>
    
    <item>
      <title>微服务分布式事务Saga模式简介</title>
      <link>https://tangxusc.github.io/blog/2019/03/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1saga%E6%A8%A1%E5%BC%8F%E7%AE%80%E4%BB%8B/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1saga%E6%A8%A1%E5%BC%8F%E7%AE%80%E4%BB%8B/</guid>
      <description>该文是基于《微服务模式》作者Chris Richardson的QCONSF 2017会议上的PPT文章(这里)和其 Eventuate Tram Saga框架之上，对Saga模式进行的原理性解说，其中包含banq个人经验总结和见解，请从批判性视角看待。Chris Richardson的另外一本书籍《POJO in Action》曾经是帮助Spring成功挑战了EJB2。
在微服务环境下为什么不能使用ACID事务？因为每个微服务都拥有自己的私有数据库，比如订单服务有自己的订单数据库，而客户服务有自己的客户数据库，如果有一个业务操作需要跨订单和客户一起操作，那么一般使用JTA+XA方式跨订单数据库和客户数据库操作：
@ Transactional //事务元注解 public void crossAction(XX){ //事务开始 //这里ORDERS是属于订单服务的私有数据库 SELECT ORDER_TOTAL FROM ORDERS WHERE CUSTOMER_ID = ? //这里CUSTOMERS是属于客户服务的私有数据库 SELECT CREDIT_LIMIT FROM CUSTOMERS WHERE CUSTOMER_ID=? INERT INTO ORDERS ..... //提交事务 }  以上JTA操作如果结合XA数据源配置，将会实现2PC两段事务提交。
通过这段事务操作主要目的是为了维持业务上的不变性约束，比如一个人下订单的总金额不能超过这个人的信用卡授信额度，也就是说：一个人购买的商品总金额只能小于或等于他的信用卡授信额度。
但是，2PC两段提交并不是微服务分布式架构的选择，因为存在单点风险，因为锁也会降低吞吐量。分布式事务如果不结合CAP定理是无法认识清楚，2PC其实只是选择了CAP中CA，虽然CA保证了可靠性，但是忽视网络通讯随时可能堵塞或失败，形成网络分区，反而不可靠，2PC带来的可靠性在分布式环境中是虚幻的。
在分布式系统中，CAP定理是King，CAP定理无论是理论高度或是工程实施高度都是要高于传统事务的，在CAP定理的干预下，传统ACID事务走向了妥协，变成了BASE，也就是走向最终一致性的柔性事务。
Saga是来自于1987年Hector GM和Kenneth Salem论文，从原理上看Saga好像比较简单： 1. 客户端发出订单创建请求createOrder() 2. OrderService会在其内部本地事务进行Order数据库操作，此时订单状态是待确认状态。 3. CustomerService会在其内部本地事务进行信用卡预授权操作，检查订单金额是否超过信用卡授信额度？ 4. OrderService会在上一步确认业务不变性约束得到满足后，再次操作订单数据库状态，将订单状态改为确认状态。
但是，传统2PC/ACID事务中在上面任何一个步骤失败时会使用回滚操作，比如第三步出错，因为是两段提交，所以，第二段就不会进行确认提交，而是进行回滚Rollback，这样订单状态就恢复到当前事务之前的状态，但是在Saga这种BASE模式下,是无法实现像2PC回滚的，因为2PC是同步的，而Saga是异步的。
那么在Saga这种异步模式如何实现客户的及时响应呢？有两种可选方案：首先是当Saga流程全部完成时再发送响应，这样的好处是响应中带有处理结果，但是这样会降低可用性，CAP定理中，分布式环境中满足了C一致性，只能降低了可用性A。
第二种方案是推荐的，也就是在创建Saga之时，并不是等这个Saga流程完成时候，就发送响应给客户端，当然客户端可能只会得到一个事务ID号，并没有得到如期的处理结果，但是这样数据一致性比较弱的情况下，我们能获得很高的可用性A。
客户端可以根据事务ID号再次查询处理结果（通过浏览器异步调用或服务器端推送都可以），比如之前调用createOrder()，获得order的id，然后，根据这个id号调用getOrder(id)，这样就能获得自己创建的订单。在传统同步环境下，这两步其实是在同一个步骤实现的，也就是createOrder()的结果就是一个订单order。
通过UI界面设计可以降低这种不一致性导致的延迟体验： 1. UI会通过异步方式进行查询调用，给用户的体验感觉还是创建订单后返回了一个创建好的订单 2. Saga处理也是可以很快的，小于100毫秒。 3. 如果会花费很长时间，可以显示“正在处理中&amp;hellip;” 4. Saga处理完成后可以采取服务器推送结果到浏览器。
Saga是否实现了ACID？ ACID是原子性 一致性 隔离性和持久性的总称：</description>
    </item>
    
    <item>
      <title>树形结构的数据库表 Schema 设计 - 基于左右值编码</title>
      <link>https://tangxusc.github.io/blog/2019/03/%E6%A0%91%E5%BD%A2%E7%BB%93%E6%9E%84%E7%9A%84%E6%95%B0%E6%8D%AE%E5%BA%93%E8%A1%A8-schema-%E8%AE%BE%E8%AE%A1-%E5%9F%BA%E4%BA%8E%E5%B7%A6%E5%8F%B3%E5%80%BC%E7%BC%96%E7%A0%81/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/%E6%A0%91%E5%BD%A2%E7%BB%93%E6%9E%84%E7%9A%84%E6%95%B0%E6%8D%AE%E5%BA%93%E8%A1%A8-schema-%E8%AE%BE%E8%AE%A1-%E5%9F%BA%E4%BA%8E%E5%B7%A6%E5%8F%B3%E5%80%BC%E7%BC%96%E7%A0%81/</guid>
      <description>本文由 简悦 SimpRead 转码， 原文地址 https://www.cnblogs.com/M-D-Luffy/p/4712846.html
 树形结构的数据库表 Schema 设计  程序设计过程中，我们常常用树形结构来表征某些数据的关联关系，如企业上下级部门、栏目结构、商品分类等等，通常而言，这些树状结构需要借助于数据库完 成持久化。然而目前的各种基于关系的数据库，都是以二维表的形式记录存储数据信息，因此是不能直接将 Tree 存入 DBMS，设计合适的 Schema 及其对 应的 CRUD 算法是实现关系型数据库中存储树形结构的关键。
 理想中树形结构应该具备如下特征：数据存储冗余度小、直观性强；检索遍历过程简单高效；节点增删改查 CRUD 操作高效。无意中在网上搜索到一种很巧妙的 设计，原文是英文，看过后感觉有点意思，于是便整理了一下。本文将介绍两种树形结构的 Schema 设计方案：一种是直观而简单的设计思路，另一种是基于左 右值编码的改进方案。
一、基本数据  本文列举了一个食品族谱的例子进行讲解，通过类别、颜色和品种组织食品，树形结构图如下：
二、继承关系驱动的 Schema 设计  对树形结构最直观的分析莫过于节点之间的继承关系上，通过显示地描述某一节点的父节点，从而能够建立二维的关系表，则这种方案的 Tree 表结构通常设计为：{Node_id,Parent_id}，上述数据可以描述为如下图所示：
 这种方案的优点很明显：设计和实现自然而然，非常直观和方便。缺点当然也是非常的突出：由于直接地记录了节点之间的继承关系，因此对 Tree 的任何 CRUD 操作都将是低效的，这主要归根于频繁的 “递归” 操作，递归过程不断地访问数据库，每次数据库 IO 都会有时间开销。当然，这种方案并非没有用武之 地，在 Tree 规模相对较小的情况下，我们可以借助于缓存机制来做优化，将 Tree 的信息载入内存进行处理，避免直接对数据库 IO 操作的性能开销。
三、基于左右值编码的 Schema 设计  在基于数据库的一般应用中，查询的需求总要大于删除和修改。为了避免对于树形结构查询时的 “递归” 过程，基于 Tree 的前序遍历设计一种全新的无递归查询、无限分组的左右值编码方案，来保存该树的数据。
 第一次看见这种表结构，相信大部分人都不清楚左值（Lft）和右值（Rgt）是如何计算出来的，而且这种表设计似乎并没有保存父子节点的继承关系。但当 你用手指指着表中的数字从 1 数到 18，你应该会发现点什么吧。对，你手指移动的顺序就是对这棵树进行前序遍历的顺序，如下图所示。当我们从根节点 Food 左侧开始，标记为 1，并沿前序遍历的方向，依次在遍历的路径上标注数字，最后我们回到了根节点 Food，并在右边写上了 18。</description>
    </item>
    
    <item>
      <title>源代码就是设计</title>
      <link>https://tangxusc.github.io/blog/2019/03/%E6%BA%90%E4%BB%A3%E7%A0%81%E5%B0%B1%E6%98%AF%E8%AE%BE%E8%AE%A1/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/%E6%BA%90%E4%BB%A3%E7%A0%81%E5%B0%B1%E6%98%AF%E8%AE%BE%E8%AE%A1/</guid>
      <description>源代码就是设计  注：本文摘自《敏捷软件开发：原则、模式与实践》（清华大学出版社，2003 年 9 月版。本人有幸翻译了该书，详见：http://www.china-pub.com/computers/common/info.asp?id=13569）中的附录 D。这是一篇伟大的论文，该文撰写于 1992 年，作者在当时就能有这样的反思，实在是非常了不起。（邓辉 译）
 ​ 至今，我仍能记起当我顿悟并最终产生下面文章时所在的地方。
​ 那是 1986 年的夏天，我在加利福尼亚中国湖海军武器中心担任临时顾问。在这期间，我有幸参加了一个关于 Ada 的研讨会。讨论当中，有一位听众提出了一个具有代表性的问题，“软件开发者是工程师吗？” 我不记得当时的回答，但是我却记得当时并没有真正解答这个问题。于是，我就退出讨论，开始思考我会怎样回答这样一个问题。现在，我无法肯定当时我为什么会记起几乎 10 年前曾经在 Datamation 杂志上阅读过的一篇论文，不过促使我记起的应该是后续讨论中的某些东西。这篇论文阐述了工程师为什么必须是好的作家（我记得该论文谈论就是这个问题——好久没有看了），但是我从该论文中得到的关键一点是：作者认为工程过程的最终结果是文档。换句话说，工程师生产的是文档，不是实物。其他人根据这些文档去制造实物。于是，我就在困惑中提出了一个问题，“除了软件项目正常产生的所有文档以外，还有可以被认为是真正的工程文档的东西吗？” 我给出的回答是，“是的，有这样的文档存在，并且只有一份——源代码。” 把源代码看作是一份工程文档——设计——完全颠覆了我对自己所选择的职业的看法。它改变了我看待一切事情的方式。此外，我对它思考的越多，我就越觉得它阐明了软件项目常常遇到的众多问题。更确切地说，我觉得大多数人不理解这个不同的看法，或者有意拒绝它这样一个事实，就足以说明很多问题。几年后，我终于有机会把我的观点公开发表。C++ Journal 中的一篇有关软件设计的论文促使我给编辑写了一封关于这个主题的信。经过几封书信交换后，编辑 Livleen Singh 同意把我关于这个主题的想法发表为一篇论文。下面就是这篇文章。
什么是软件设计？ ​ 面向对象技术，特别是 C++，似乎给软件界带来了不小的震动。出现了大量的论文和书籍去描述如何应用这项新技术。总的来说，那些关于面向对象技术是否只是一个骗局的问题已经被那些关于如何付出最小的努力即可获得收益的问题所替代。面向对象技术出现已经有一段时间了，但是这种爆炸式的流行却似乎有点不寻常。人们为何会突然关注它呢？对于这个问题，人们给出了各种各样的解释。事实上，很可能就没有单一的原因。也许，把多种因素的结合起来才能最终取得突破，并且这项工作正在进展之中。尽管如此，在软件革命的这个最新阶段中，C++ 本身看起来似乎成为了一个主要因素。同样，对于这个问题，很可能也存在很多种理由，不过我想从一个稍微不同的视角给出一个答案：C++ 之所以变得流行，是因为它使软件设计变得更容易的同时，也使编程变得更容易。虽然这个解释好像有点奇特，但是它却是深思熟虑的结果。在这篇论文中，我就是想要关注一下编程和程序设计之间的关系。
​ 近 10 年来，我一直觉得整个软件行业都没有觉察到做出一个软件设计和什么是真正的软件设计之间的一个微妙的不同点。只要看到了这一点，我认为我们就可以从 C++ 增长的流行趋势中，学到关于如何才能成为更好的软件工程师的意义深远的知识。这个知识就是，编程不是构建软件，而是设计软件。 ​ 几年前，我参见了一个讨论会，其中讨论到软件开发是否是一门工程学科的问题。虽然我不记得了讨论结果，但是我却记得它是如何促使我认识到：软件业已经做出了一些错误的和硬件工程的比较，而忽视了一些绝对正确的对比。其实，我认为我们不是软件工程师，因为我们没有认识到什么才是真正的软件设计。现在，我对这一点更是确信无疑。 ​ 任何工程活动的最终目标都是某些类型的文档。当设计工作完成时，设计文档就被转交给制造团队。该团队是一个和设计团队完全不同的群体，并且其技能也和设计团队完全不同。如果设计文档正确地描绘了一个完整的设计，那么制造团队就可以着手构建产品。事实上，他们可以着手构建该产品的许多实物，完全无需设计者的任何进一步的介入。在按照我的理解方式审查了软件开发的生命周期后，我得出一个结论：实际上满足工程设计标准的惟一软件文档，就是源代码清单。 ​ 对于这个观点，人们进行了很多的争论，无论是赞成的还是反对的都足以写成无数的论文。本文假定最终的源代码就是真正的软件设计，然后仔细研究了该假定带来的一些结果。我可能无法证明这个观点是正确的，但是我希望证明：它确实解释了软件行业中一些已经观察到的事实，包括 C++ 的流行。 ​ 在把代码看作是软件设计所带来的结果中，有一个结果完全盖过了所有其他的结果。它非常重要并且非常明显，也正因为如此，对于大多数软件机构来说，它完全是一个盲点。这个结果就是：软件的构建是廉价的。它根本就不具有昂贵的资格；它非常的廉价，几乎就是免费的。如果源代码是软件设计，那么实际的软件构建就是由编译器和连接器完成的。我们常常把编译和连接一个完整的软件系统的过程称为 “进行一次构建”。在软件构建设备上所进行的主要投资是很少的——实际需要的只有一台计算机、一个编辑器、一个编译器以及一个连接器。一旦具有了一个构建环境，那么实际的软件构建只需花费少许的时间。编译 50 000 行的 C++ 程序也许会花费很长的时间，但是构建一个具有和 50 000 行 C++ 程序同样设计复杂性的硬件系统要花费多长的时间呢？ ​ 把源代码看作是软件设计的另外一个结果是，软件设计相对易于创作，至少在机械意义上如此。通常，编写（也就是设计）一个具有代表性的软件模块（50 至 100 行代码）只需花费几天的时间（对它进行完全的调试是另外一个议题，稍后会对它进行更多的讨论）。我很想问一下，是否还有任何其他的学科可以在如此短的时间内，产生出和软件具有同样复杂性的设计来，不过，首先我们必须要弄清出如何来度量和比较复杂性。然而，有一点是明显的，那就是软件设计可以 极为迅速地变得非常庞大。 ​ 假设软件设计相对易于创作，并且在本质上构建起来也没有什么代价，一个不令人吃惊的发现是，软件设计往往是难以置信的庞大和复杂。这看起来似乎很明显，但是问题的重要性却常常被忽视。学校中的项目通常具有数千行的代码。具有 10 000 行代码（设计）的软件产品被它们的设计者丢弃的情况也是有的。我们早就不再关注于简单的软件。典型的商业软件的设计都是由数十万行代码组成的。许多软件设计达到了上百万行代码。另外，软件设计几乎总是在不断地演化。虽然当前的设计可能只有几千行代码，但是在产品的生命期中，实际上可能要编写许多倍的代码。 ​ 尽管确实存在一些硬件设计，它们看起来似乎和软件设计一样复杂，但是请注意两个有关现代硬件的事实。第一，复杂的硬件工程成果未必总是没有错误的，在这一点上，它不存在像软件那样让我们相信的评判标准。多数的微处理器在发售时都具有一些逻辑错误：桥梁坍塌，大坝破裂，飞机失事以及数以千计的汽车和其他消费品被召回——所有的这些我们都记忆犹新，所有的这些都是设计错误的结果。第二，复杂的硬件设计具有与之对应的复杂、昂贵的构建阶段。结果，制造这种系统所需的能力限制了真正能够生产复杂硬件设计公司的数目。对于软件来说，没有这种限制。目前，已经有数以百计的软件机构和数以千计的非常复杂的软件系统存在，并且数量以及复杂性每天都在增长。这意味着软件行业不可能通过仿效硬件开发者找到针对自身问题的解决办法。倘若一定要说出有什么相同之处的话，那就是，当 CAD 和 CAM 可以做到帮助硬件设计者创建越来越复杂的设计时，硬件工程才会变得和软件开发越来越像。 ​ 设计软件是一种管理复杂性的活动。复杂性存在于软件设计本身之中，存在于公司的软件机构之中，也存在于整个软件行业之中。软件设计和系统设计非常相似。它可以跨越多种技术并且常常涉及多个学科分支。软件的规格说明往往不固定、经常快速变化，这种变化常常在正进行软件设计时发生。同样，软件开发团队也往往不固定，常常在设计过程的中间发生变化。在许多方面，软件都要比硬件更像复杂的社会或者有机系统。所有这些都使得软件设计成为了一个困难的并且易出错的过程。虽然所有这些都不是创造性的想法，但是在软件工程革命开始将近 30 年后的今天，和其他工程行业相比，软件开发看起来仍然像是一种未受过训练（undisciplined）的技艺。 一般的看法认为，当真正的工程师完成了一个设计，不管该设计有多么复杂，他们都非常确信该设计是可以工作的。他们也非常确信该设计可以使用公认的技术建造出来。为了做到这一点，硬件工程师花费了大量的时间去验证和改进他们的设计。例如，请考虑一个桥梁设计。在这样一个设计实际建造之前，工程师会进行结构分析——他们建立计算机模型并进行仿真，他们建立比例模型并在风洞中或者用其他一些方法进行测试。简而言之，在建造前，设计者会使用他们能够想到的一切方法来证实设计是正确的。对于一架新型客机的设计来说，情况甚至更加严重；必须要构建出和原物同尺寸的原型，并且必须要进行飞行测试来验证设计中的种种预计。 ​ 对于大多数人来说，软件中明显不存在和硬件设计同样严格的工程。然而，如果我们把源代码看做是设计，那么就会发现软件工程师实际上对他们的设计做了大量的验证和改进。软件工程师不把这称为工程，而称它为测试和调试。大多数人不把测试和调试看作是真正的 “工程”——在软件行业中肯定没有被看作是。造成这种看法的原因，更多的是因为软件行业拒绝把代码看作设计，而不是任何实际的工程差别。事实上，试验模型、原型以及电路试验板已经成为其他工程学科公认的组成部分。软件设计者之所以不具有或者没有使用更多的正规方法来验证他们的设计，是因为软件构建周期的简单经济规律。 ​ 第一个启示：仅仅构建设计并测试它比做任何其他事情要廉价一些，也简单一些。我们不关心做了多少次构建——这些构建在时间方面的代价几乎为零，并且如果我们丢弃了构建，那么它所使用的资源完全可以重新利用。请注意，测试并非仅仅是让当前的设计正确，它也是改进设计的过程的一部分。复杂系统的硬件工程师常常建立模型（或者，至少他们把设计用计算机图形直观地表现出来）。这就使得他们获得了对于设计的一种 “感觉”，而仅仅去检查设计是不可能获得这种感觉的。对于软件来说，构建这样一个模型既不可能也无必要。我们仅仅构建产品本身。即使正规的软件验证可以和编译器一样自动进行，我们还是会去进行构建 / 测试循环。因此，正规的验证对于软件行业来说从来没有太多的实际意义。 这就是现今软件开发过程的现实。数量不断增长的人和机构正在创建着更加复杂的软件设计。这些设计会被先用某些编程语言编写出来，然后通过构建 / 测试循环进行验证和改进。过程易于出错，并且不是特别的严格。相当多的软件开发人员并不想相信这就是过程的运作方式，也正因为这一点，使问题变得更加复杂。 当前大多数的软件过程都试图把软件设计的不同阶段分离到不同的类别中。必须要在顶层的设计完成并且冻结后，才能开始编码。测试和调试只对清除建造错误是必要的。程序员处在中间位置，他们是软件行业的建造工人。许多人认为，如果我们可以让程序员不再进行 “随意的编码（hacking）” 并且按照交给他们的设计去进行构建（还要在过程中，犯更少的错误），那么软件开发就可以变得成熟，从而成为一门真正的工程学科。但是，只要过程忽视了工程和经济学事实，这就不可能发生。 ​ 例如，任何一个现代行业都无法忍受在其制造过程中出现超过 100% 的返工率。如果一个建造工人常常不能在第一次就构建正确，那么不久他就会失业。但是在软件业中，即使最小的一块代码，在测试和调试期间，也很可能会被修正或者完全重写。在一个创造性的过程中（比如：设计），我们认可这种改进不是制造过程的一部分。没有人会期望工程师第一次就创建出完美的设计。即使她做到了，仍然必须让它经受改进过程，目的就是为了证明它是完美的。 ​ 即使我们从日本的管理方法中没有学到任何东西，我们也应该知道由于在过程中犯错误而去责备工人是无益于提高生产率的。我们不应该不断地强迫软件开发去符合不正确的过程模型，相反，我们需要去改进过程，使之有助于而不是阻碍产生更好的软件。这就是 “软件工程” 的石蕊测试。工程是关于你如何实施过程的，而不是关于是否需要一个 CAD 系统来产生最终的设计文档。 ​ 关于软件开发有一个压倒性的问题，那就是一切都是设计过程的一部分。编码是设计，测试和调试是设计的一部分，并且我们通常认为的设计仍然是设计的一部分。虽然软件构建起来很廉价，但是设计起来却是难以置信的昂贵。软件非常的复杂，具有众多不同方面的设计内容以及它们所导致的设计考虑。问题在于，所有不同方面的内容是相互关连的（就像硬件工程中的一样）。我们希望顶层设计者可以忽视模块算法设计的细节。同样，我们希望程序员在设计模块内部算法时不必考虑顶层设计问题。糟糕的是，一个设计层面中的问题侵入到了其他层面之中。对于整个软件系统的成功来说，为一个特定模块选择算法可能和任何一个更高层次的设计问题同样重要。在软件设计的不同方面内容中，不存在重要性的等级。最低层模块中的一个不正确设计可能和最高层中的错误一样致命。软件设计必须在所有的方面都是完整和正确的，否则，构建于该设计基础之上的所有软件都会是错误的。 ​ 为了管理复杂性，软件被分层设计。当程序员在考虑一个模块的详细设计时，可能还有数以百计的其他模块以及数以千计的细节，他不可能同时顾及。例如，在软件设计中，有一些重要方面的内容不是完全属于数据结构和算法的范畴。在理想情况下，程序员不应该在设计代码时还得去考虑设计的这些其他方面的内容。 ​ 但是，设计并不是以这种方式工作的，并且原因也开始变得明朗。软件设计只有在其被编写和测试后才算完成。测试是设计验证和改进过程的基础部分。高层结构的设计不是完整的软件设计；它只是细节设计的一个结构框架。在严格地验证高层设计方面，我们的能力是非常有限的。详细设计最终会对高层设计造成的影响至少和其他的因素一样多（或者应该允许这种影响）。对设计的各个方面进行改进，是一个应该贯穿整个设计周期的过程。如果设计的任何一个方面内容被冻结在改进过程之外，那么对于最终设计将会是糟糕的或者甚至无法工作这一点，就不会觉得奇怪了。 ​ 如果高层的软件设计可以成为一个更加严格的工程过程，那该有多好呀，但是软件系统的真实情况不是严格的。软件非常的复杂，它依赖于太多的其他东西。或许，某些硬件没有按照设计者认为的那样工作，或者一个库例程具有一个文档中没有说明的限制。每一个软件项目迟早都会遇到这些种类的问题。这些种类的问题会在测试期间被发现（如果我们的测试工作做得好的话），之所以如此是因为没有办法在早期就发现它们。当它们被发现时，就迫使对设计进行更改。如果我们幸运，那么对设计的更改是局部的。时常，更改会波及到整个软件设计中的一些重要部分（莫非定律）。当受到影响的设计的一部分由于某种原因不能更改时，那么为了能够适应影响，设计的其他部分就必须得遭到破坏。这通常导致的结果就是管理者所认为的 “随意编码”，但是这就是软件开发的现实。 ​ 例如，在我最近工作的一个项目中，发现了模块 A 的内部结构和另一个模块 B 之间的一个时序依赖关系。糟糕的是，模块 A 的内部结构隐藏在一个抽象体的后面，而该抽象体不允许以任何方法把对模块 B 的调用合入到它的正确调用序列中。当问题被发现时，当然已经错过了更改 A 的抽象体的时机。正如所料，所发生的就是把一个日益增长的复杂的 “修正” 集应用到 A 的内部设计上。在我们还没有安装完版本 1 时，就普遍感觉到设计正在衰退。每一个新的修正很可能都会破坏一些老的修正。这是一个正规的软件开发项目。最后，我和我的同事决定对设计进行更改，但是为了得到管理层的同意，我们不得不自愿无偿加班。 ​ 在任何一般规模的软件项目中，肯定会出现像这样的问题，尽管人们使用了各种方法来防止它的出现，但是仍然会忽视一些重要的细节。这就是工艺和工程之间的区别。如果经验可以把我们引向正确的方向，这就是工艺。如果经验只会把我们带入未知的领域，然后我们必须使用一开始所使用的方法并通过一个受控的改进过程把它变得更好，这就是工程。 ​ 我们来看一下只是作为其中很小一点的内容，所有的程序员都知道，在编码之后而不是之前编写软件设计文档会产生更加准确的文档。现在，原因是显而易见的。用代码来表现的最终设计是惟一一个在构建 / 测试循环期间被改进的东西。在这个循环期间，初始设计保持不变的可能性和模块的数量以及项目中程序员的数量成反比。它很快就会变得毫无价值。 ​ 在软件工程中，我们非常需要在各个层次都优秀的设计。我们特别需要优秀的顶层设计。初期的设计越好，详细设计就会越容易。设计者应该使用任何可以提供帮助的东西。结构图表、Booch 图、状态表、PDL 等等——如果它能够提供帮助，就去使用它。但是，我们必须记住，这些工具和符号都不是软件设计。最后，我们必须创建真正的软件设计，并且是使用某种编程语言完成的。因此，当我们得出设计时，我们不应该害怕对它们进行编码。在必要时，我们必须应该乐于去改进它们。 ​ 至今，还没有任何设计符号可以同时适用于顶层设计和详细设计。设计最终会表现为以某种编程语言编写的代码。这意味着在详细设计可以开始前，顶层设计符号必须被转换成目标编程语言。这个转换步骤耗费时间并且会引入错误。程序员常常是对需求进行回顾并且重新进行顶层设计，然后根据它们的实际去进行编码，而不是从一个可能没有和所选择的编程语言完全映射的符号进行转换。这同样也是软件开发的部分现实情况。 ​ 也许，如果让设计者本人来编写初始代码，而不是后来让其他人去转换语言无关的设计，就会更好一些。我们所需要的是一个适用于各个层次设计的统一符号。换句话说，我们需要一种编程语言，它同样也适用于捕获高层的设计概念。C＋＋正好可以满足这个要求。C++ 是一门适用于真实项目的编程语言，同时它也是一个非常具有表达力的软件设计语言。C++ 允许我们直接表达关于设计组件的高层信息。这样，就可以更容易地进行设计，并且以后可以更容易地改进设计。由于它具有更强大的类型检查机制，所以也有助于检测到设计中的错误。这就产生了一个更加健壮的设计，实际上也是一个更好的工程化设计。 ​ 最后，软件设计必须要用某种编程语言表现出来，然后通过一个构建 / 测试循环对其进行验证和改进。除此之外的任何其他主张都完全没有用。请考虑一下都有哪些软件开发工具和技术得以流行。结构化编程在它的时代被认为是创造性的技术。Pascal 使之变得流行，从而自己也变得流行。面向对象设计是新的流行技术，而 C++ 是它的核心。现在，请考虑一下那些没有成效的东西。CASE 工具，流行吗？是的；通用吗？不是。结构图表怎么样？情况也一样。同样地，还有 Warner-Orr 图、Booch 图、对象图以及你能想起的一切。每一个都有自己的强项，以及惟一的一个根本弱点——它不是真正的软件设计。事实上，惟一一个可以被普遍认可的软件设计符号是 PDL，而它看起来像什么呢？ ​ 这表明，在软件业的共同潜意识中本能地知道，编程技术，特别是实际开发所使用的编程语言的改进和软件行业中任何其他东西相比，具有压倒性的重要性。这还表明，程序员关心的是设计。当出现更加具有表达力的编程语言时，软件开发者就会使用它们。 ​ 同样，请考虑一下软件开发过程是如何变化的。从前，我们使用瀑布式过程。现在，我们谈论的是螺旋式开发和快速原型。虽然这种技术常常被认为可以 “消除风险” 以及“缩短产品的交付时间”，但是它们事实上也只是为了在软件的生命周期中更早地开始编码。这是好事。这使得构建 / 测试循环可以更早地开始对设计进行验证和改进。这同样也意味着，顶层软件设计者很有可能也会去进行详细设计。 ​ 正如上面所表明的，工程更多的是关于如何去实施过程的，而不是关于最终产品看起来的像什么。处在软件行业中的我们，已经接近工程师的标准，但是我们需要一些认知上的改变。编程和构建 / 测试循环是工程软件过程的中心。我们需要以像这样的方式去管理它们。构建 / 测试循环的经济规律，再加上软件系统几乎可以表现任何东西的事实，就使得我们完全不可能找出一种通用的方法来验证软件设计。我们可以改善这个过程，但是我们不能脱离它。 ​ 最后一点：任何工程设计项目的目标是一些文档产品。显然，实际设计的文档是最重要的，但是它们并非惟一要产生的文档。最终，会期望某些人来使用软件。同样，系统很可能也需要后续的修改和增强。这意味着，和硬件项目一样，辅助文档对于软件项目具有同样的重要性。虽然暂时忽略了用户手册、安装指南以及其他一些和设计过程没有直接联系的文档，但是仍然有两个重要的需求需要使用辅助设计文档来解决。 ​ 辅助文档的第一个用途是从问题空间中捕获重要的信息，这些信息是不能直接在设计中使用的。软件设计需要创造一些软件概念来对问题空间中的概念进行建模。这个过程需要我们得出一个对问题空间中概念的理解。通常，这个理解中会包含一些最后不会被直接建模到软件空间中的信息，但是这些信息却仍然有助于设计者确定什么是本质概念以及如何最好地对它们建模。这些信息应该被记录在某处，以防以后要去更改模型。 ​ 对辅助文档的第二个重要需要是对设计的某些方面的内容进行记录，而这些方面的内容是难以直接从设计本身中提取的。它们既可以是高层方面的内容，也可以是低层方面内容。对于这些方面内容中的许多来说，图形是最好的描述方式。这就使得它们难以作为注释包含在代码中。这并不是说要用图形化的软件设计符号代替编程语言。这和用一些文本描述来对硬件科目的图形化设计文档进行补充没有什么区别。 ​ 决不要忘记，是源代码决定了实际设计的真实样子，而不是辅助文档。在理想情况下，可以使用软件工具对源代码进行后期处理并产生出辅助文档。对于这一点，我们可能期望过高了。次一点的情况是，程序员（或者技术方面的编写者）可以使用一些工具从源代码中提取出一些特定的信息，然后可以把这些信息以其他一些方式文档化。毫无疑问，手工对这种文档保持更新是困难的。这是另外一个支持需要更具表达力的编程语言的理由。同样，这也是一个支持使这种辅助文档保持最小并且尽可能在项目晚期才使之变成正式的理由。同样，我们可以使用一些好的工具；不然的话，我们就得求助于铅笔、纸以及黑板。 ​ 总结如下：  实际的软件运行于计算机之中。它是存储在某种磁介质中的 0 和 1 的序列。它不是使用 C++ 语言（或者其他任何编程语言）编写的程序。  程序清单是代表软件设计的文档。实际上把软件设计构建出来的是编译器和连接器。  构建实际软件设计的廉价程度是令人难以置信的，并且它始终随着计算机速度的加快而变得更加廉价。  设计实际软件的昂贵程度是令人难以置信的，之所以如此，是因为软件的复杂性是令人难以置信的，并且软件项目的几乎所有步骤都是设计过程的一部分。  编程是一种设计活动——好的软件设计过程认可这一点，并且在编码显得有意义时，就会毫不犹豫的去编码。  编码要比我们所认为的更频繁地显现出它的意义。通常，在代码中表现设计的过程会揭示出一些疏漏以及额外的设计需要。这发生的越早，设计就会越好。  因为软件构建起来非常廉价，所以正规的工程验证方法在实际的软件开发中没有多大用处。仅仅建造设计并测试它要比试图去证明它更简单、更廉价。  测试和调试是设计活动——对于软件来说，它们就相当于其他工程学科中的设计验证和改进过程。好的软件设计过程认可这一点，并且不会试图去减少这些步骤。  还有一些其他的设计活动——称它们为高层设计、模块设计、结构设计、构架设计或者诸如此类的东西。好的软件设计过程认可这一点，并且慎重地包含这些步骤。  所有的设计活动都是相互影响的。好的软件设计过程认可这一点，并且当不同的设计步骤显示出有必要时，它会允许设计改变，有时甚至是根本上的改变，  许多不同的软件设计符号可能是有用的——它们可以作为辅助文档以及工具来帮助简化设计过程。它们不是软件设计。  软件开发仍然还是一门工艺，而不是一个工程学科。主要是因为缺乏验证和改善设计的关键过程中所需的严格性。  最后，软件开发的真正进步依赖于编程技术的进步，而这又意味着编程语言的进步。C++ 就是这样的一个进步。它已经取得了爆炸式的流行，因为它是一门直接支持更好的软件设计的主流编程语言。  C++ 在正确的方向上迈出了一步，但是还需要更大的进步。</description>
    </item>
    
    <item>
      <title>用 JGit 通过 Java 来操作 Git</title>
      <link>https://tangxusc.github.io/blog/2019/03/%E7%94%A8-jgit-%E9%80%9A%E8%BF%87-java-%E6%9D%A5%E6%93%8D%E4%BD%9C-git/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/%E7%94%A8-jgit-%E9%80%9A%E8%BF%87-java-%E6%9D%A5%E6%93%8D%E4%BD%9C-git/</guid>
      <description>本文由 简悦 SimpRead 转码， 原文地址 http://qinghua.github.io/jgit/
 文章目录
 1. 概念 2. 准备环境 3. 动手  3.1. 获取仓库 3.2. 常用操作 3.3. 其它对象  4. 参考资料  JGit 是一个由 Eclipse 基金会开发、用于操作 git 的纯 Java 库。它本身也是 Eclispe 的一部分，实际上 Eclipse 的插件 EGit 便是基于 JGit 的。如果你像我这样有使用代码来操作 git 的需求，那就准备好拥抱 JGit 吧。目前来看别的竞品没它靠谱。
概念 从用户指南的概念一节中可以看到，JGit 的基本概念如下：
 Git 对象（Git Objects）：就是 git 的对象。它们在 git 中用 SHA-1 来表示。在 JGit 中用AnyObjectId和ObjectId表示。而它又包含了四种类型：  二进制大对象（blob）：文件数据 树（tree）：指向其它的 tree 和 blob 提交（commit）：指向某一棵 tree 标签（tag）：把一个 commit 标记为一个标签  引用（Ref）：对某一个 git 对象的引用。 仓库（Repository）：顾名思义，就是用于存储所有 git 对象和 Ref 的仓库。 RevWalk：该类用于从 commit 的关系图（graph）中遍历 commit。晦涩难懂？看到范例就清楚了。 RevCommit：表示一个 git 的 commit RevTag：表示一个 git 的 tag RevTree：表示一个 git 的 tree TreeWalk：类似 RevWalk，但是用于遍历一棵 tree  准备环境 让我们从一个最典型的用例开始吧。首先在/tmp/jgit/repo中创建一个 git 仓库：</description>
    </item>
    
    <item>
      <title>自动部署k8s基础应用</title>
      <link>https://tangxusc.github.io/blog/2019/03/%E8%87%AA%E5%8A%A8%E9%83%A8%E7%BD%B2k8s%E5%9F%BA%E7%A1%80%E5%BA%94%E7%94%A8/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/%E8%87%AA%E5%8A%A8%E9%83%A8%E7%BD%B2k8s%E5%9F%BA%E7%A1%80%E5%BA%94%E7%94%A8/</guid>
      <description>自动部署k8s基础应用 在k8s集群安装完成后,我们需要为集群安装很多初始应用,此处通过一个简易脚本方式安装以下应用:
 helm-tiller kubeapp rook rook-cluster elasticsearch-fluentd-kinaba prometheus metrics-server jaeger dashboard  该脚本使用如下:
curl https://gitee.com/tanx/kubernetes-test/raw/master/kubernetes/init/init.sh | sh  对于国内用户,推荐使用cn脚本:
curl https://gitee.com/tanx/kubernetes-test/raw/master/kubernetes/init-cn/init.sh | sh  脚本说明 helm-tiller wget https://storage.googleapis.com/kubernetes-helm/helm-v2.13.0-linux-amd64.tar.gz &amp;amp;&amp;amp; tar -zxvf helm-v2.13.0-linux-amd64.tar.gz &amp;amp;&amp;amp; sudo cp linux-amd64/helm /usr/local/bin helm init --upgrade -i registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.13.0 --stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts --service-account=clusterrole-aggregation-controller helm repo add rook-stable https://charts.rook.io/stable helm repo add bitnami https://charts.bitnami.com/bitnami helm repo update  kubeapp helm install --name kubeapps --namespace kubeapps bitnami/kubeapps kubectl create serviceaccount kubeapps-operator kubectl create clusterrolebinding kubeapps-operator --clusterrole=cluster-admin --serviceaccount=default:kubeapps-operator export kubeappsPWD=$( kubectl get secret $(kubectl get serviceaccount kubeapps-operator -o jsonpath=&#39;{.</description>
    </item>
    
    <item>
      <title>设置终端使用代理的几种方法</title>
      <link>https://tangxusc.github.io/blog/2019/03/%E8%AE%BE%E7%BD%AE%E7%BB%88%E7%AB%AF%E4%BD%BF%E7%94%A8%E4%BB%A3%E7%90%86%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E6%B3%95/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/%E8%AE%BE%E7%BD%AE%E7%BB%88%E7%AB%AF%E4%BD%BF%E7%94%A8%E4%BB%A3%E7%90%86%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E6%B3%95/</guid>
      <description>设置终端使用代理的几种方法 方法 1: 在终端中直接运行命令
export http_proxy=http://proxyAddress:port  这个办法的好处是简单直接，并且影响面很小（只对当前终端有效，退出就不行了）。 如果你用的是 ss 代理，在当前终端运行以下命令，那么wget curl 这类网络命令都会经过 ss 代理
export ALL_PROXY=socks5://127.0.0.1:1080  方法 2: 把代理服务器地址写入 shell 配置文件.bashrc或者.zshrc
直接在.bashrc或者.zshrc添加下面内容
export http_proxy=&amp;quot;http://localhost:port&amp;quot; export https_proxy=&amp;quot;http://localhost:port&amp;quot;  以使用 shadowsocks 代理为例，ss 的代理端口为1080, 那么应该设置为
export http_proxy=&amp;quot;socks5://127.0.0.1:1080&amp;quot; export https_proxy=&amp;quot;socks5://127.0.0.1:1080&amp;quot;  或者直接设置 ALL_PROXY
export ALL_PROXY=socks5://127.0.0.1:1080  localhost就是一个域名，域名默认指向 127.0.0.1，两者是一样的。
然后ESC后:wq保存文件，接着在终端中执行 source ~/.bashrc
或者退出当前终端再起一个终端。 这个办法的好处是把代理服务器永久保存了，下次就可以直接用了。
或者通过设置 alias 简写来简化操作，每次要用的时候输入setproxy，不用了就unsetproxy。
alias setproxy=&amp;quot;export ALL_PROXY=socks5://127.0.0.1:1080&amp;quot; alias unsetproxy=&amp;quot;unset ALL_PROXY&amp;quot; alias ip=&amp;quot;curl -i http://ip.cn&amp;quot;  方法 3: 改相应工具的配置，比如apt的配置
sudo vim /etc/apt/apt.</description>
    </item>
    
    <item>
      <title>设计模式 (十七) 状态模式 State（对象行为型</title>
      <link>https://tangxusc.github.io/blog/2019/03/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E5%8D%81%E4%B8%83-%E7%8A%B6%E6%80%81%E6%A8%A1%E5%BC%8F-state%E5%AF%B9%E8%B1%A1%E8%A1%8C%E4%B8%BA%E5%9E%8B/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E5%8D%81%E4%B8%83-%E7%8A%B6%E6%80%81%E6%A8%A1%E5%BC%8F-state%E5%AF%B9%E8%B1%A1%E8%A1%8C%E4%B8%BA%E5%9E%8B/</guid>
      <description>本文由 简悦 SimpRead 转码， 原文地址 https://blog.csdn.net/hguisu/article/details/7557252
 设计模式 (十七) 状态模式 State（对象行为型）
1. 概述 在软件开发过程中，应用程序可能会根据不同的情况作出不同的处理。最直接的解决方案是将这些所有可能发生的情况全都考虑到。然后使用 if&amp;hellip; ellse 语句来做状态判断来进行不同情况的处理。但是对复杂状态的判断就显得 “力不从心了”。随着增加新的状态或者修改一个状体（if else(或 switch case) 语句的增多或者修改）可能会引起很大的修改，而程序的可读性，扩展性也会变得很弱。维护也会很麻烦。那么我就考虑只修改自身状态的模式。
 例子 1：按钮来控制一个电梯的状态，一个电梯开们，关门，停，运行。每一种状态改变，都有可能要根据其他状态来更新处理。例如，开门状体，你不能在运行的时候开门，而是在电梯定下后才能开门。
例子 2：我们给一部手机打电话，就可能出现这几种情况：用户开机，用户关机，用户欠费停机，用户消户等。 所以当我们拨打这个号码的时候：系统就要判断，该用户是否在开机且不忙状态，又或者是关机，欠费等状态。但不管是那种状态我们都应给出对应的处理操作。
 2. 问题 对象如何在每一种状态下表现出不同的行为？
3. 解决方案 状态模式：允许一个对象在其内部状态改变时改变它的行为。对象看起来似乎修改了它的类。 在很多情况下，一个对象的行为取决于一个或多个动态变化的属性，这样的属性叫做状态，这样的对象叫做有状态的 (stateful) 对象，这样的对象状态是从事先定义好的一系列值中取出的。当一个这样的对象与外部事件产生互动时，其内部状态就会改变，从而使得系统的行为也随之发生变化。
4. 适用性 在下面的两种情况下均可使用 State 模式: 1. 一个对象的行为取决于它的状态, 并且它必须在运行时刻根据状态改变它的行为。 2. 代码中包含大量与对象状态有关的条件语句: 一个操作中含有庞大的多分支的条件（if else(或 switch case) 语句，且这些分支依赖于该对象的状态。这个状态通常用一个或多个枚举常量表示。通常 , 有多个操作包含这一相同的条件结构。 State 模式将每一个条件分支放入一个独立的类中。这使得你可以根据对象自身的情况将对象的状态作为一个对象，这一对象可以不依赖于其他对象而独立变化。
5. 结构 6. 模式的组成  环境类（Context）: 定义客户感兴趣的接口。维护一个 ConcreteState 子类的实例，这个实例定义当前状态。 抽象状态类（State）: 定义一个接口以封装与 Context 的一个特定状态相关的行为。 具体状态类（ConcreteState）: 每一子类实现一个与 Context 的一个状态相关的行为。  7.</description>
    </item>
    
    <item>
      <title>设计模式 - 命令模式</title>
      <link>https://tangxusc.github.io/blog/2019/03/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E5%91%BD%E4%BB%A4%E6%A8%A1%E5%BC%8F/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E5%91%BD%E4%BB%A4%E6%A8%A1%E5%BC%8F/</guid>
      <description>本文由 简悦 SimpRead 转码， 原文地址 https://www.cnblogs.com/f-zhao/p/6203208.html
 设计模式 - 命令模式 一、定义 命令模式是一个高内聚的模式，其定义为：Encapsulate a request as an object,there by letting you parameterize clients with different requests,queue or log requests,and support undoable operations.（将一个请求封装成一个对象，从而让你使用不同的请求把客户端参数化，对请 求排队或者记录请求日志，可以提供命令的撤销和恢复功能。）
通用类图：
在该类图中，我们看到三个角色：
 Receiver 接受者角色：该角色就是干活的角色，命令传递到这里是应该被执行的 Command 命令角色：需要执行的所有命令都在这里声明 Invoker 调用者角色：接收到命令，并执行命令
//通用Receiver类 public abstract class Receiver { public abstract void doSomething(); } //具体Receiver类 public class ConcreteReciver1 extends Receiver{ //每个接收者都必须处理一定的业务逻辑 public void doSomething(){ } } public class ConcreteReciver2 extends Receiver{ //每个接收者都必须处理一定的业务逻辑 public void doSomething(){ } } //抽象Command类 public abstract class Command { public abstract void execute(); } //具体的Command类 public class ConcreteCommand1 extends Command { //对哪个Receiver类进行命令处理 private Receiver receiver; //构造函数传递接收者 public ConcreteCommand1(Receiver _receiver){ this.</description>
    </item>
    
    <item>
      <title>迈向istio-jwt认证</title>
      <link>https://tangxusc.github.io/blog/2019/03/%E8%BF%88%E5%90%91istio-jwt%E8%AE%A4%E8%AF%81/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/%E8%BF%88%E5%90%91istio-jwt%E8%AE%A4%E8%AF%81/</guid>
      <description>istio-jwt认证 [TOC]
背景 在建设企业的各种项目中,我们一定离不开,或者总要和认证授权系统打交道,应用总会入侵一些认证和授权部分的代码,现在在java等方面有大量的安全框架,例如spring security,shiro等等框架,这些框架也是为了解决这个重复性的做认证和授权等功能的问题,这也是我们在单体服务的时候一直做的,将各种各样的框架集成到系统中,那么现在在微服务时代,或者说在istio有没有解决方法能解决这个问题呢,让服务真正回归业务,不再去过多的管理认证的问题呢.
在本节中我们将会将我们的应用构建为一个需要使用jwt token才能访问的服务,在没有jwt token的情况下,将会返回401 未授权.
服务图 target配置 #记得开启自动注入 #kubectl label namespace test istio-injection=enabled apiVersion: extensions/v1beta1 kind: Deployment metadata: name: target namespace: test labels: app: target version: v1 spec: template: metadata: labels: app: target version: v1 spec: containers: - name: target image: service-proxy:go-1 ports: - containerPort: 8090 name: http protocol: TCP --- kind: Service apiVersion: v1 metadata: name: target namespace: test spec: selector: app: target ports: - port: 80 name: http #一定注意命名 protocol: TCP targetPort: 8090  在k8s中部署服务 $ kubectl create ns test $ kubectl label namespace test istio-injection=enabled $ kubectl apply -f k8s.</description>
    </item>
    
    <item>
      <title>迈向istio-opa授权</title>
      <link>https://tangxusc.github.io/blog/2019/03/%E8%BF%88%E5%90%91istio-opa%E6%8E%88%E6%9D%83/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/%E8%BF%88%E5%90%91istio-opa%E6%8E%88%E6%9D%83/</guid>
      <description>istio-opa授权 [TOC]
背景 在上一章节中,我们使用jwt进行了认证,那么我们如何对资源进行授权检查呢?
在istio中,我们一个请求实际的调用链,我们来看看一个请求的流程图:
start=&amp;gt;start: 请求 end=&amp;gt;end: 结束 gateway=&amp;gt;operation: gateway target=&amp;gt;operation: 目标服务 out401=&amp;gt;inputoutput: 返回401 out200=&amp;gt;inputoutput: 返回200 mixer=&amp;gt;condition: mixer-check start-&amp;gt;gateway-&amp;gt;mixer mixer(no)-&amp;gt;out401 mixer(yes)-&amp;gt;target-&amp;gt;out200  一个请求在到达网关后,需要请求mixer-check进行请求的授权验证,对于符合此授权验证的,则开始请求目标服务,如果不符合的,那么则会直接拒绝这个请求,向前端返回401/403
那么我们如何接入istio的授权验证呢?看过istio的可能会说istio提供了rbac的handler,那么这里有必要分析一下,istio提供的rbac的handler有什么要求.
 istio中所有的认证和授权都希望我们将信息放在istio的系统中,这样在本地就完成了认证和授权,则会少一次链路调用 rbac的handler需要将授权信息以特定格式放在k8s的文件系统中,或者存放在k8s的etcd中   那么这有什么缺陷呢?
 有一些信息放在istio中是可以的,例如jwt的公钥,这样可以加快认证 授权信息放在k8s还是有待商榷的,权限是用户*资源*权限的集合,这个集合比较大,放在etcd中是否不太合适呢? 有一些地址不需要检查权限  那么我们如何解决这些问题呢? 通过查找istio文档,找到了另外一种open policy agent(简称OPA) Handler,我们看看opa官方的介绍:
 OPA是一种轻量级的通用策略引擎，可以与您的服务共存。您可以将OPA集成为边车，主机级守护程序或库。
服务通过执行*查询*将策略决策卸载到OPA 。OPA评估策略和数据以生成查询结果（将其发送回客户端）。策略使用高级声明性语言编写，可以通过文件系统或定义良好的API加载到OPA中。
 现在opa已经内置在mixer中(istio 1.0.4),可以为我们提供了一种mixer之外的认证模式:
流程图 start=&amp;gt;start: 请求 end=&amp;gt;end: 结束 gateway=&amp;gt;operation: gateway target=&amp;gt;operation: 目标服务 gateway=&amp;gt;operation: gateway out401=&amp;gt;inputoutput: 返回401 out200=&amp;gt;inputoutput: 返回200 opa认证=&amp;gt;subroutine: opa认证 mixer=&amp;gt;operation: mixer-check 认证服务=&amp;gt;condition: 认证服务 start-&amp;gt;gateway-&amp;gt;mixer-&amp;gt;opa认证-&amp;gt;认证服务 认证服务(no)-&amp;gt;out401 认证服务(yes)-&amp;gt;target-&amp;gt;out200  istio中的rule/handler/instance 在istio中我们需要理解rule handler instance这三个对象,这里我简要说明一下这三个对象</description>
    </item>
    
    <item>
      <title>迈向istio-tls</title>
      <link>https://tangxusc.github.io/blog/2019/03/%E8%BF%88%E5%90%91istio-tls/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/%E8%BF%88%E5%90%91istio-tls/</guid>
      <description>istio-tls [TOC]
我们已经完成了我们服务的路由,并且也已经有了镜像流量了,那么接下来我们要做什么呢?
背景 安全-所有应用都离不开的话题,在微服务中也一样会存在安全问题,特别是在一个企业的微服务系统中,各服务对外的接口安全都做的很好,但是服务间的安全等问题很多企业都做的不够好.这里我给大家一个例子:
企业x有一个混合云集群,node节点分别位于 某云服务器提供商和本地机房这两个地方,并且部署了两个服务,其中存在不少敏感信息,我们用一个表单来说明这样的情况:
    某云 本地     服务A  local-node1   服务B cloud-node2     这个时候,我们发现,服务A调用服务B实际上是一次跨云之旅,但是这个跨云之旅中途会出现以下几个安全问题:
 中途被劫道(劫持) 到目标服务的可能是间谍(请求被修改,替换)  那么,我们如何应对这样的情况呢?
在istio的安全章节中 我找到了一个应对此问题的答案,各位请跟我来,我指给各位看istio-安全
好了,大幕拉开,表演开始.
服务图 target配置 #记得开启自动注入 #kubectl label namespace test istio-injection=enabled apiVersion: extensions/v1beta1 kind: Deployment metadata: name: target namespace: test labels: app: target version: v1 spec: template: metadata: labels: app: target version: v1 spec: containers: - name: target image: service-proxy:go-1 ports: - containerPort: 8090 name: http protocol: TCP --- kind: Service apiVersion: v1 metadata: name: target namespace: test spec: selector: app: target ports: - port: 80 name: http #一定注意命名 protocol: TCP targetPort: 8090  在k8s中部署服务 $ kubectl create ns test $ kubectl label namespace test istio-injection=enabled $ kubectl apply -f k8s.</description>
    </item>
    
    <item>
      <title>迈向istio-多服务通信</title>
      <link>https://tangxusc.github.io/blog/2019/03/%E8%BF%88%E5%90%91istio-%E5%A4%9A%E6%9C%8D%E5%8A%A1%E9%80%9A%E4%BF%A1/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/%E8%BF%88%E5%90%91istio-%E5%A4%9A%E6%9C%8D%E5%8A%A1%E9%80%9A%E4%BF%A1/</guid>
      <description>istio-多服务通信 [TOC]
在之前的示例中,我们在istio中启动了nginx,tomcat等服务,那在此节中,我们再深入的进行一些功能的使用;
在微服务的背景下,现在越来越多的被拆分为单个服务了,那么这些服务怎么在istio上运行,服务间如何进行通信呢?在本节中我们将尝试构建一个proxy服务和target服务
服务规划  请忽略nginx,这个是我用来做测试的&amp;hellip;
 proxy服务 proxy服务使用golang进行构建,serviceProxy.go文件内容如下:
package main import ( &amp;quot;fmt&amp;quot; &amp;quot;io/ioutil&amp;quot; &amp;quot;log&amp;quot; &amp;quot;net/http&amp;quot; &amp;quot;os&amp;quot; ) func main() { http.HandleFunc(&amp;quot;/proxy&amp;quot;, handler) http.HandleFunc(&amp;quot;/index&amp;quot;, indexHandler) serve := http.ListenAndServe(&amp;quot;0.0.0.0:8090&amp;quot;, nil) if serve != nil { log.Fatalf(&amp;quot;启动失败,%v&amp;quot;, serve) } else { fmt.Fprintf(os.Stdout, &amp;quot;启动成功&amp;quot;) } } func handler(writer http.ResponseWriter, request *http.Request) { fmt.Printf(&amp;quot;proxy请求begin\n&amp;quot;) request.ParseForm() get := request.Form.Get(&amp;quot;url&amp;quot;) fmt.Printf(&amp;quot;请求地址:%s\n&amp;quot;, get) for key, value := range request.Form { fmt.Printf(&amp;quot;请求参数 [%s]:%s \n&amp;quot;, key, value) } for key, value := range request.</description>
    </item>
    
    <item>
      <title>迈向istio-安装</title>
      <link>https://tangxusc.github.io/blog/2019/03/%E8%BF%88%E5%90%91istio-%E5%AE%89%E8%A3%85/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/%E8%BF%88%E5%90%91istio-%E5%AE%89%E8%A3%85/</guid>
      <description>istio-安装 [TOC]
下载istio的release curl -L https://git.io/getLatestIstio | sh - cd istio-1.0.2 #设置环境变量,以便后面可以执行istioctl命令 export PATH=$PWD/bin:$PATH  下载helm wget https://storage.googleapis.com/kubernetes-helm/helm-v2.11.0-linux-amd64.tar.gz tar zxvf helm-v2.11.0-linux-amd64.tar.gz #或者使用我给你下载的 wget https://gitee.com/tanx/kubernetes-test/raw/master/helm/helm-v2.11.0-linux-amd64.tar tar zxvf helm-v2.11.0-linux-amd64.tar.gz export PATH=$PWD/linux-amd64/:$PATH  开始安装 $ kubectl apply -f install/kubernetes/helm/istio/templates/crds.yaml  有两种方式均可以安装istio,这里我们使用helm生成yaml的方式(因为helm的方式安装的istio,不用再去自己下载gcr.io中的镜像)
因为我本人使用的是本地电脑,所以我需要对istio安装的参数进行一定的修改,具体修改如下:
 复制一个新的istio-1.0.2/install/kubernetes/helm/istio/values.yaml文件出来
 修改gateways为NodePort
gateways: enabled: true #省略n多节点 type: ClusterIP #修改为NodePort  启用grafana,将grafana节点下的enabled修改为true
grafana: enabled: false #修改为true #省略... service: annotations: {} name: http type: ClusterIP #修改为NodePort externalPort: 3000 internalPort: 3000  启用prometheus</description>
    </item>
    
    <item>
      <title>迈向istio-引入外部服务</title>
      <link>https://tangxusc.github.io/blog/2019/03/%E8%BF%88%E5%90%91istio-%E5%BC%95%E5%85%A5%E5%A4%96%E9%83%A8%E6%9C%8D%E5%8A%A1/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/%E8%BF%88%E5%90%91istio-%E5%BC%95%E5%85%A5%E5%A4%96%E9%83%A8%E6%9C%8D%E5%8A%A1/</guid>
      <description>istio-引入外部服务 [TOC]
在istio中所有的流量都是通过istio的initContainer启动的时候对iptable进行了劫持的,那么外部流量就无法通过dns等服务发现机制进行路由了,这个时候怎么办呢? 这一节我们就来解决这个问题.
服务规划 注意:istio-egressGateway这个服务是路由的出口,这样做有几个优点
 企业通过统一的ingressGateway控制流量的进入
 通过EgressGateway控制流量的总体出
  这样更有利于网络管理人员进行统一的流量控制和安全检查.
但是这也会引起几个问题,希望各位注意:
 IngressGateway需要高可用,并且流量较大 EgressGateway外部出口流量一样不会小,这个时候很考研公司的基础网络环境  那么在我们这一节的示例中我们引入 www.baidu.com 这个服务来做我们的测试
 注意:此节基于上节部署的服务
 百度(baidu)服务 istio.yaml
apiVersion: networking.istio.io/v1alpha3 kind: ServiceEntry metadata: name: baidu namespace: test spec: hosts: - &amp;quot;www.baidu.com&amp;quot; location: MESH_EXTERNAL # location: MESH_INTERNAL ports: - number: 80 name: http protocol: HTTP resolution: DNS # endpoints: # - address: www.baidu.com # ports: # http: 80 --- apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: istio-egressgateway namespace: test spec: selector: istio: egressgateway servers: - port: number: 80 name: http protocol: HTTP hosts: - &amp;quot;*&amp;quot; --- apiVersion: networking.</description>
    </item>
    
    <item>
      <title>迈向istio-服务路由</title>
      <link>https://tangxusc.github.io/blog/2019/03/%E8%BF%88%E5%90%91istio-%E6%9C%8D%E5%8A%A1%E8%B7%AF%E7%94%B1/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/%E8%BF%88%E5%90%91istio-%E6%9C%8D%E5%8A%A1%E8%B7%AF%E7%94%B1/</guid>
      <description>服务路由 [TOC]
在上一节中,我们使用nginx开启了我们istio的第一个应用,现在我们加入另外一个服务tomcat
 本节内容基于上节内容,请先运行上一节的yaml文件,然后再体验本节内容
 tomcat tomcat.yaml文件如下:
apiVersion: extensions/v1beta1 kind: Deployment metadata: name: tomcat namespace: test labels: app: tomcat version: v1 spec: template: metadata: labels: app: tomcat version: v1 spec: containers: - name: tomcat image: tomcat:8 ports: - containerPort: 8080 name: http protocol: TCP --- kind: Service apiVersion: v1 metadata: name: tomcat namespace: test spec: type: ClusterIP selector: app: tomcat ports: - port: 8890 protocol: TCP targetPort: 8080  创建tomcat服务 $ istioctl kube-inject -f tomcat.</description>
    </item>
    
    <item>
      <title>迈向istio-示例</title>
      <link>https://tangxusc.github.io/blog/2019/03/%E8%BF%88%E5%90%91istio-%E7%A4%BA%E4%BE%8B/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/%E8%BF%88%E5%90%91istio-%E7%A4%BA%E4%BE%8B/</guid>
      <description>istio-示例 [TOC]
在上一节中我们已经成功的安装了istio的各个组件,接下来我们一起来运行一个nginx,体验一下istio的功能
nginx示例 nginx.yaml文件如下:
apiVersion: extensions/v1beta1 kind: Deployment metadata: name: nginx namespace: test labels: app: nginx version: v1 spec: template: metadata: labels: app: nginx version: v1 spec: containers: - name: nginx image: nginx ports: - containerPort: 80 name: http protocol: TCP --- kind: Service apiVersion: v1 metadata: name: nginx namespace: test spec: type: ClusterIP selector: app: nginx ports: - port: 7890 protocol: TCP targetPort: 80  在kubernetes中创建命名空间test $ kubectl create namespace test #在istio中开启自动sidecar注入,如果不能支持自动注入,则使用下面的方式 $ kubectl label namespace test istio-injection=enable #使用istioctl注入sidecar $ istioctl kube-inject -f nginx.</description>
    </item>
    
    <item>
      <title>迈向istio-网关</title>
      <link>https://tangxusc.github.io/blog/2019/03/%E8%BF%88%E5%90%91istio-%E7%BD%91%E5%85%B3/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/%E8%BF%88%E5%90%91istio-%E7%BD%91%E5%85%B3/</guid>
      <description>istio-网关 [TOC]
在上一节中我们已经成功的简单运行了istio的一个路由,也有了一番流量管理的体验,那么很多人都不禁要问,这些配置和yaml是什么意思呢?
那接下来我们基于istio示例中的配置,一点一点的解析这些yaml文件.
 nginx.yaml中的内容为k8s的yaml文件,再此不做赘述.
 gateway apiVersion: networking.istio.io/v1alpha3 kind: Gateway	#声明类型 metadata: name: nginx	#名称 namespace: test	#作用的namespace spec: selector:	#配置选择器,istio将根据此选择器选择具体的pod来作为网关用于承载网格边缘的进入和发出连接 istio: ingressgateway servers:	#声明具体的服务的host和port的绑定关系 - port:	#注意,此处是pod已经开放的端口,如果pod没有开放此端口,配置将不生效 number: 80 name: http protocol: HTTP hosts:	#声明port绑定的host - &amp;quot;*&amp;quot;  VirtualService apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: nginx namespace: test spec: hosts:	#流量的目标主机,可以是带有通配符前缀的DNS名称，也可以是IP地址,FQDN地址,使用FQDN地址要格外注意,例如配置host为nginx,那么在此处服务全路径为nginx.test.svc.cluster.local - &amp;quot;*&amp;quot; gateways:	#gateway 名称列表	- nginx http:	#HTTP 流量规则的有序列表,用于匹配端口服务前缀为http-、http2-、grpc- 或者协议为HTTP、HTTP2、GRPC 以及终结的TLS - match:	#激活规则所需的匹配条件 - uri: prefix: /test rewrite:	#重写请求url uri: &amp;quot;/&amp;quot; route:	#对流量可能进行重定向或者转发配置 - destination:	#请求或连接在经过路由规则的处理之后，就会被发送给目标 host: nginx	#目标的host subset: v1	#目标的子集   使用FQDN地址要格外注意,例如配置host为nginx,那么在此处服务全路径为nginx.</description>
    </item>
    
    <item>
      <title>迈向istio-自定义mixer adapter</title>
      <link>https://tangxusc.github.io/blog/2019/03/%E8%BF%88%E5%90%91istio-%E8%87%AA%E5%AE%9A%E4%B9%89mixer-adapter/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/%E8%BF%88%E5%90%91istio-%E8%87%AA%E5%AE%9A%E4%B9%89mixer-adapter/</guid>
      <description>自定义mixer adapter [TOC]
本节我们将自定义一个adapter,adapter和mixer通信使用grpc,所以本节需要对grpc和mixer的adapter有一定的了解.
基于的环境:
 istio 1.0.4 golang 1.11(go module) goland (或者其他go IDE)  mixer介绍 mixer是istio负责策略和遥测的组件,实际上是一个抽象的基础设施后端,用于实现访问控制,遥测捕获,配额管理,计费等功能.
在mixer中提供了adapter的机制用来扩展mixer功能,sidecar在每次请求前调用mixer进行check,在请求完成后向mixer进行report,具体来说mixer提供了:
 后端adapter的抽象: mixer抽象了后端adapter的实现,sidecar只需要和mixer交互,不再依赖具体的adapter 关注点分离: mixer提供的check,report机制让adapter只需要关注具体的行为,进行细粒度的控制  具体的策略和遥测收集如下图所示:
adapter介绍 adapter是用来扩展mixer行为的组件,mixer和adapter之间使用grpc通信,adapter可以实现日志记录,监控,配额检查,权限检查等,adapter需要向mixer注册,注册后,使用handler/instance/rule进行绑定才能生效.
在mixer中提供了两种类型的adapter实现方式:
 mixer内部的adapter: 放在mixer组件内部,并且编译在mixer中,随着mixer一起发行  优点:不用进行grpc网络通信,速度快
缺点:在mixer内部,无法自定义,如果自定义那么需要求改mixer源码重新编译
 外部的adapter: 在k8s集群中以工作负载的方式运行  优点:可自定义,并且以工作负载等方式运行部署方便,编译简单
缺点:grpc网络通信,不过grpc通信是可以复用的(http2)
本节我们就要自定义一个外部的adapter来扩展mixer (不实现具体功能,具体功能各位可自行实现,较为简单)
attributes介绍 在扩展上有mixer和adapter配合进行扩展,在通信协议上使用grpc,那么具体的通信内容是什么呢?
在mixer中有一个重要的概念是attribute,用于描述请求的所有环境和变量等等,属性的示例如下:
request.path: xyz/abc request.size: 234 request.time: 12:34:56.789 04/17/2017 source.ip: 192.168.0.1 destination.service: example  mixer的本质实际上就是一个属性的处理器,将基础的属性处理后(处理方式参照),发起到具体adapter的grpc调用.
支持的属性可以通过命令k8s命令查看
kubectl get attributemanifests -o yaml -n istio-system  在使用基础属性时,肯定会有很多属性不满足具体的使用需求,这个时候需要使用到属性表达式来做一些简单的属性编辑,具体示例如下:
source_name: source.</description>
    </item>
    
    <item>
      <title>迈向istio-错误排查清单</title>
      <link>https://tangxusc.github.io/blog/2019/03/%E8%BF%88%E5%90%91istio-%E9%94%99%E8%AF%AF%E6%8E%92%E6%9F%A5%E6%B8%85%E5%8D%95/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/%E8%BF%88%E5%90%91istio-%E9%94%99%E8%AF%AF%E6%8E%92%E6%9F%A5%E6%B8%85%E5%8D%95/</guid>
      <description>istio-错误排查清单 [TOC]
使用到istio的时候,我发现istio对于调试方面,错误提示方面还是不怎么友好,很多时候都不知道去哪里找错误原因,突然想到飞机那么复杂的系统是如何做到一直按照正确的方式运行的呢,遂提出此错误排查清单,用于排查部分错误,各位同仁可以一句此错误清单进行异常的排查,或者一句不同的因素对错误进行处理.
错误排查清单  #### 工作负载中的service端口必须正确的命名   服务端口必须进行命名。端口名称只允许是&amp;lt;协议&amp;gt;[-&amp;lt;后缀&amp;gt;-]模式，其中&amp;lt;协议&amp;gt;部分可选择范围包括 http、http2、grpc、mongo 以及 redis，Istio 可以通过对这些协议的支持来提供路由能力。例如 name: http2-foo 和 name: http 都是有效的端口名，但 name: http2foo 就是无效的。如果没有给端口进行命名，或者命名没有使用指定前缀，那么这一端口的流量就会被视为普通 TCP 流量（除非显式的用 Protocol: UDP 声明该端口是 UDP 端口）
  #### pod必须关联到服务,并且同一端口只能同一协议   Pod 必须关联到service，如果一个 Pod 属于多个服务，这些服务不能再同一端口上使用不同协议，例如 HTTP 和 TCP
  #### Deployment 应带有 app 以及 version 标签   在使用 Kubernetes Deployment 进行 Pod 部署的时候，建议显式的为 Deployment 加上 app 以及 version标签。每个 Deployment 都应该有一个有意义的 app 标签和一个用于标识 Deployment 版本的 version 标签。app 标签在分布式跟踪的过程中会被用来加入上下文信息。Istio 还会用 app 和 version 标签来给遥测指标数据加入上下文信息。</description>
    </item>
    
    <item>
      <title>迈向istio-镜像流量</title>
      <link>https://tangxusc.github.io/blog/2019/03/%E8%BF%88%E5%90%91istio-%E9%95%9C%E5%83%8F%E6%B5%81%E9%87%8F/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/%E8%BF%88%E5%90%91istio-%E9%95%9C%E5%83%8F%E6%B5%81%E9%87%8F/</guid>
      <description>istio-镜像流量 [TOC]
我们在前面几个章节中使用了两个服务(proxy,target),现在我们想对target进行一次升级,但是现在我们这个代码写的还不够好(没人能说他的代码一次就是期望的行为),希望通过复制一部分现在的流量 用来测试这个服务是否正确,那么这个时候就会使用到istio的镜像流量功能了
好了,大幕拉开,开始我们的表演.
服务图 target代码 serviceProxy2.go
package main import ( &amp;quot;fmt&amp;quot; &amp;quot;io/ioutil&amp;quot; &amp;quot;log&amp;quot; &amp;quot;net/http&amp;quot; &amp;quot;os&amp;quot; ) func main() { http.HandleFunc(&amp;quot;/proxy&amp;quot;, handler) http.HandleFunc(&amp;quot;/index&amp;quot;, indexHandler) serve := http.ListenAndServe(&amp;quot;0.0.0.0:8090&amp;quot;, nil) if serve != nil { log.Fatalf(&amp;quot;启动失败,%v&amp;quot;, serve) } else { fmt.Fprintf(os.Stdout, &amp;quot;启动成功&amp;quot;) } } func handler(writer http.ResponseWriter, request *http.Request) { fmt.Printf(&amp;quot;我是v2的proxy请求begin\n&amp;quot;)	fmt.Printf(&amp;quot;我是v2的proxy请求begin\n&amp;quot;) request.ParseForm() get := request.Form.Get(&amp;quot;url&amp;quot;) fmt.Printf(&amp;quot;我是v2的请求地址:%s\n&amp;quot;, get) for key, value := range request.Form { fmt.Printf(&amp;quot;我是v2的请求参数 [%s]:%s \n&amp;quot;, key, value) } for key, value := range request.</description>
    </item>
    
    <item>
      <title>配置redis外网可访问</title>
      <link>https://tangxusc.github.io/blog/2019/03/%E9%85%8D%E7%BD%AEredis%E5%A4%96%E7%BD%91%E5%8F%AF%E8%AE%BF%E9%97%AE/</link>
      <pubDate>Wed, 20 Mar 2019 14:15:59 +0800</pubDate>
      
      <guid>https://tangxusc.github.io/blog/2019/03/%E9%85%8D%E7%BD%AEredis%E5%A4%96%E7%BD%91%E5%8F%AF%E8%AE%BF%E9%97%AE/</guid>
      <description>本文由 简悦 SimpRead 转码， 原文地址 https://blog.csdn.net/hel12he/article/details/46911159
 环境描述 在 Linux 中安装了 redis 服务
在 Windows 中安装了 xampp 环境。以供 PHP 运行
PHP 代码如下：
&amp;lt;?php $redis = new Redis(); $redis-&amp;gt;connect(&#39;192.168.1.4&#39;, 6379); $redis-&amp;gt;set(&#39;tag&#39;, &#39;hello&#39;); echo &#39;name:&#39;, $redis-&amp;gt;get(&#39;tag&#39;);  执行以上代码时，报错如下：
 Fatal error: Uncaught exception ‘RedisException’ with message ‘Redis server went away’ in xxxx
RedisException: Redis server went away in xxxxxx
 解错误分析及解决办法 错误的原因很简单，就是没有连接上 redis 服务，由于 redis 采用的安全策略，默认会只准许本地访问。需要通过简单配置，完成允许外网访问。
 修改 redis 的配置文件，将所有 bind 信息全部屏蔽。</description>
    </item>
    
    <item>
      <title>adapter.my.config</title>
      <link>https://tangxusc.github.io/blog/1/01/adapter.my.config/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://tangxusc.github.io/blog/1/01/adapter.my.config/</guid>
      <description>config for my-adapter
Params  config for my-adapter
  Field Type Description    filePath string  Path of the file to save the information about runtime requests.
     </description>
    </item>
    
  </channel>
</rss>