<!DOCTYPE html>
<html lang="en-us">
  <head>
    
    <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="generator" content="Hugo 0.55.0-DEV with theme Tranquilpeak 0.4.3-SNAPSHOT">
<meta name="author" content="苏连云">
<meta name="keywords" content="kubeadm, k8s, install">
<meta name="description" content="本文由 简悦 SimpRead 转码， 原文地址 https://www.cnblogs.com/RainingNight/p/using-kubeadm-to-create-a-cluster.html
 使用 kubeadm 搭建 Kubernetes(1.10.2) 集群（国内环境） [TOC]
目标  在您的机器上建立一个安全的 Kubernetes 集群。 在集群里安装网络插件，以便应用之间可以相互通讯。 在集群上运行一个简单的微服务。  准备 主机  一台或多台运行 Ubuntu 16.04 &#43; 的主机。 最好选至少有 2 GB 内存的双核主机。 集群中完整的网络连接，公网或者私网都可以。  软件 安装 Docker sudo apt-get update sudo apt-get install -y docker.io  Kubunetes 建议使用老版本的docker.io，如果需要使用最新版的docker-ce，可参考上一篇博客：Docker 初体验。
禁用 swap 文件 然后需要禁用 swap 文件，这是 Kubernetes 的强制步骤。实现它很简单，编辑/etc/fstab文件，注释掉引用swap的行，保存并重启后输入sudo swapoff -a即可。
 对于禁用swap内存，你可能会有点不解，具体原因可以查看 Github 上的 Issue：Kubelet/Kubernetes should work with Swap Enabled。">


<meta property="og:description" content="本文由 简悦 SimpRead 转码， 原文地址 https://www.cnblogs.com/RainingNight/p/using-kubeadm-to-create-a-cluster.html
 使用 kubeadm 搭建 Kubernetes(1.10.2) 集群（国内环境） [TOC]
目标  在您的机器上建立一个安全的 Kubernetes 集群。 在集群里安装网络插件，以便应用之间可以相互通讯。 在集群上运行一个简单的微服务。  准备 主机  一台或多台运行 Ubuntu 16.04 &#43; 的主机。 最好选至少有 2 GB 内存的双核主机。 集群中完整的网络连接，公网或者私网都可以。  软件 安装 Docker sudo apt-get update sudo apt-get install -y docker.io  Kubunetes 建议使用老版本的docker.io，如果需要使用最新版的docker-ce，可参考上一篇博客：Docker 初体验。
禁用 swap 文件 然后需要禁用 swap 文件，这是 Kubernetes 的强制步骤。实现它很简单，编辑/etc/fstab文件，注释掉引用swap的行，保存并重启后输入sudo swapoff -a即可。
 对于禁用swap内存，你可能会有点不解，具体原因可以查看 Github 上的 Issue：Kubelet/Kubernetes should work with Swap Enabled。">
<meta property="og:type" content="article">
<meta property="og:title" content="使用 kubeadm 搭建 Kubernetes(1.10.2) 集群（国内环境）">
<meta name="twitter:title" content="使用 kubeadm 搭建 Kubernetes(1.10.2) 集群（国内环境）">
<meta property="og:url" content="https://tangxusc.github.io/blog/2019/03/%E4%BD%BF%E7%94%A8-kubeadm-%E6%90%AD%E5%BB%BA-kubernetes1.10.2-%E9%9B%86%E7%BE%A4%E5%9B%BD%E5%86%85%E7%8E%AF%E5%A2%83/">
<meta property="twitter:url" content="https://tangxusc.github.io/blog/2019/03/%E4%BD%BF%E7%94%A8-kubeadm-%E6%90%AD%E5%BB%BA-kubernetes1.10.2-%E9%9B%86%E7%BE%A4%E5%9B%BD%E5%86%85%E7%8E%AF%E5%A2%83/">
<meta property="og:site_name" content="苏连云的博客">
<meta property="og:description" content="本文由 简悦 SimpRead 转码， 原文地址 https://www.cnblogs.com/RainingNight/p/using-kubeadm-to-create-a-cluster.html
 使用 kubeadm 搭建 Kubernetes(1.10.2) 集群（国内环境） [TOC]
目标  在您的机器上建立一个安全的 Kubernetes 集群。 在集群里安装网络插件，以便应用之间可以相互通讯。 在集群上运行一个简单的微服务。  准备 主机  一台或多台运行 Ubuntu 16.04 &#43; 的主机。 最好选至少有 2 GB 内存的双核主机。 集群中完整的网络连接，公网或者私网都可以。  软件 安装 Docker sudo apt-get update sudo apt-get install -y docker.io  Kubunetes 建议使用老版本的docker.io，如果需要使用最新版的docker-ce，可参考上一篇博客：Docker 初体验。
禁用 swap 文件 然后需要禁用 swap 文件，这是 Kubernetes 的强制步骤。实现它很简单，编辑/etc/fstab文件，注释掉引用swap的行，保存并重启后输入sudo swapoff -a即可。
 对于禁用swap内存，你可能会有点不解，具体原因可以查看 Github 上的 Issue：Kubelet/Kubernetes should work with Swap Enabled。">
<meta name="twitter:description" content="本文由 简悦 SimpRead 转码， 原文地址 https://www.cnblogs.com/RainingNight/p/using-kubeadm-to-create-a-cluster.html
 使用 kubeadm 搭建 Kubernetes(1.10.2) 集群（国内环境） [TOC]
目标  在您的机器上建立一个安全的 Kubernetes 集群。 在集群里安装网络插件，以便应用之间可以相互通讯。 在集群上运行一个简单的微服务。  准备 主机  一台或多台运行 Ubuntu 16.04 &#43; 的主机。 最好选至少有 2 GB 内存的双核主机。 集群中完整的网络连接，公网或者私网都可以。  软件 安装 Docker sudo apt-get update sudo apt-get install -y docker.io  Kubunetes 建议使用老版本的docker.io，如果需要使用最新版的docker-ce，可参考上一篇博客：Docker 初体验。
禁用 swap 文件 然后需要禁用 swap 文件，这是 Kubernetes 的强制步骤。实现它很简单，编辑/etc/fstab文件，注释掉引用swap的行，保存并重启后输入sudo swapoff -a即可。
 对于禁用swap内存，你可能会有点不解，具体原因可以查看 Github 上的 Issue：Kubelet/Kubernetes should work with Swap Enabled。">
<meta property="og:locale" content="en-us">

  
    <meta property="article:published_time" content="2019-03-20T14:15:59">
  
  
    <meta property="article:modified_time" content="2019-03-20T14:15:59">
  
  
  
    
      <meta property="article:section" content="kubeadm">
    
      <meta property="article:section" content="k8s">
    
  
  
    
      <meta property="article:tag" content="kubeadm">
    
      <meta property="article:tag" content="k8s">
    
      <meta property="article:tag" content="install">
    
  


<meta name="twitter:card" content="summary">











  <meta property="og:image" content="https://www.gravatar.com/avatar/a3d740bc2618da5f5a9de4fe94c05429?s=640">
  <meta property="twitter:image" content="https://www.gravatar.com/avatar/a3d740bc2618da5f5a9de4fe94c05429?s=640">


    <title>使用 kubeadm 搭建 Kubernetes(1.10.2) 集群（国内环境）</title>

    <link rel="icon" href="https://tangxusc.github.io/blog/favicon.png">
    

    

    <link rel="canonical" href="https://tangxusc.github.io/blog/2019/03/%E4%BD%BF%E7%94%A8-kubeadm-%E6%90%AD%E5%BB%BA-kubernetes1.10.2-%E9%9B%86%E7%BE%A4%E5%9B%BD%E5%86%85%E7%8E%AF%E5%A2%83/">

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha256-eZrrJcwDc/3uDhsdt61sL2oOBY362qM3lon1gyExkL0=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/jquery.fancybox.min.css" integrity="sha256-vuXZ9LGmmwtjqFX1F+EKin1ThZMub58gKULUyf0qECk=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/helpers/jquery.fancybox-thumbs.min.css" integrity="sha256-SEa4XYAHihTcEP1f5gARTB2K26Uk8PsndQYHQC1f4jU=" crossorigin="anonymous" />
    
    
    <link rel="stylesheet" href="https://tangxusc.github.io/blog/css/style-nnm2spxvve8onlujjlegkkytaehyadd4ksxc1hyzzq9a2wvtrgbljqyulomn.min.css" />
    
    

    
      
    
    
  </head>

  <body>
    <div id="blog">
      <header id="header" data-behavior="4">
  <i id="btn-open-sidebar" class="fa fa-lg fa-bars"></i>
  <div class="header-title">
    <a class="header-title-link" href="https://tangxusc.github.io/blog/">苏连云的博客</a>
  </div>
  
    
      <a class="header-right-picture "
         href="https://tangxusc.github.io/blog/#about">
    
    
    
      
        <img class="header-picture" src="https://www.gravatar.com/avatar/a3d740bc2618da5f5a9de4fe94c05429?s=90" alt="Author&#39;s picture" />
      
    
    </a>
  
</header>

      <nav id="sidebar" data-behavior="4">
  <div class="sidebar-container">
    
      <div class="sidebar-profile">
        <a href="https://tangxusc.github.io/blog/#about">
          <img class="sidebar-profile-picture" src="https://www.gravatar.com/avatar/a3d740bc2618da5f5a9de4fe94c05429?s=110" alt="Author&#39;s picture" />
        </a>
        <h4 class="sidebar-profile-name">苏连云</h4>
        
          <h5 class="sidebar-profile-bio">酒剑仙,醉仙酒</h5>
        
      </div>
    
    <ul class="sidebar-buttons">
      
  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://tangxusc.github.io/blog/">
    
      <i class="sidebar-button-icon fa fa-lg fa-home"></i>
      
      <span class="sidebar-button-desc">Home</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://tangxusc.github.io/blog/categories">
    
      <i class="sidebar-button-icon fa fa-lg fa-bookmark"></i>
      
      <span class="sidebar-button-desc">Categories</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://tangxusc.github.io/blog/tags">
    
      <i class="sidebar-button-icon fa fa-lg fa-tags"></i>
      
      <span class="sidebar-button-desc">Tags</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://tangxusc.github.io/blog/archives">
    
      <i class="sidebar-button-icon fa fa-lg fa-archive"></i>
      
      <span class="sidebar-button-desc">Archives</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://tangxusc.github.io/blog/#about">
    
      <i class="sidebar-button-icon fa fa-lg fa-question"></i>
      
      <span class="sidebar-button-desc">About</span>
    </a>
  </li>


    </ul>
    <ul class="sidebar-buttons">
      
  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://github.com/tangxusc/blog" target="_blank" rel="noopener">
    
      <i class="sidebar-button-icon fa fa-lg fa-github"></i>
      
      <span class="sidebar-button-desc">GitHub</span>
    </a>
  </li>


    </ul>
    <ul class="sidebar-buttons">
      

    </ul>
  </div>
</nav>

      

      <div id="main" data-behavior="4"
        class="
               hasCoverMetaIn
               ">
        <article class="post" itemscope itemType="http://schema.org/BlogPosting">
          
          
            <div class="post-header main-content-wrap text-left">
  
    <h1 class="post-title" itemprop="headline">
      使用 kubeadm 搭建 Kubernetes(1.10.2) 集群（国内环境）
    </h1>
  
  
  <div class="postShorten-meta post-meta">
    
      <time itemprop="datePublished" datetime="2019-03-20T14:15:59&#43;08:00">
        
  March 20, 2019

      </time>
    
    
  
  
    <span>in</span>
    
      <a class="category-link" href="https://tangxusc.github.io/blog/categories/kubeadm">kubeadm</a>, 
    
      <a class="category-link" href="https://tangxusc.github.io/blog/categories/k8s">k8s</a>
    
  

  </div>

</div>
          
          <div class="post-content markdown" itemprop="articleBody">
            <div class="main-content-wrap">
              

<blockquote>
<p>本文由 <a href="http://ksria.com/simpread/">简悦 SimpRead</a> 转码， 原文地址 <a href="https://www.cnblogs.com/RainingNight/p/using-kubeadm-to-create-a-cluster.html">https://www.cnblogs.com/RainingNight/p/using-kubeadm-to-create-a-cluster.html</a></p>
</blockquote>

<h1 id="使用-kubeadm-搭建-kubernetes-1-10-2-集群-国内环境">使用 kubeadm 搭建 Kubernetes(1.10.2) 集群（国内环境）</h1>

<p>[TOC]</p>

<h2 id="目标">目标</h2>

<ul>
<li>在您的机器上建立一个安全的 Kubernetes 集群。</li>
<li>在集群里安装网络插件，以便应用之间可以相互通讯。</li>
<li>在集群上运行一个简单的微服务。</li>
</ul>

<h2 id="准备">准备</h2>

<h3 id="主机">主机</h3>

<ul>
<li>一台或多台运行 Ubuntu 16.04 + 的主机。</li>
<li>最好选至少有 2 GB 内存的双核主机。</li>
<li>集群中完整的网络连接，公网或者私网都可以。</li>
</ul>

<h3 id="软件">软件</h3>

<h4 id="安装-docker">安装 Docker</h4>

<pre><code>sudo apt-get update
sudo apt-get install -y docker.io
</code></pre>

<p>Kubunetes 建议使用老版本的<code>docker.io</code>，如果需要使用最新版的<code>docker-ce</code>，可参考上一篇博客：<a href="http://www.cnblogs.com/RainingNight/p/first-docker-note.html#安装">Docker 初体验</a>。</p>

<h4 id="禁用-swap-文件">禁用 swap 文件</h4>

<p>然后需要禁用 swap 文件，这是 Kubernetes 的强制步骤。实现它很简单，编辑<code>/etc/fstab</code>文件，注释掉引用<code>swap</code>的行，保存并重启后输入<code>sudo swapoff -a</code>即可。</p>

<blockquote>
<p>对于禁用<code>swap</code>内存，你可能会有点不解，具体原因可以查看 Github 上的 Issue：<a href="https://github.com/kubernetes/kubernetes/issues/53533">Kubelet/Kubernetes should work with Swap Enabled</a>。</p>
</blockquote>

<h2 id="步骤">步骤</h2>

<h3 id="1-4-安装-kubeadm-kubelet-and-kubectl">(<sup>1</sup>&frasl;<sub>4</sub>) 安装 kubeadm, kubelet and kubectl</h3>

<ul>
<li>kubeadm: 引导启动 k8s 集群的命令工具。</li>
<li>kubelet: 在群集中的所有计算机上运行的组件, 并用来执行如启动 pods 和 containers 等操作。</li>

<li><p>kubectl: 用于操作运行中的集群的命令行工具。</p>

<pre><code>sudo apt-get update &amp;&amp; sudo apt-get install -y apt-transport-https
curl -s https://tangxusc.github.io/blog/post/kubeadm/apt-key.gpg | sudo apt-key add -

sudo cat &lt;&lt;EOF &gt;/etc/apt/sources.list.d/kubernetes.list
deb http://mirrors.ustc.edu.cn/kubernetes/apt/ kubernetes-xenial main
EOF

sudo apt-get update
sudo apt-get install -y kubelet=1.10.2-00 kubeadm=1.10.2-00 kubectl=1.10.2-00
</code></pre></li>
</ul>

<blockquote>
<p>apt-key 下载地址使用了国内镜像，官方地址为：<a href="https://packages.cloud.google.com/apt/doc/apt-key.gpg">https://packages.cloud.google.com/apt/doc/apt-key.gpg</a>。
apt 安装包地址使用了中科大的镜像，官方地址为：<a href="http://apt.kubernetes.io/">http://apt.kubernetes.io/</a>。</p>
</blockquote>

<h3 id="2-4-初始化-master-节点">(<sup>2</sup>&frasl;<sub>4</sub>) 初始化 master 节点</h3>

<p>由于网络原因，我们需要提前拉取 k8s 初始化需要用到的 Images，并添加对应的<code>k8s.gcr.io</code>标签:</p>

<pre><code>## 拉取镜像
docker pull reg.qiniu.com/k8s/kube-apiserver-amd64:v1.10.2
docker pull reg.qiniu.com/k8s/kube-controller-manager-amd64:v1.10.2
docker pull reg.qiniu.com/k8s/kube-scheduler-amd64:v1.10.2
docker pull reg.qiniu.com/k8s/kube-proxy-amd64:v1.10.2
docker pull reg.qiniu.com/k8s/etcd-amd64:3.1.12
docker pull reg.qiniu.com/k8s/pause-amd64:3.1

## 添加Tag
docker tag reg.qiniu.com/k8s/kube-apiserver-amd64:v1.10.2 k8s.gcr.io/kube-apiserver-amd64:v1.10.2
docker tag reg.qiniu.com/k8s/kube-scheduler-amd64:v1.10.2 k8s.gcr.io/kube-scheduler-amd64:v1.10.2
docker tag reg.qiniu.com/k8s/kube-controller-manager-amd64:v1.10.2 k8s.gcr.io/kube-controller-manager-amd64:v1.10.2
docker tag reg.qiniu.com/k8s/kube-proxy-amd64:v1.10.2 k8s.gcr.io/kube-proxy-amd64:v1.10.2
docker tag reg.qiniu.com/k8s/etcd-amd64:3.1.12 k8s.gcr.io/etcd-amd64:3.1.12
docker tag reg.qiniu.com/k8s/pause-amd64:3.1 k8s.gcr.io/pause-amd64:3.1

## 在Kubernetes 1.10 中，增加了CoreDNS，如果使用CoreDNS(默认关闭)，则不需要下面三个镜像。
docker pull reg.qiniu.com/k8s/k8s-dns-sidecar-amd64:1.14.10
docker pull reg.qiniu.com/k8s/k8s-dns-kube-dns-amd64:1.14.10
docker pull reg.qiniu.com/k8s/k8s-dns-dnsmasq-nanny-amd64:1.14.10

docker tag reg.qiniu.com/k8s/k8s-dns-sidecar-amd64:1.14.10 k8s.gcr.io/k8s-dns-sidecar-amd64:1.14.10
docker tag reg.qiniu.com/k8s/k8s-dns-kube-dns-amd64:1.14.10 k8s.gcr.io/k8s-dns-kube-dns-amd64:1.14.10
docker tag reg.qiniu.com/k8s/k8s-dns-dnsmasq-nanny-amd64:1.14.10 k8s.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.14.10
</code></pre>

<blockquote>
<p>据说 kubeadm 可以自定义镜像 Registry，但我并没有找到选项。</p>
</blockquote>

<h4 id="注意事项"><strong>注意事项</strong></h4>

<ul>
<li>请注意:在此时请使用 <code>kubectl get all --all-namespaces</code>来关注各容器运行情况,默认情况下应该除了DNS容器,其他均会到running状态,如果未能在此状态,请检查pod运行状态</li>

<li><p>错误情况1: pod一直未pedding状态(或者block状态),使用<code>kubectl describe pod名字</code> 查看后发现 <code>pod with UID &quot;xxx&quot;  specified privileged container, but is disallowed</code>,请依次检查中</p>

<pre><code>ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS --allow_privileged
</code></pre>

<p>这命令中是否加入了&ndash;allow_privileged 和 <code>/etc/kubernetes/manifests/kube-apiserver.yaml</code> apiServer是否启用了 <code>--allow-privileged=true</code> 参照 <a href="https://github.com/kubernetes/kubernetes/issues/6530">kubelet privileged</a></p></li>
</ul>

<p>Master 节点就是运行着控制组件的机器，包括 etcd(集群数据库) 和 API 服务 (kubectl CLI 通讯服务)。
初始化 master 节点, 只需随便在一台装过 kubeadm 的机器上运行如下命令:</p>

<pre><code>sudo kubeadm init --kubernetes-version=v1.10.2 --feature-gates=CoreDNS=true --pod-network-cidr=192.168.0.0/16
</code></pre>

<p>init 常用主要参数：</p>

<ul>
<li><p>&ndash;kubernetes-version: 指定 Kubenetes 版本，如果不指定该参数，会从 google 网站下载最新的版本信息。</p></li>

<li><p>&ndash;pod-network-cidr: 指定 pod 网络的 IP 地址范围，它的值取决于你在下一步选择的哪个网络网络插件，比如我在本文中使用的是 Calico 网络，需要指定为<code>192.168.0.0/16</code>。</p></li>

<li><p>&ndash;apiserver-advertise-address: 指定 master 服务发布的 Ip 地址，如果不指定，则会自动检测网络接口，通常是内网 IP。</p></li>

<li><p>&ndash;feature-gates=CoreDNS: 是否使用 CoreDNS，值为 true/false，CoreDNS 插件在 1.10 中提升到了 Beta 阶段，最终会成为 Kubernetes 的缺省选项。</p></li>
</ul>

<p>关于 kubeadm 更详细的的介绍请参考 <a href="https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm/">kubeadm 官方文档</a>。</p>

<p>最终输出如下：</p>

<pre><code class="language-shell">raining@raining-ubuntu:~$ sudo kubeadm init --kubernetes-version=v1.10.2 --feature-gates=CoreDNS=true --pod-network-cidr=192.168.0.0/16
[sudo] password for raining: 
[init] Using Kubernetes version: v1.10.2
[init] Using Authorization modes: [Node RBAC]
[preflight] Running pre-flight checks.
    [WARNING SystemVerification]: docker version is greater than the most recently validated version. Docker version: 17.12.1-ce. Max validated version: 17.03
    [WARNING Service-Docker]: docker service is not enabled, please run 'systemctl enable docker.service'
    [WARNING FileExisting-crictl]: crictl not found in system path
Suggestion: go get github.com/kubernetes-incubator/cri-tools/cmd/crictl
[preflight] Starting the kubelet service
[certificates] Generated ca certificate and key.
[certificates] Generated apiserver certificate and key.
[certificates] apiserver serving cert is signed for DNS names [raining-ubuntu kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.0.8]
[certificates] Generated apiserver-kubelet-client certificate and key.
[certificates] Generated etcd/ca certificate and key.
[certificates] Generated etcd/server certificate and key.
[certificates] etcd/server serving cert is signed for DNS names [localhost] and IPs [127.0.0.1]
[certificates] Generated etcd/peer certificate and key.
[certificates] etcd/peer serving cert is signed for DNS names [raining-ubuntu] and IPs [192.168.0.8]
[certificates] Generated etcd/healthcheck-client certificate and key.
[certificates] Generated apiserver-etcd-client certificate and key.
[certificates] Generated sa key and public key.
[certificates] Generated front-proxy-ca certificate and key.
[certificates] Generated front-proxy-client certificate and key.
[certificates] Valid certificates and keys now exist in &quot;/etc/kubernetes/pki&quot;
[kubeconfig] Wrote KubeConfig file to disk: &quot;/etc/kubernetes/admin.conf&quot;
[kubeconfig] Wrote KubeConfig file to disk: &quot;/etc/kubernetes/kubelet.conf&quot;
[kubeconfig] Wrote KubeConfig file to disk: &quot;/etc/kubernetes/controller-manager.conf&quot;
[kubeconfig] Wrote KubeConfig file to disk: &quot;/etc/kubernetes/scheduler.conf&quot;
[controlplane] Wrote Static Pod manifest for component kube-apiserver to &quot;/etc/kubernetes/manifests/kube-apiserver.yaml&quot;
[controlplane] Wrote Static Pod manifest for component kube-controller-manager to &quot;/etc/kubernetes/manifests/kube-controller-manager.yaml&quot;
[controlplane] Wrote Static Pod manifest for component kube-scheduler to &quot;/etc/kubernetes/manifests/kube-scheduler.yaml&quot;
[etcd] Wrote Static Pod manifest for a local etcd instance to &quot;/etc/kubernetes/manifests/etcd.yaml&quot;
[init] Waiting for the kubelet to boot up the control plane as Static Pods from directory &quot;/etc/kubernetes/manifests&quot;.
[init] This might take a minute or longer if the control plane images have to be pulled.
[apiclient] All control plane components are healthy after 39.501722 seconds
[uploadconfig] Storing the configuration used in ConfigMap &quot;kubeadm-config&quot; in the &quot;kube-system&quot; Namespace
[markmaster] Will mark node raining-ubuntu as master by adding a label and a taint
[markmaster] Master raining-ubuntu tainted and labelled with key/value: node-role.kubernetes.io/master=&quot;&quot;
[bootstraptoken] Using token: vtyk9m.g4afak37myq3rsdi
[bootstraptoken] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstraptoken] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstraptoken] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstraptoken] Creating the &quot;cluster-info&quot; ConfigMap in the &quot;kube-public&quot; namespace
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes master has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of machines by running the following on each node
as root:

  kubeadm join 192.168.0.8:6443 --token vtyk9m.g4afak37myq3rsdi --discovery-token-ca-cert-hash sha256:19246ce11ba3fc633fe0b21f2f8aaaebd7df9103ae47138dc0dd615f61a32d99
</code></pre>

<p>如果想在非 root 用户下使用<code>kubectl</code>，可以执行如下命令 (也是<code>kubeadm init</code>输出的一部分)：</p>

<pre><code>mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
</code></pre>

<p>kubeadm init 输出的 token 用于 master 和加入节点间的身份认证，token 是机密的，需要保证它的安全，因为拥有此标记的人都可以随意向集群中添加节点。你也可以使用<code>kubeadm</code>命令列出，创建，删除 Token，有关详细信息, 请参阅<a href="https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-token">官方引用文档</a>。</p>

<p>我们在浏览器中输入<code>https://&lt;master-ip&gt;:6443</code>来验证一下是否部署成功，返回如下：</p>

<pre><code>{
  &quot;kind&quot;: &quot;Status&quot;,
  &quot;apiVersion&quot;: &quot;v1&quot;,
  &quot;metadata&quot;: {

  },
  &quot;status&quot;: &quot;Failure&quot;,
  &quot;message&quot;: &quot;forbidden: User \&quot;system:anonymous\&quot; cannot get path \&quot;/\&quot;&quot;,
  &quot;reason&quot;: &quot;Forbidden&quot;,
  &quot;details&quot;: {

  },
  &quot;code&quot;: 403
}
</code></pre>

<h3 id="3-4-安装网络插件">(<sup>3</sup>&frasl;<sub>4</sub>) 安装网络插件</h3>

<p>安装一个网络插件是必须的，因为你的 pods 之间需要彼此通信。</p>

<p>网络部署必须是优先于任何应用的部署，如<code>kube-dns</code>(本文中使用的是<code>coredns</code>) 在网络部署成功之前是无法使用的。kubeadm 只支持容器网络接口（CNI）的网络类型（不支持 kubenet）。</p>

<p>比较常见的 network addon 有：Calico, Canal, Flannel, Kube-router, Romana, Weave Net 等。详细的网络列表可参考<a href="https://kubernetes.io/docs/concepts/cluster-administration/addons/">插件页面</a>。</p>

<p>使用下列命令来安装网络插件:</p>

<pre><code>kubectl apply -f &lt;add-on.yaml&gt;
</code></pre>

<p>在本文中，我使用的是 <strong>Calico</strong> 网络，安装如下：</p>

<pre><code>#下载镜像
docker pull calico/typha:v0.7.4
docker pull calico/node:v3.1.3
docker pull calico/cni:v3.1.3
#tag转换
docker tag calico/typha:v0.7.4 quay.io/calico/typha:v0.7.4
docker tag calico/node:v3.1.3 quay.io/calico/node:v3.1.3
docker tag calico/cni:v3.1.3 quay.io/calico/cni:v3.1.3
#创建
kubectl apply -f https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/rbac-kdd.yaml
kubectl apply -f https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/kubernetes-datastore/calico-networking/1.7/calico.yaml
</code></pre>

<blockquote>
<p>为了 Calico 可以正常运行，必须在执行 kubeadm init 时使用 <code>--pod-network-cidr=192.168.0.0/16</code>。</p>
</blockquote>

<p>更详细的可以查看 Calico 官方文档：<a href="https://docs.projectcalico.org/v3.1/getting-started/kubernetes">kubeadm quickstart</a>。</p>

<p>网络插件安装完成后，可以通过检查<code>coredns pod</code>的运行状态来判断网络插件是否正常运行：</p>

<pre><code>kubectl get pods --all-namespaces

# 输出
NAMESPACE     NAME                                      READY     STATUS    RESTARTS   AGE
kube-system   calico-etcd-zxmvh                         1/1       Running   0          4m
kube-system   calico-kube-controllers-f9d6c4cb6-42w9j   1/1       Running   0          4m
kube-system   calico-node-jq5qb                         2/2       Running   0          4m
kube-system   coredns-7997f8864c-kfswc                  1/1       Running   0          1h
kube-system   coredns-7997f8864c-ttvj2                  1/1       Running   0          1h
kube-system   etcd-raining-ubuntu                       1/1       Running   0          1h
kube-system   kube-apiserver-raining-ubuntu             1/1       Running   0          1h
kube-system   kube-controller-manager-raining-ubuntu    1/1       Running   0          1h
kube-system   kube-proxy-vrjlq                          1/1       Running   0          1h
kube-system   kube-scheduler-raining-ubuntu             1/1       Running   0          1h
</code></pre>

<p>等待<code>coredns pod</code>的状态变成 <strong>Running</strong>，就可以继续添加从节点了。</p>

<h4 id="隔离主节点">隔离主节点</h4>

<p>默认情况下，出于安全的考虑，并不会在主节点上运行 pod，如果你想在主节点上运行 pod，比如：运行一个单机版的 kubernetes 集群时，可运行下面的命令：</p>

<pre><code>kubectl taint nodes --all node-role.kubernetes.io/master-
</code></pre>

<p>输出类似这样：</p>

<pre><code>node &quot;test-01&quot; untainted
taint key=&quot;dedicated&quot; and effect=&quot;&quot; not found.
taint key=&quot;dedicated&quot; and effect=&quot;&quot; not found.
</code></pre>

<p>这将移除所有节点的<code>node-role.kubernetes.io/master</code>标志，包括主节点，Scheduler 便可以在任何节点上安排运行 pod 了。</p>

<h3 id="4-4-加入其他节点">(<sup>4</sup>&frasl;<sub>4</sub>) 加入其他节点</h3>

<p>节点就是你的负载（容器和 pod 等等）运行的地方，往集群里添加节点，只需要在每台机器上执行下列几步：</p>

<ul>
<li>SSH 登录机器</li>
<li>切换到 root (比如 sudo su -)</li>
<li>执行 <em>kubeadm init</em> 输出的那句命令: <code>kubeadm join --token &lt;token&gt; &lt;master-ip&gt;:&lt;master-port&gt; --discovery-token-ca-cert-hash sha256:&lt;hash&gt;</code></li>
</ul>

<p>执行后输出类似这样:</p>

<pre><code>raining@ubuntu1:~$ sudo kubeadm join 192.168.0.8:6443 --token vtyk9m.g4afak37myq3rsdi --discovery-token-ca-cert-hash sha256:19246ce11ba3fc633fe0b21f2f8aaaebd7df9103ae47138dc0dd615f61a32d99
[preflight] Running pre-flight checks.
    [WARNING SystemVerification]: docker version is greater than the most recently validated version. Docker version: 17.12.1-ce. Max validated version: 17.03
    [WARNING Service-Docker]: docker service is not enabled, please run 'systemctl enable docker.service'
    [WARNING FileExisting-crictl]: crictl not found in system path
Suggestion: go get github.com/kubernetes-incubator/cri-tools/cmd/crictl
[preflight] Starting the kubelet service
[discovery] Trying to connect to API Server &quot;192.168.0.8:6443&quot;
[discovery] Created cluster-info discovery client, requesting info from &quot;https://192.168.0.8:6443&quot;
[discovery] Requesting info from &quot;https://192.168.0.8:6443&quot; again to validate TLS against the pinned public key
[discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server &quot;192.168.0.8:6443&quot;
[discovery] Successfully established connection with API Server &quot;192.168.0.8:6443&quot;

This node has joined the cluster:
* Certificate signing request was sent to master and a response
  was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the master to see this node join the cluster.
</code></pre>

<p>几秒后，你在主节点上运行<code>kubectl get nodes</code>就可以看到新加的机器了：</p>

<pre><code>NAME             STATUS    ROLES     AGE       VERSION
raining-ubuntu   Ready     master    1h        v1.10.2
ubuntu1          Ready     &lt;none&gt;    2m        v1.10.2
</code></pre>

<h3 id="可选-在非主节点上管理集群">(可选) 在非主节点上管理集群</h3>

<p>为了可以在其他电脑上使用 kubectl 来管理你的集群，可以从主节点上复制管理员 的 kubeconfig 文件到你的电脑上：</p>

<pre><code>scp root@&lt;master ip&gt;:/etc/kubernetes/admin.conf .
kubectl --kubeconfig ./admin.conf get nodes
</code></pre>

<h3 id="可选-映射-api-服务到本地">(可选) 映射 API 服务到本地</h3>

<p>如果你想从集群外部连接到 API 服务，可以使用工具<code>kubectl proxy</code>:</p>

<pre><code>scp root@&lt;master ip&gt;:/etc/kubernetes/admin.conf .
kubectl --kubeconfig ./admin.conf proxy
</code></pre>

<p>这样就可以在本地这样 <code>http://localhost:8001/api/v1</code> 访问到 API 服务了。</p>

<h3 id="可选-部署一个微服务">(可选) 部署一个微服务</h3>

<p>现在可以测试你新搭建的集群了，Sock Shop 就是一个微服务的样本，它体现了在 Kubernetes 里如何运行和连接一系列的服务。想了解更多关于微服务的内容，请查看 <a href="https://github.com/microservices-demo/microservices-demo">GitHub README</a>。</p>

<pre><code>kubectl create namespace sock-shop
kubectl apply -n sock-shop -f &quot;https://github.com/microservices-demo/microservices-demo/blob/master/deploy/kubernetes/complete-demo.yaml?raw=true&quot;
</code></pre>

<p>可以通过以下命令来查看前端服务是否有开放对应的端口：</p>

<pre><code>kubectl -n sock-shop get svc front-end
</code></pre>

<p>输出类似:</p>

<pre><code>NAME        TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
front-end   NodePort   10.107.207.35   &lt;none&gt;        80:30001/TCP   31s
</code></pre>

<p>可能需要几分钟时间来下载和启用所有的容器，通过<code>kubectl get pods -n sock-shop</code>来获取服务的状态。</p>

<p>输出如下：</p>

<pre><code>raining@raining-ubuntu:~$ kubectl get pods -n sock-shop
NAME                            READY     STATUS    RESTARTS   AGE
carts-6cd457d86c-wdbsg          1/1       Running   0          1m
carts-db-784446fdd6-9gsrs       1/1       Running   0          1m
catalogue-779cd58f9b-nf6n4      1/1       Running   0          1m
catalogue-db-6794f65f5d-kwc2x   1/1       Running   0          1m
front-end-679d7bcb77-4hbjq      1/1       Running   0          1m
orders-755bd9f786-gbspz         1/1       Running   0          1m
orders-db-84bb8f48d6-98wsm      1/1       Running   0          1m
payment-674658f686-xc7gk        1/1       Running   0          1m
queue-master-5f98bbd67-xgqr6    1/1       Running   0          1m
rabbitmq-86d44dd846-nf2g6       1/1       Running   0          1m
shipping-79786fb956-bs7jn       1/1       Running   0          1m
user-6995984547-nvqw4           1/1       Running   0          1m
user-db-fc7b47fb9-zcf5r         1/1       Running   0          1m
</code></pre>

<p>然后在你的浏览器里访问集群节点的 IP 和对应的端口，比如<code>http://&lt;master_ip&gt;/&lt;cluster-ip&gt;:&lt;port&gt;</code>。 在这个例子里，可能是 30001，但是它可能跟你的不一样。如果有防火墙的话，确保在你访问之前开放了对应的端口。</p>

<p><img src="https://images2018.cnblogs.com/blog/347047/201805/347047-20180502084227253-1773772236.png" alt="" /></p>

<blockquote>
<p>需要注意的是，如果在多节点部署时，要使用节点的 IP 进行访问，而不是 Master 服务器的 IP。</p>
</blockquote>

<p>最后，卸载 <em>socks shop</em>, 只需要在主节点上运行:</p>

<pre><code>kubectl delete namespace sock-shop
</code></pre>

<h2 id="卸载集群">卸载集群</h2>

<p>想要撤销 kubeadm 做的事，首先要<a href="https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#drain">排除节点</a>，并确保在关闭节点之前要清空节点。</p>

<p>在主节点上运行：</p>

<pre><code>kubectl drain &lt;node name&gt; --delete-local-data --force --ignore-daemonsets
kubectl delete node &lt;node name&gt;
</code></pre>

<p>然后在需要移除的节点上，重置 kubeadm 的安装状态：</p>

<pre><code>kubeadm reset
</code></pre>

<p>如果你想重新配置集群，只需运行<code>kubeadm init</code>或者<code>kubeadm join</code>并使用所需的参数即可。</p>

<h2 id="参考资料">参考资料</h2>

<ul>
<li><a href="https://kubernetes.io/docs/setup/independent/install-kubeadm/">install-kubeadm</a></li>
<li><a href="https://console.cloud.google.com/gcr/images/google-containers">google-containers</a></li>
</ul>

              
            </div>
          </div>
          <div id="post-footer" class="post-footer main-content-wrap">
            
              
                
                
                  <div class="post-footer-tags">
                    <span class="text-color-light text-small">TAGGED IN</span><br/>
                    
  <a class="tag tag--primary tag--small" href="https://tangxusc.github.io/blog/tags/kubeadm/">kubeadm</a>

  <a class="tag tag--primary tag--small" href="https://tangxusc.github.io/blog/tags/k8s/">k8s</a>

  <a class="tag tag--primary tag--small" href="https://tangxusc.github.io/blog/tags/install/">install</a>

                  </div>
                
              
            
            <div class="post-actions-wrap">
  
      <nav >
        <ul class="post-actions post-action-nav">
          
            <li class="post-action">
              
                <a class="post-action-btn btn btn--default tooltip--top" href="https://tangxusc.github.io/blog/2019/03/%E4%BD%BF%E7%94%A8-golang-%E5%88%A9%E7%94%A8-ectd-%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/" data-tooltip="使用 Golang 利用 ectd 实现一个分布式锁">
              
                  <i class="fa fa-angle-left"></i>
                  <span class="hide-xs hide-sm text-small icon-ml">NEXT</span>
                </a>
            </li>
            <li class="post-action">
              
                <a class="post-action-btn btn btn--default tooltip--top" href="https://tangxusc.github.io/blog/2019/03/%E4%BD%BF%E7%94%A8go-mod1.11%E5%AE%89%E8%A3%85grpc/" data-tooltip="使用go mod(1.11)安装grpc">
              
                  <span class="hide-xs hide-sm text-small icon-mr">PREVIOUS</span>
                  <i class="fa fa-angle-right"></i>
                </a>
            </li>
          
        </ul>
      </nav>
    <ul class="post-actions post-action-share" >
      
        <li class="post-action hide-lg hide-md hide-sm">
          <a class="post-action-btn btn btn--default btn-open-shareoptions" href="#btn-open-shareoptions">
            <i class="fa fa-share-alt"></i>
          </a>
        </li>
        
      
      
        <li class="post-action">
          <a class="post-action-btn btn btn--default" href="#disqus_thread">
            <i class="fa fa-comment-o"></i>
          </a>
        </li>
      
      <li class="post-action">
        
          <a class="post-action-btn btn btn--default" href="#">
        
          <i class="fa fa-list"></i>
        </a>
      </li>
    </ul>
  
</div>

            
              
                <div id="disqus_thread">
  <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
              
            
          </div>
        </article>
        <footer id="footer" class="main-content-wrap">
  <span class="copyrights">
    &copy; 2019 苏连云. All Rights Reserved
  </span>
</footer>

      </div>
      <div id="bottom-bar" class="post-bottom-bar" data-behavior="4">
        <div class="post-actions-wrap">
  
      <nav >
        <ul class="post-actions post-action-nav">
          
            <li class="post-action">
              
                <a class="post-action-btn btn btn--default tooltip--top" href="https://tangxusc.github.io/blog/2019/03/%E4%BD%BF%E7%94%A8-golang-%E5%88%A9%E7%94%A8-ectd-%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/" data-tooltip="使用 Golang 利用 ectd 实现一个分布式锁">
              
                  <i class="fa fa-angle-left"></i>
                  <span class="hide-xs hide-sm text-small icon-ml">NEXT</span>
                </a>
            </li>
            <li class="post-action">
              
                <a class="post-action-btn btn btn--default tooltip--top" href="https://tangxusc.github.io/blog/2019/03/%E4%BD%BF%E7%94%A8go-mod1.11%E5%AE%89%E8%A3%85grpc/" data-tooltip="使用go mod(1.11)安装grpc">
              
                  <span class="hide-xs hide-sm text-small icon-mr">PREVIOUS</span>
                  <i class="fa fa-angle-right"></i>
                </a>
            </li>
          
        </ul>
      </nav>
    <ul class="post-actions post-action-share" >
      
        <li class="post-action hide-lg hide-md hide-sm">
          <a class="post-action-btn btn btn--default btn-open-shareoptions" href="#btn-open-shareoptions">
            <i class="fa fa-share-alt"></i>
          </a>
        </li>
        
      
      
        <li class="post-action">
          <a class="post-action-btn btn btn--default" href="#disqus_thread">
            <i class="fa fa-comment-o"></i>
          </a>
        </li>
      
      <li class="post-action">
        
          <a class="post-action-btn btn btn--default" href="#">
        
          <i class="fa fa-list"></i>
        </a>
      </li>
    </ul>
  
</div>

      </div>
      <div id="share-options-bar" class="share-options-bar" data-behavior="4">
  <i id="btn-close-shareoptions" class="fa fa-close"></i>
  <ul class="share-options">
    
  </ul>
</div>
<div id="share-options-mask" class="share-options-mask"></div>
    </div>
    
    <div id="about">
  <div id="about-card">
    <div id="about-btn-close">
      <i class="fa fa-remove"></i>
    </div>
    
      <img id="about-card-picture" src="https://www.gravatar.com/avatar/a3d740bc2618da5f5a9de4fe94c05429?s=110" alt="Author&#39;s picture" />
    
    <h4 id="about-card-name">苏连云</h4>
    
      <div id="about-card-bio">酒剑仙,醉仙酒</div>
    
    
      <div id="about-card-job">
        <i class="fa fa-briefcase"></i>
        <br/>
        小农民
      </div>
    
    
      <div id="about-card-location">
        <i class="fa fa-map-marker"></i>
        <br/>
        chengdu
      </div>
    
  </div>
</div>

    <div id="algolia-search-modal" class="modal-container">
  <div class="modal">
    <div class="modal-header">
      <span class="close-button"><i class="fa fa-close"></i></span>
      <a href="https://algolia.com" target="_blank" rel="noopener" class="searchby-algolia text-color-light link-unstyled">
        <span class="searchby-algolia-text text-color-light text-small">by</span>
        <img class="searchby-algolia-logo" src="https://www.algolia.com/static_assets/images/press/downloads/algolia-light.svg">
      </a>
      <i class="search-icon fa fa-search"></i>
      <form id="algolia-search-form">
        <input type="text" id="algolia-search-input" name="search"
          class="form-control input--large search-input" placeholder="Search" />
      </form>
    </div>
    <div class="modal-body">
      <div class="no-result text-color-light text-center">no post found</div>
      <div class="results">
        
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://tangxusc.github.io/blog/2019/03/controller-manager%E9%AB%98%E5%8F%AF%E7%94%A8%E5%AE%9E%E7%8E%B0%E6%96%B9%E5%BC%8F/">
                <h3 class="media-heading">Controller manager高可用实现方式</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Mar 3, 2019
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">本文由 简悦 SimpRead 转码， 原文地址 https://www.colabug.com/2801661.html
 这不是一系列入门级别的文章，也不是按部就班而来的，而是我看到哪里，发现有些代码写的精妙的地方，都值得我们学习下，顺手记录下来，一方面是让自己将来可以有迹可循，另外对大家应该也会有所帮助。而且记录本身成本并不是很高。
高可用部署情况下，需要部署多个 controller manager （以下简称 cm ），每个 cm 需要 --leader-elect=true 启动参数，即告知 cm 以高可用方式启动，谁要想进行真正的工作，必须先抢到锁，被选举为 leader 才行，而抢不到所得只能待机，在 leader 因为异常终止的时候，由剩余的其余节点再次获得锁。
关于分布式锁的实现很多，可以自己从零开始制造。当然更简单的是基于现有中间件，比如有基于 Redis 或数据库的实现方式，最近 Zookeeper/ETCD 也提供了相关功能。但 K8s 的实现并没有使用这些方式，而是另辟蹊径使用了资源锁的概念，简单来说就是通过创建 K8s 的资源（当前的实现中实现了 ConfigMap 和 Endpoint 两种类型的资源）来维护锁的状态。
分布式锁一般实现原理就是大家先去抢锁，抢到的人成为 leader ，然后 leader 会定期更新锁的状态，声明自己的活动状态，不让其他人把锁抢走。K8s 的资源锁也类似，抢到锁的节点会将自己的标记（目前是 hostname）设为锁的持有者，其他人则需要通过对比锁的更新时间和持有者来判断自己是否能成为新的 leader ，而 leader 则可以通过更新 RenewTime 来确保持续保有该锁。
大概看了下 K8s 的实现，老实说其实现方式并不算高雅，但是却给我们开拓了一种思路：K8s 里的 resource 是万能的，不要以为 Endpoint 只是 Endpoint 。不过反过来有时候也挺让人费解的，刚了解的时候容易摸不着头脑，也不是好事。而且 scheduler 和 cm 都采用了资源锁，但是实现起来却不尽相同，也值得吐槽下。不管怎么说，这个实现算是挺有意思的实现，值得我们深入了解下。
我们首先来看一下 cm 启动的时候，是如何去 初始化 抢锁的。启动的时候，如果指定了 --leader-elect=true 参数的话，则会进入下面的代码，首先获取自己的资源标志（这里是 hostname 加一串随机数字）。</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://tangxusc.github.io/blog/2019/03/gitflow-%E5%B7%A5%E4%BD%9C%E6%B5%81/">
                <h3 class="media-heading">Gitflow 工作流</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Mar 3, 2019
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">本文由 简悦 SimpRead 转码， 原文地址 http://blog.jobbole.com/76867/
 这节介绍的 Gitflow工作流借鉴自在 nvie 的 _Vincent Driessen_。
Gitflow工作流定义了一个围绕项目发布的严格分支模型。虽然比功能分支工作流复杂几分，但提供了用于一个健壮的用于管理大型项目的框架。
Gitflow工作流没有用超出功能分支工作流的概念和命令，而是为不同的分支分配一个很明确的角色，并定义分支之间如何和什么时候进行交互。除了使用功能分支，在做准备、维护和记录发布也使用各自的分支。当然你可以用上功能分支工作流所有的好处：Pull Requests、隔离实验性开发和更高效的协作。
工作方式 Gitflow工作流仍然用中央仓库作为所有开发者的交互中心。和其它的工作流一样，开发者在本地工作并push分支到要中央仓库中。
历史分支 相对使用仅有的一个master分支，Gitflow工作流使用 2 个分支来记录项目的历史。master分支存储了正式发布的历史，而develop分支作为功能的集成分支。这样也方便master分支上的所有提交分配一个版本号。
剩下要说明的问题围绕着这 2 个分支的区别展开。
功能分支 每个新功能位于一个自己的分支，这样可以 push到中央仓库以备份和协作。但功能分支不是从master分支上拉出新分支，而是使用develop分支作为父分支。当新功能完成时，合并回develop分支。新功能提交应该从不直接与master分支交互。
注意，从各种含义和目的上来看，功能分支加上develop分支就是功能分支工作流的用法。但Gitflow工作流没有在这里止步。
发布分支 一旦develop分支上有了做一次发布（或者说快到了既定的发布日）的足够功能，就从develop分支上fork一个发布分支。新建的分支用于开始发布循环，所以从这个时间点开始之后新的功能不能再加到这个分支上 —— 这个分支只应该做Bug修复、文档生成和其它面向发布任务。一旦对外发布的工作都完成了，发布分支合并到master分支并分配一个版本号打好Tag。另外，这些从新建发布分支以来的做的修改要合并回develop分支。
使用一个用于发布准备的专门分支，使得一个团队可以在完善当前的发布版本的同时，另一个团队可以继续开发下个版本的功能。 这也打造定义良好的开发阶段（比如，可以很轻松地说，『这周我们要做准备发布版本 4.0』，并且在仓库的目录结构中可以实际看到）。
常用的分支约定：
用于新建发布分支的分支: develop 用于合并的分支: master 分支命名: release-* 或 release/*
维护分支 维护分支或说是热修复（hotfix）分支用于生成快速给产品发布版本（production releases）打补丁，这是唯一可以直接从master分支fork出来的分支。修复完成，修改应该马上合并回master分支和develop分支（当前的发布分支），master分支应该用新的版本号打好Tag。
为Bug修复使用专门分支，让团队可以处理掉问题而不用打断其它工作或是等待下一个发布循环。你可以把维护分支想成是一个直接在master分支上处理的临时发布。
示例 下面的示例演示本工作流如何用于管理单个发布循环。假设你已经创建了一个中央仓库。
创建开发分支 第一步为master分支配套一个develop分支。简单来做可以本地创建一个空的develop分支，push到服务器上：
git branch develop git push -u origin develop
以后这个分支将会包含了项目的全部历史，而master分支将只包含了部分历史。其它开发者这时应该克隆中央仓库，建好develop分支的跟踪分支：
git clone ssh://user@host/path/to/repo.git git checkout -b develop origin/develop</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://tangxusc.github.io/blog/2019/03/go-%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E5%8F%8A-pprof-%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95%E8%AF%A6%E8%A7%A3/">
                <h3 class="media-heading">Go 程序性能优化及 pprof 使用方法详解</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Mar 3, 2019
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">本文由 简悦 SimpRead 转码， 原文地址 https://www.jb51.net/article/127551.htm
 Go 程序性能优化及 pprof 使用方法详解  更新时间：2017 年 11 月 05 日 10:50:22 作者：snowInPluto
这篇文章主要为大家详细介绍了 Go 程序性能优化及 pprof 的使用方法，具有一定的参考价值，感兴趣的小伙伴们可以参考一下
Go 程序的性能优化及 pprof 的使用
程序的性能优化无非就是对程序占用资源的优化。对于服务器而言，最重要的两项资源莫过于 CPU 和内存。性能优化，就是在对于不影响程序数据处理能力的情况下，我们通常要求程序的 CPU 的内存占用尽量低。反过来说，也就是当程序 CPU 和内存占用不变的情况下，尽量地提高程序的数据处理能力或者说是吞吐量。
Go 的原生工具链中提供了非常多丰富的工具供开发者使用，其中包括 pprof。
对于 pprof 的使用要分成下面两部分来说。
Web 程序使用 pprof
先写一个简单的 Web 服务程序。程序在 9876 端口上接收请求。
package main import ( &quot;bytes&quot; &quot;io/ioutil&quot; &quot;log&quot; &quot;math/rand&quot; &quot;net/http&quot; _ &quot;net/http/pprof&quot; ) func main() { http.HandleFunc(&quot;/test&quot;, handler) log.Fatal(http.ListenAndServe(&quot;:9876&quot;, nil)) } func handler(w http.</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://tangxusc.github.io/blog/2019/03/golang%E5%A4%A7%E6%9D%80%E5%99%A8%E4%B9%8B%E6%80%A7%E8%83%BD%E5%89%96%E6%9E%90pprof/">
                <h3 class="media-heading">Golang大杀器之性能剖析PProf</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Mar 3, 2019
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">本文由 简悦 SimpRead 转码， 原文地址 https://github.com/EDDYCJY/blog/blob/master/golang/2018-09-15-Golang%20%E5%A4%A7%E6%9D%80%E5%99%A8%E4%B9%8B%E6%80%A7%E8%83%BD%E5%89%96%E6%9E%90%20PProf.md
 Golang 大杀器之性能剖析 PProf 前言 写了几吨代码，实现了几百个接口。功能测试也通过了，终于成功的部署上线了
结果，性能不佳，什么鬼？
想做性能分析 PProf 想要进行性能优化，首先瞩目在 Go 自身提供的工具链来作为分析依据，本文将带你学习、使用 Go 后花园，涉及如下：
 runtime/pprof：采集程序（非 Server）的运行数据进行分析 net/http/pprof：采集 HTTP Server 的运行时数据进行分析  是什么 pprof 是用于可视化和分析性能分析数据的工具
pprof 以 profile.proto 读取分析样本的集合，并生成报告以可视化并帮助分析数据（支持文本和图形报告）
profile.proto 是一个 Protocol Buffer v3 的描述文件，它描述了一组 callstack 和 symbolization 信息， 作用是表示统计分析的一组采样的调用栈，是很常见的 stacktrace 配置文件格式
支持什么使用模式  Report generation：报告生成 Interactive terminal use：交互式终端使用 Web interface：Web 界面  可以做什么  CPU Profiling：CPU 分析，按照一定的频率采集所监听的应用程序 CPU（含寄存器）的使用情况，可确定应用程序在主动消耗 CPU 周期时花费时间的位置 Memory Profiling：内存分析，在应用程序进行堆分配时记录堆栈跟踪，用于监视当前和历史内存使用情况，以及检查内存泄漏 Block Profiling：阻塞分析，记录 goroutine 阻塞等待同步（包括定时器通道）的位置 Mutex Profiling：互斥锁分析，报告互斥锁的竞争情况  一个简单的例子 我们将编写一个简单且有点问题的例子，用于基本的程序初步分析</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://tangxusc.github.io/blog/2019/03/go%E6%A8%A1%E5%9D%97%E7%AE%80%E4%BB%8B/">
                <h3 class="media-heading">Go模块简介</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Mar 3, 2019
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">本文由 简悦 SimpRead 转码， 原文地址 https://roberto.selbach.ca/intro-to-go-modules/
 Go模块简介 发表于 2018年8月18日(https://roberto.selbach.ca/intro-to-go-modules/) 作者：Roberto Selbach
即将发布的Go编程语言版本1.11将为_模块_带来实验性支持 ，几天前Go.A的新依赖管理系统，我写了一篇关于它的快速帖子。自那篇文章上线以来，事情发生了一些变化，因为我们现在非常接近新版本，我认为现在是另一篇文章更适合实践的好时机。所以这就是我们要做的：我们将创建一个新的包，然后我们将发布一些版本，看看它是如何工作的。
创建模块 首先要做的事情。让我们创建我们的包。我们称之为“testmod”。这里有一个重要的细节：这个目录应该 在你的外面，因为默认情况下，模块支持在其中被禁用。Go模块是可能在某些时候完全消除的第一步。$GOPATH$GOPATH
$ mkdir testmod $ cd testmod  我们的包很简单：
package testmod import &quot;fmt&quot; // Hi returns a friendly greeting func Hi(name string) string { return fmt.Sprintf(&quot;Hi, %s&quot;, name) }  包完成但它仍然不是_模块_。让我们改变这一点。
$ go mod init github.com/robteix/testmod go: creating new go.mod: module github.com/robteix/testmod  这将go.mod在包目录中创建一个新文件，其中包含以下内容：
module github.com/robteix/testmod  这里不是很多，但这有效地将我们的包变成了一个 _模块_。我们现在可以将这个代码推送到一个存储库：
$ git init $ git add * $ git commit -am &quot;First commit&quot; $ git push -u origin master  到目前为止，任何愿意使用此软件包的人都会go get ：</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://tangxusc.github.io/blog/2019/03/intellij-idea-%E5%9F%BA%E4%BA%8E%E7%BC%96%E8%BE%91%E5%99%A8%E7%9A%84-rest-%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%BB%8B%E7%BB%8D/">
                <h3 class="media-heading">Intellij IDEA 基于编辑器的 REST 客户端介绍</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Mar 3, 2019
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">本文由 简悦 SimpRead 转码， 原文地址 https://blog.csdn.net/u011054333/article/details/78705256
 最近 Intellij IDEA 更新到了 2017.3 这一版本，这个版本又增加了很多新功能。我觉得其中这个基于编辑器的 REST 客户端这个功能很不错，可以为我们带来很多方便。这个功能并不仅仅在 Intellij IDEA 才有，最近更新的所有 Jetbrains 系 IIDE 都有这个功能。
以往我们开发和调试网络程序，用到的无非是这几种办法：浏览器 F12 工具、Fiddler、Wireshark、curl 等命令行工具、手动使用 HTTP 客户端类库编程。不过这些方法总是有些不好用。Jetbrains 这个基于编辑器的 REST 客户端用起来倒是让我眼前一亮。
使用方法 要使用这个功能很简单，在 IDE 中新建一个后缀名为.http的文件，然后就可以使用这个功能了。截图如下。
这个功能使用起来非常简单，使用大写的 HTTP 动词（GET、POST、DELETE、PUT 等等）后面加上要访问的网址即可，如果端口号不是 80 或者 443，可以使用冒号 + 端口号的形式写在网址后面。如果需要修改 Cookie、ContentType、UA 等设置，直接写在后面几行即可，Jetbrains 提供了非常完善的补全支持，我们只要敲第一个大写字母即可获得相应的代码提示。想要发起一个请求的时候，直接点击前面的绿色运行按钮即可。一个文件中可以保存多个请求，如果以后还想再次运行只要打开这个文件即可。
配置环境变量 Jetbrains 还提供了一个环境变量的功能，让我们使用这个编辑器 REST 客户端更加简单。只要在项目中添加一个名为rest-client.env.json的文件，然后配置不同环境下要使用的环境变量。然后就能在 REST 客户端中使用了。例如配置文件是这样的。
{ &quot;dev&quot;: { &quot;host&quot;: &quot;http://httpbin.org&quot; }, &quot;prod&quot;: { &quot;host&quot;: &quot;http://httpbin.org&quot; } }  那么在点击运行按钮的时候就会弹出选择要使用哪个环境变量。我们只要选择就可以针对不同环境使用不同配置了。在代码中只要使用双括号引用环境变量即可。</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://tangxusc.github.io/blog/2019/03/java-%E5%A5%87%E6%8A%80%E6%B7%AB%E5%B7%A7-%E6%8F%92%E4%BB%B6%E5%8C%96%E6%B3%A8%E8%A7%A3%E5%A4%84%E7%90%86-apipluggable-annotation-processing-api/">
                <h3 class="media-heading">Java 奇技淫巧 - 插件化注解处理 API(Pluggable Annotation Processing API)</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Mar 3, 2019
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">本文由 简悦 SimpRead 转码， 原文地址 https://www.cnblogs.com/throwable/p/9139908.html
 插件化注解处理 API(Pluggable Annotation Processing API) Java 奇技淫巧 - 插件化注解处理 API(Pluggable Annotation Processing API) 参考资料  JDK6 的新特性之六: 插入式注解处理 API(Pluggable Annotation Processing API) Java Annotation Processing and Creating a Builder  简介 插件化注解处理 (Pluggable Annotation Processing)APIJSR 269 提供一套标准 API 来处理 AnnotationsJSR 175, 实际上 JSR 269 不仅仅用来处理 Annotation，我觉得更强大的功能是它建立了 Java 语言本身的一个模型, 它把 method、package、constructor、type、variable、enum、annotation 等 Java 语言元素映射为 Types 和 Elements，从而将 Java 语言的语义映射成为对象，我们可以在 javax.lang.model 包下面可以看到这些类。所以我们可以利用 JSR 269 提供的 API 来构建一个功能丰富的元编程 (metaprogramming) 环境。JSR 269 用 Annotation Processor 在编译期间而不是运行期间处理 Annotation, Annotation Processor 相当于编译器的一个插件, 所以称为插入式注解处理.</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://tangxusc.github.io/blog/2019/03/java-%E8%AF%BB%E5%86%99%E9%94%81%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/">
                <h3 class="media-heading">Java 读写锁实现原理</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Mar 3, 2019
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">本文由 简悦 SimpRead 转码， 原文地址 https://my.oschina.net/editorial-story/blog/1928306
 最近做的一个小项目中有这样的需求：整个项目有一份*config.json*保存着项目的一些配置，是存储在本地文件的一个资源，并且应用中存在读写（读 &gt;&gt; 写）更新问题。既然读写并发操作，那么就涉及到操作互斥，这里自然想到了读写锁，本文对读写锁方面的知识做个梳理。
为什么需要读写锁？ 与传统锁不同的是读写锁的规则是可以共享读，但只能一个写，总结起来为：读读不互斥，读写互斥，写写互斥，而一般的独占锁是：读读互斥，读写互斥，写写互斥，而场景中往往读远远大于写，读写锁就是为了这种优化而创建出来的一种机制。
注意是读远远大于写，一般情况下独占锁的效率低来源于高并发下对临界区的激烈竞争导致线程上下文切换。因此当并发不是很高的情况下，读写锁由于需要额外维护读锁的状态，可能还不如独占锁的效率高。因此需要根据实际情况选择使用。
一个简单的读写锁实现 根据上面理论可以利用两个 int 变量来简单实现一个读写锁，实现虽然烂，但是原理都是差不多的，值得阅读下。
public class ReadWriteLock { /** * 读锁持有个数 */ private int readCount = 0; /** * 写锁持有个数 */ private int writeCount = 0; /** * 获取读锁,读锁在写锁不存在的时候才能获取 */ public synchronized void lockRead() throws InterruptedException { // 写锁存在,需要wait while (writeCount &gt; 0) { wait(); } readCount++; } /** * 释放读锁 */ public synchronized void unlockRead() { readCount--; notifyAll(); } /** * 获取写锁,当读锁存在时需要wait.</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://tangxusc.github.io/blog/2019/03/java8-%E6%97%A5%E6%9C%9F%E5%92%8C%E6%97%B6%E9%97%B4/">
                <h3 class="media-heading">Java8 日期和时间</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Mar 3, 2019
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">本文由 简悦 SimpRead 转码， 原文地址 https://blog.csdn.net/a80596890555/article/details/58687444
 如何正确处理时间 现实生活的世界里，时间是不断向前的，如果向前追溯时间的起点，可能是宇宙出生时，又或是是宇宙出现之前， 但肯定是我们目前无法找到的，我们不知道现在距离时间原点的精确距离。所以我们要表示时间， 就需要人为定义一个原点。
原点被规定为，格林威治时间 (GMT)1970 年 1 月 1 日的午夜 为起点, 之于为啥是 GMT 时间，大概是因为本初子午线在那的原因吧。
java 中的时间 如果你跟你朋友说：“我们 1484301456 一起去吃饭，别迟到！”，而你朋友能马上理解你说的时间，表示时间就会很简单， 只需要一个 long 值来表示原点的偏移量，这是个绝对时间，在世界范围内都适用。但实际上我们不能马上理解这串数字， 而且我们需要不同的时间单位来表示时间的跨度，比如一个季度是 3 个月，一个月有 30 天等。 你可以跟朋友约好 “明天这个时候再见面”, 你朋友很容易理解明天的意思，但要是没有’天’这个单位， 他就需要在那串数字上加上 86400(一天是 86400 秒)。
Java 三次引入处理时间的 API，JDK1.0 中包含了一个Date类，但大多数方法在 java1.1 引入Calendear类之后被弃用了。 它的实例都是可变的，而且它的 API 很难使用，比如月份是从 0 开始这种反人类的设置。
java8 引入的java.time API 已经纠正了之前的问题。它已经完全实现了JSR310规范。
java8 时间 API 介绍及使用 在新的时间 API 中，Instant表示一个精确的时间点，Duration和Period表示两个时间点之间的时间量。 LocalDate表示日期，即 xx 年 xx 月 xx 日，即不包括时间也不带时区。LocalTime与LocalDate类似， 但只包含时间。LocalDateTime则包含日期和时间。ZoneDateTime表示一个带时区的时间。 DateTimeFormatter提供格式化和解析功能。下面详细的介绍使用方法。</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://tangxusc.github.io/blog/2019/03/java%E6%B3%A8%E9%87%8A%E5%A4%84%E7%90%86%E5%92%8C%E5%88%9B%E5%BB%BA%E6%9E%84%E5%BB%BA%E5%99%A8/">
                <h3 class="media-heading">Java注释处理和创建构建器</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Mar 3, 2019
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">本文由 简悦 SimpRead 转码， 原文地址 http://www.baeldung.com/java-annotation-processing-builder
 1.简介 本文是Java源代码级别注释处理的简介，并提供了使用此技术在编译期间生成其他源文件的示例。
2.注释处理的应用 源级注释处理首先出现在Java 5中。它是一种在编译阶段生成其他源文件的便捷技术。
源文件不必是Java文件 - 您可以根据源代码中的注释生成任何类型的描述，元数据，文档，资源或任何其他类型的文件。
注释处理在许多无处不在的Java库中被广泛使用，例如，在QueryDSL和JPA中生成元类，以使用Lombok库中的样板代码来扩充类。
需要注意的一件重要事情是注释处理API的局限性 - 它只能用于生成新文件，而不能用于更改现有文件。
值得注意的例外是Lombok库，它使用注释处理作为引导机制，将自身包含在编译过程中，并通过一些内部编译器API修改AST。这种hacky技术与注释处理的预期目的无关，因此本文不讨论。
3.注释处理API 注释处理在多轮中完成。每一轮都从编译器搜索源文件中的注释并选择适合这些注释的注释处理器开始。反过来，每个注释处理器在相应的源上被调用。
如果在此过程中生成了任何文件，则会以生成的文件作为输入启动另一轮。此过程将继续，直到在处理阶段没有生成新文件。
反过来，每个注释处理器在相应的源上被调用。如果在此过程中生成了任何文件，则会以生成的文件作为输入启动另一轮。此过程将继续，直到在处理阶段没有生成新文件。
注释处理API位于_javax.annotation.processing_包中。您必须实现的主要接口是_Processor_接口，它具有_AbstractProcessor_类形式的部分实现。这个类是我们要扩展的类，以创建我们自己的注释处理器。
4.设置项目 为了演示注释处理的可能性，我们将开发一个简单的处理器，用于为带注释的类生成流畅的对象构建器。
我们将把项目分成两个Maven模块。其中之一，_注释处理器_模块，将包含处理器本身和注释，另一个_注释用户_模块将包含注释类。这是注释处理的典型用例。
_注释处理器_模块的设置如下。我们将使用Google的自动服务库来生成稍后将讨论的处理器元数据文件，以及针对Java 8源代码调整的_maven-compiler-plugin_。这些依赖项的版本将提取到属性部分。
可以在Maven Central存储库中找到最新版本的自动服务库和maven-compiler-plugin：
&lt;properties&gt; &lt;auto-service.version&gt;1.0-rc2&lt;/auto-service.version&gt; &lt;maven-compiler-plugin.version&gt; 3.5.1 &lt;/maven-compiler-plugin.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.google.auto.service&lt;/groupId&gt; &lt;artifactId&gt;auto-service&lt;/artifactId&gt; &lt;version&gt;${auto-service.version}&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;${maven-compiler-plugin.version}&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;  带有注释源的注释用户 Maven模块不需要任何特殊调整，除了在依赖项部分中添加对注释处理器模块的依赖：
&lt;dependency&gt; &lt;groupId&gt;com.baeldung&lt;/groupId&gt; &lt;artifactId&gt;annotation-processing&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt;  5.</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
      </div>
    </div>
    <div class="modal-footer">
      <p class="results-count text-medium"
         data-message-zero="no post found"
         data-message-one="1 post found"
         data-message-other="{n} posts found">
         76 posts found
      </p>
    </div>
  </div>
</div>
    
  
    
    <div id="cover" style="background-image:url('https://tangxusc.github.io/blog/images/cover.jpg');"></div>
  


    
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js" integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44=" crossorigin="anonymous"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.7/js/jquery.fancybox.min.js" integrity="sha256-GEAnjcTqVP+vBp3SSc8bEDQqvWAZMiHyUSIorrWwH50=" crossorigin="anonymous"></script>


<script src="https://tangxusc.github.io/blog/js/script-qi9wbxp2ya2j6p7wx1i6tgavftewndznf4v0hy2gvivk1rxgc3lm7njqb6bz.min.js"></script>


<script lang="javascript">
window.onload = updateMinWidth;
window.onresize = updateMinWidth;
document.getElementById("sidebar").addEventListener("transitionend", updateMinWidth);
function updateMinWidth() {
  var sidebar = document.getElementById("sidebar");
  var main = document.getElementById("main");
  main.style.minWidth = "";
  var w1 = getComputedStyle(main).getPropertyValue("min-width");
  var w2 = getComputedStyle(sidebar).getPropertyValue("width");
  var w3 = getComputedStyle(sidebar).getPropertyValue("left");
  main.style.minWidth = `calc(${w1} - ${w2} - ${w3})`;
}
</script>

<script>
$(document).ready(function() {
  hljs.configure({ classPrefix: '', useBR: false });
  $('pre.code-highlight > code, pre > code').each(function(i, block) {
    if (!$(this).hasClass('codeblock')) {
      $(this).addClass('codeblock');
    }
    hljs.highlightBlock(block);
  });
});
</script>


  
    
      <script>
        var disqus_config = function () {
          this.page.url = 'https:\/\/tangxusc.github.io\/blog\/2019\/03\/%E4%BD%BF%E7%94%A8-kubeadm-%E6%90%AD%E5%BB%BA-kubernetes1.10.2-%E9%9B%86%E7%BE%A4%E5%9B%BD%E5%86%85%E7%8E%AF%E5%A2%83\/';
          
            this.page.identifier = '\/2019\/03\/%E4%BD%BF%E7%94%A8-kubeadm-%E6%90%AD%E5%BB%BA-kubernetes1.10.2-%E9%9B%86%E7%BE%A4%E5%9B%BD%E5%86%85%E7%8E%AF%E5%A2%83\/'
          
        };
        (function() {
          
          
          if (window.location.hostname == "localhost") {
            return;
          }
          var d = document, s = d.createElement('script');
          var disqus_shortname = 'hugo-tranquilpeak-theme';
          s.src = '//' + disqus_shortname + '.disqus.com/embed.js';

          s.setAttribute('data-timestamp', +new Date());
          (d.head || d.body).appendChild(s);
        })();
      </script>
    
  




    
  </body>
</html>

